---
name: Benchmark Suite

'on':
  schedule:
    - cron: '0 2 * * 1'  # Weekly on Monday at 2 AM
  workflow_dispatch:
    inputs:
      run_large:
        description: 'Run large dataset benchmarks'
        required: false
        type: boolean
        default: false
      benchmark_suite:
        description: 'Benchmark suite to run'
        required: false
        type: choice
        options:
          - 'basic'
          - 'extended'
          - 'all'
        default: 'basic'
  push:
    paths:
      - 'nlsq/**'
      - 'benchmark/**'
      - '.github/workflows/benchmark.yml'
    branches: [main, develop]

concurrency:
  group: benchmark-${{ github.ref }}
  cancel-in-progress: true

env:
  # Performance optimizations for scientific computing
  OMP_NUM_THREADS: 4
  MKL_NUM_THREADS: 4
  OPENBLAS_NUM_THREADS: 4
  JAX_PLATFORMS: cpu
  PYTHONHASHSEED: 0
  PYTHONIOENCODING: utf-8
  PIP_DISABLE_PIP_VERSION_CHECK: 1
  PIP_NO_CACHE_DIR: 1

jobs:
  # Primary Ubuntu benchmarking
  benchmark:
    runs-on: ubuntu-latest
    timeout-minutes: 45
    strategy:
      fail-fast: false
      matrix:
        python-version: ['3.12']
        benchmark-type: ['performance', 'memory']

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 1  # Shallow clone for speed

      - name: Free disk space (Linux)
        run: |
          sudo rm -rf /usr/share/dotnet /opt/ghc /usr/local/share/boost
          sudo apt-get clean
          df -h

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Cache package installation
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/pip
            build/
          key: benchmark-ubuntu-py${{ matrix.python-version }}-${{ hashFiles('pyproject.toml') }}
          restore-keys: |
            benchmark-ubuntu-py${{ matrix.python-version }}-
            benchmark-ubuntu-

      - name: Install dependencies (optimized)
        run: |
          python -m pip install --upgrade pip wheel setuptools
          # Install benchmark dependencies efficiently
          pip install --only-binary=all -e ".[benchmark]" ||
            pip install -e ".[all]"
          pip install memory_profiler psutil

      - name: System information
        run: |
          python -c "import sys; print(f'Python: {sys.version}')"
          python -c "import numpy; print(f'NumPy: {numpy.__version__}')"
          python -c "import jax; print(f'JAX: {jax.__version__}'); print(f'JAX devices: {jax.devices()}')"
          free -h || true
          nproc || true

      - name: Run performance benchmarks
        if: matrix.benchmark-type == 'performance'
        run: |
          # Create results directory
          mkdir -p benchmark_results

          # Determine suite based on inputs
          SUITE="${{ github.event.inputs.benchmark_suite }}"
          if [ -z "$SUITE" ] || [ "$SUITE" == "null" ]; then
            SUITE="basic"
          fi

          echo "Running $SUITE benchmark suite..."
          timeout 1800 python benchmark/benchmark.py \
            --suite "$SUITE" \
            --save \
            --output-dir benchmark_results \
            --format markdown \
            --timeout 300 \
            > benchmark_results/performance_results.md || echo "Some benchmarks timed out"

      - name: Run large dataset benchmarks
        if: |
          (github.event.inputs.run_large == 'true' || github.event_name == 'schedule') &&
          matrix.benchmark-type == 'performance'
        run: |
          echo "Running large dataset benchmarks..."
          timeout 1800 python benchmark/benchmark.py \
            --suite large \
            --save \
            --output-dir benchmark_results \
            --format markdown \
            --timeout 600 \
            >> benchmark_results/performance_results.md || echo "Large benchmarks timed out"

      - name: Profile memory usage
        if: matrix.benchmark-type == 'memory' || github.event_name == 'schedule'
        run: |
          echo "Profiling memory usage..."
          mkdir -p benchmark_results
          python -m memory_profiler benchmark/benchmark.py \
            --suite basic \
            --memory-profile \
            > benchmark_results/memory_profile.md || echo "Memory profiling completed with warnings"

      - name: Generate comprehensive benchmark report
        run: |
          # Create comprehensive report
          mkdir -p benchmark_results

          # Get system information
          CPU_COUNT=$(nproc)
          MEMORY_INFO=$(free -h | grep 'Mem:' | awk '{print $2}')

          cat > benchmark_results/BENCHMARK_REPORT.md << EOF
          # Benchmark Report - $(date -u '+%Y-%m-%d %H:%M:%S UTC')

          ## System Information
          - **OS**: ubuntu-latest
          - **Python**: ${{ matrix.python-version }}
          - **Benchmark Type**: ${{ matrix.benchmark-type }}
          - **GPU**: false
          - **CPU Cores**: ${CPU_COUNT}
          - **Available Memory**: ${MEMORY_INFO}
          - **JAX Platform**: ${{ env.JAX_PLATFORMS }}
          - **OMP Threads**: ${{ env.OMP_NUM_THREADS }}

          ## Benchmark Configuration
          - **Suite**: ${{ github.event.inputs.benchmark_suite || 'basic' }}
          - **Large Datasets**: ${{ github.event.inputs.run_large || 'false' }}
          - **Trigger**: ${{ github.event_name }}

          ---

          EOF

          # Append performance results if they exist
          if [ -f "benchmark_results/performance_results.md" ]; then
            echo "## Performance Results" >> benchmark_results/BENCHMARK_REPORT.md
            cat benchmark_results/performance_results.md >> benchmark_results/BENCHMARK_REPORT.md
            echo "" >> benchmark_results/BENCHMARK_REPORT.md
          fi

          # Append memory profile if it exists
          if [ -f "benchmark_results/memory_profile.md" ]; then
            echo "## Memory Profile" >> benchmark_results/BENCHMARK_REPORT.md
            cat benchmark_results/memory_profile.md >> benchmark_results/BENCHMARK_REPORT.md
          fi

          # Add system resource usage
          echo "## System Resources (Post-Benchmark)" >> benchmark_results/BENCHMARK_REPORT.md
          echo "\`\`\`" >> benchmark_results/BENCHMARK_REPORT.md
          free -h >> benchmark_results/BENCHMARK_REPORT.md
          df -h / >> benchmark_results/BENCHMARK_REPORT.md
          echo "\`\`\`" >> benchmark_results/BENCHMARK_REPORT.md

      - name: Upload benchmark artifacts
        uses: actions/upload-artifact@v4
        with:
          name: >
            benchmark-results-${{ matrix.benchmark-type }}-ubuntu-py${{ matrix.python-version }}
          path: benchmark_results/
          retention-days: 30

      - name: Performance regression check
        if: matrix.benchmark-type == 'performance' && github.ref == 'refs/heads/main'
        continue-on-error: true
        run: |
          # Simple performance regression detection
          if [ -f "benchmark_results/performance_results.md" ]; then
            echo "Checking for performance regressions..."
            # Extract timing information and compare with previous runs
            grep -E "Time:|seconds" benchmark_results/performance_results.md || echo "No timing data found"
          fi

  # Cross-platform benchmarking - conditional
  benchmark-cross-platform:
    runs-on: ${{ matrix.os }}
    timeout-minutes: 45
    if: github.event_name == 'schedule' || github.event_name == 'release'
    strategy:
      fail-fast: false
      matrix:
        os: [windows-latest, macos-latest]
        python-version: ['3.12']
        benchmark-type: ['performance']

    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: Free disk space (Windows)
        if: runner.os == 'Windows'
        run: |
          Get-WmiObject -Class Win32_LogicalDisk |
            Select-Object DeviceID,Size,FreeSpace
        shell: powershell

      - name: Free disk space (macOS)
        if: runner.os == 'macOS'
        run: |
          df -h
          brew cleanup --prune=all || true

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Cache package installation
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/pip
            build/
          key: benchmark-${{ runner.os }}-py${{ matrix.python-version }}-${{ hashFiles('pyproject.toml') }}
          restore-keys: |
            benchmark-${{ runner.os }}-py${{ matrix.python-version }}-
            benchmark-${{ runner.os }}-

      - name: Install dependencies (optimized)
        run: |
          python -m pip install --upgrade pip wheel setuptools
          pip install --only-binary=all -e ".[benchmark]" ||
            pip install -e ".[all]"
          pip install memory_profiler psutil

      - name: System information (Windows)
        if: runner.os == 'Windows'
        run: |
          python -c "import sys; print(f'Python: {sys.version}')"
          python -c "import numpy; print(f'NumPy: {numpy.__version__}')"
          python -c "import jax; print(f'JAX: {jax.__version__}'); print(f'JAX devices: {jax.devices()}')"
          systeminfo | findstr /C:"Total Physical Memory"
          echo "Processor Count: %NUMBER_OF_PROCESSORS%"
        shell: cmd

      - name: System information (macOS)
        if: runner.os == 'macOS'
        run: |
          python -c "import sys; print(f'Python: {sys.version}')"
          python -c "import numpy; print(f'NumPy: {numpy.__version__}')"
          python -c "import jax; print(f'JAX: {jax.__version__}'); print(f'JAX devices: {jax.devices()}')"
          vm_stat | head -10 || true
          sysctl -n hw.ncpu || true

      - name: Run performance benchmarks
        run: |
          mkdir -p benchmark_results
          SUITE="${{ github.event.inputs.benchmark_suite }}"
          if [ -z "$SUITE" ] || [ "$SUITE" == "null" ]; then
            SUITE="basic"
          fi

          echo "Running $SUITE benchmark suite..."
          timeout 1800 python benchmark/benchmark.py \
            --suite "$SUITE" \
            --save \
            --output-dir benchmark_results \
            --format markdown \
            --timeout 300 \
            > benchmark_results/performance_results.md || echo "Some benchmarks timed out"

      - name: Upload benchmark artifacts
        uses: actions/upload-artifact@v4
        with:
          name: >
            benchmark-results-${{ matrix.benchmark-type }}-${{ matrix.os }}-py${{ matrix.python-version }}
          path: benchmark_results/
          retention-days: 30

  # Summary job to report overall benchmark status
  benchmark-summary:
    runs-on: ubuntu-latest
    needs: [benchmark]
    if: always()

    steps:
      - name: Download all artifacts
        uses: actions/download-artifact@v4

      - name: Generate summary
        run: |
          echo "# Benchmark Summary" > BENCHMARK_SUMMARY.md
          echo "" >> BENCHMARK_SUMMARY.md
          echo "**Workflow**: ${{ github.workflow }}" >> BENCHMARK_SUMMARY.md
          echo "**Trigger**: ${{ github.event_name }}" >> BENCHMARK_SUMMARY.md
          echo "**Date**: $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> BENCHMARK_SUMMARY.md
          echo "" >> BENCHMARK_SUMMARY.md

          # List all benchmark results
          echo "## Available Results" >> BENCHMARK_SUMMARY.md
          find . -name "BENCHMARK_REPORT.md" -exec echo "- {}" \; >> BENCHMARK_SUMMARY.md

          # Job status summary
          echo "" >> BENCHMARK_SUMMARY.md
          echo "## Job Status" >> BENCHMARK_SUMMARY.md
          echo "- Benchmark job: ${{ needs.benchmark.result }}" >> BENCHMARK_SUMMARY.md

      - name: Upload summary
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-summary
          path: BENCHMARK_SUMMARY.md
          retention-days: 90