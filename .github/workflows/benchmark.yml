---
name: Benchmarks

on:
  schedule:
    - cron: '0 2 * * 1'  # Weekly on Monday at 2 AM UTC
  workflow_dispatch:
    inputs:
      suite:
        description: 'Benchmark suite (basic/extended/all)'
        required: false
        type: choice
        options:
          - basic
          - extended
          - all
        default: basic
      include_large:
        description: 'Include large dataset benchmarks'
        required: false
        type: boolean
        default: false

concurrency:
  group: benchmark-${{ github.ref }}
  cancel-in-progress: true

env:
  PYTHONUNBUFFERED: "1"
  OMP_NUM_THREADS: "4"
  MKL_NUM_THREADS: "4"
  JAX_PLATFORMS: cpu
  JAX_ENABLE_X64: "true"

jobs:
  benchmark:
    name: Run Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 30
    steps:
      - uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"
          cache: pip

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -e ".[benchmark]"

      - name: System information
        run: |
          python -c "import sys, numpy, jax, psutil; print(f'Python: {sys.version}'); print(f'NumPy: {numpy.__version__}'); print(f'JAX: {jax.__version__}'); print(f'CPU cores: {psutil.cpu_count()}'); print(f'Memory: {psutil.virtual_memory().total / (1024**3):.2f} GB')"

      - name: Run performance benchmarks
        run: |
          SUITE="${{ github.event.inputs.suite || 'basic' }}"
          python benchmark/run_benchmarks.py --quick --output benchmark-results.json

      - name: Run regression tests
        continue-on-error: true
        run: |
          pytest benchmark/test_performance_regression.py -v --tb=short

      - name: Generate report
        if: always()
        run: |
          if [ -f benchmark-results.json ]; then
            echo "# Benchmark Results" > benchmark-report.md
            echo "" >> benchmark-report.md
            echo "Benchmark completed successfully." >> benchmark-report.md
            cat benchmark-results.json >> benchmark-report.md
          fi

      - name: Upload benchmark results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: |
            benchmark-results.json
            benchmark-report.md
          retention-days: 90

      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        continue-on-error: true
        with:
          script: |
            const fs = require('fs');
            if (fs.existsSync('benchmark-report.md')) {
              const report = fs.readFileSync('benchmark-report.md', 'utf8');
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: '## ðŸ“Š Benchmark Results\n\n' + report
              });
            }
