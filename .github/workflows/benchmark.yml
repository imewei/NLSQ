---
name: Benchmark Suite

'on':
  schedule:
    - cron: '0 2 * * 1'  # Weekly on Monday at 2 AM
  workflow_dispatch:
    inputs:
      run_large:
        description: 'Run large dataset benchmarks'
        required: false
        type: boolean
        default: false
      benchmark_suite:
        description: 'Benchmark suite to run'
        required: false
        type: choice
        options:
          - 'basic'
          - 'extended'
          - 'all'
        default: 'basic'
  push:
    paths:
      - 'nlsq/**'
      - 'benchmark/**'
      - '.github/workflows/benchmark.yml'
    branches: [main, develop]

concurrency:
  group: benchmark-${{ github.ref }}
  cancel-in-progress: true

env:
  # Performance optimizations
  OMP_NUM_THREADS: 4
  MKL_NUM_THREADS: 4
  OPENBLAS_NUM_THREADS: 4
  JAX_PLATFORMS: cpu
  JAX_ENABLE_X64: true  # Enable 64-bit precision for NLSQ
  PYTHONHASHSEED: 0
  PYTHONIOENCODING: utf-8
  PIP_DISABLE_PIP_VERSION_CHECK: 1
  PIP_NO_CACHE_DIR: 1  # Disable pip caching to avoid issues

jobs:
  # Primary benchmarking job
  benchmark:
    runs-on: ${{ matrix.os }}
    timeout-minutes: 45
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest]
        python-version: ['3.12']
        benchmark-type: ['performance', 'memory']
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 1  # Shallow clone for speed

      - name: Free disk space (Ubuntu)
        if: runner.os == 'Linux'
        run: |
          sudo rm -rf /usr/share/dotnet /opt/ghc /usr/local/share/boost "$AGENT_TOOLSDIRECTORY"
          sudo apt-get clean
          df -h

      - name: Free disk space (Windows)
        if: runner.os == 'Windows'
        run: |
          Get-WmiObject -Class Win32_LogicalDisk | Select-Object DeviceID,Size,FreeSpace
        shell: powershell

      - name: Free disk space (macOS)
        if: runner.os == 'macOS'
        run: |
          df -h
          brew cleanup --prune=all || true

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip setuptools wheel
          pip install -e ".[benchmark]" || pip install -e ".[all]"
          pip install memory_profiler psutil

      - name: System information
        run: |
          python -c "import sys; print(f'Python: {sys.version}')"
          python -c "import numpy; print(f'NumPy: {numpy.__version__}')"
          python -c "import jax; print(f'JAX: {jax.__version__}'); print(f'JAX devices: {jax.devices()}')"
          python -c "import psutil; print(f'CPU cores: {psutil.cpu_count()}'); print(f'Memory: {psutil.virtual_memory().total / (1024**3):.2f} GB')"

      - name: Run performance benchmarks
        if: matrix.benchmark-type == 'performance'
        run: |
          # Create results directory
          mkdir -p benchmark_results

          # Determine suite based on inputs - Match `make benchmark`
          SUITE="${{ github.event.inputs.benchmark_suite }}"
          if [ -z "$SUITE" ] || [ "$SUITE" == "null" ]; then
            SUITE="basic"
          fi

          echo "Running $SUITE benchmark suite..."
          timeout 1800 python benchmark/benchmark.py \
            --suite "$SUITE" \
            --save \
            --output-dir benchmark_results \
            --format markdown \
            --timeout 300 \
            > benchmark_results/performance_results.md || echo "Some benchmarks timed out"

      - name: Run large dataset benchmarks
        if: |
          (github.event.inputs.run_large == 'true' || github.event_name == 'schedule') &&
          matrix.benchmark-type == 'performance'
        run: |
          # Match `make benchmark-large`
          echo "Running large dataset benchmarks..."
          timeout 1800 python benchmark/benchmark.py \
            --large-datasets \
            --save \
            --output-dir benchmark_results \
            --format markdown \
            --timeout 600 \
            >> benchmark_results/performance_results.md || echo "Large benchmarks timed out"

      - name: Profile memory usage
        if: matrix.benchmark-type == 'memory'
        run: |
          # Match `make profile`
          echo "Profiling memory usage..."
          mkdir -p benchmark_results
          python -m memory_profiler benchmark/benchmark.py \
            --memory-profile \
            > benchmark_results/memory_profile.md || echo "Memory profiling completed with warnings"

      - name: Generate benchmark report
        run: |
          # Create comprehensive report
          mkdir -p benchmark_results

          # Get system information
          CPU_INFO=$(python -c "import psutil; print(f'{psutil.cpu_count()} cores')")
          MEM_INFO=$(python -c "import psutil; print(f'{psutil.virtual_memory().total / (1024**3):.2f} GB')")
          OS_INFO="${{ matrix.os }}"

          cat > benchmark_results/BENCHMARK_REPORT.md << EOF
          # Benchmark Report - $(date -u '+%Y-%m-%d %H:%M:%S UTC')

          ## System Information
          - **OS**: ${OS_INFO}
          - **Python**: ${{ matrix.python-version }}
          - **Benchmark Type**: ${{ matrix.benchmark-type }}
          - **CPU**: ${CPU_INFO}
          - **Memory**: ${MEM_INFO}
          - **JAX Platform**: ${{ env.JAX_PLATFORMS }}
          - **OMP Threads**: ${{ env.OMP_NUM_THREADS }}

          ## Benchmark Configuration
          - **Suite**: ${{ github.event.inputs.benchmark_suite || 'basic' }}
          - **Large Datasets**: ${{ github.event.inputs.run_large || 'false' }}
          - **Trigger**: ${{ github.event_name }}
          - **Branch**: ${{ github.ref }}
          - **Commit**: ${{ github.sha }}

          ---

          EOF

          # Append performance results if they exist
          if [ -f "benchmark_results/performance_results.md" ]; then
            echo "## Performance Results" >> benchmark_results/BENCHMARK_REPORT.md
            cat benchmark_results/performance_results.md >> benchmark_results/BENCHMARK_REPORT.md
            echo "" >> benchmark_results/BENCHMARK_REPORT.md
          fi

          # Append memory profile if it exists
          if [ -f "benchmark_results/memory_profile.md" ]; then
            echo "## Memory Profile" >> benchmark_results/BENCHMARK_REPORT.md
            cat benchmark_results/memory_profile.md >> benchmark_results/BENCHMARK_REPORT.md
          fi

          # Add comparison with baseline if on main branch
          if [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
            echo "## Performance Baseline Comparison" >> benchmark_results/BENCHMARK_REPORT.md
            echo "Baseline comparison would go here if configured" >> benchmark_results/BENCHMARK_REPORT.md
          fi
        shell: bash

      - name: Upload benchmark artifacts
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-${{ matrix.benchmark-type }}-${{ matrix.os }}-py${{ matrix.python-version }}
          path: benchmark_results/
          retention-days: 30

      - name: Performance regression check
        if: |
          matrix.benchmark-type == 'performance' &&
          github.ref == 'refs/heads/main'
        continue-on-error: true
        run: |
          # Simple performance regression detection
          if [ -f "benchmark_results/performance_results.md" ]; then
            echo "Checking for performance regressions..."
            # Extract timing information and compare with baseline
            grep -E "Time:|seconds|ms" benchmark_results/performance_results.md || echo "No timing data found"
          fi

      - name: Comment PR with benchmark results
        if: |
          github.event_name == 'pull_request' &&
          matrix.benchmark-type == 'performance' &&
          matrix.os == 'ubuntu-latest'
        uses: actions/github-script@v7
        continue-on-error: true
        with:
          script: |
            const fs = require('fs');
            const path = 'benchmark_results/BENCHMARK_REPORT.md';
            if (fs.existsSync(path)) {
              const report = fs.readFileSync(path, 'utf8');
              // Truncate if too long
              const truncated = report.length > 65536 ?
                report.substring(0, 65000) + '\n\n... (truncated)' : report;

              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: '## ðŸ“Š Benchmark Results\n\n' + truncated
              });
            }

  # Summary job
  benchmark-summary:
    runs-on: ubuntu-latest
    needs: [benchmark]
    if: always()

    steps:
      - name: Download all artifacts
        uses: actions/download-artifact@v4
        continue-on-error: true

      - name: Generate summary
        run: |
          echo "# Benchmark Summary" > $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Workflow**: ${{ github.workflow }}" >> $GITHUB_STEP_SUMMARY
          echo "**Trigger**: ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
          echo "**Date**: $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
          echo "**Status**: ${{ needs.benchmark.result }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # List all benchmark results
          echo "## Available Results" >> $GITHUB_STEP_SUMMARY
          find . -name "BENCHMARK_REPORT.md" -exec echo "- {}" \; >> $GITHUB_STEP_SUMMARY || echo "No reports found" >> $GITHUB_STEP_SUMMARY

          # Also create artifact
          cp $GITHUB_STEP_SUMMARY BENCHMARK_SUMMARY.md || true

      - name: Upload summary
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-summary
          path: BENCHMARK_SUMMARY.md
          retention-days: 90
