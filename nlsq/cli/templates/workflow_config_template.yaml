# ==============================================================================
# NLSQ Workflow Configuration Template
# ==============================================================================
# Version: 5.0 (NLSQ v0.6.3 - 3-Workflow System)
#
# This template provides configuration options for NLSQ curve fitting workflows.
# Copy this file to your project and customize as needed.
#
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ CLI USAGE                                                                   │
# ├─────────────────────────────────────────────────────────────────────────────┤
# │ Single fit:    nlsq fit workflow.yaml                                       │
# │ Batch fits:    nlsq batch w1.yaml w2.yaml w3.yaml                           │
# │                nlsq batch configs/*.yaml                                    │
# │ System info:   nlsq info                                                    │
# │ Get template:  nlsq config --workflow                                       │
# └─────────────────────────────────────────────────────────────────────────────┘
#
# Quick Start:
#   1. Copy this file: cp workflow_config_template.yaml my_workflow.yaml
#   2. Configure: data.input_file, data.columns, model.type, model.name
#   3. Run: nlsq fit my_workflow.yaml
#
# Minimal Configuration:
#   data:
#     input_file: "data/experiment.csv"
#     columns: { x: 0, y: 1 }
#   model:
#     type: "builtin"
#     name: "exponential_decay"
#     auto_p0: true
#   export:
#     results_file: "results.json"
#
# Documentation: https://nlsq.readthedocs.io/
# ==============================================================================

# ==============================================================================
# 3-WORKFLOW SYSTEM (v0.6.3)
# ==============================================================================
#
# NLSQ uses a simplified 3-workflow system that automatically selects optimal
# processing strategies based on dataset size and available memory.
#
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ WORKFLOW OPTIONS                                                            │
# ├─────────────────────────────────────────────────────────────────────────────┤
# │                                                                             │
# │ workflow: "auto" (DEFAULT)                                                  │
# │   Memory-aware local optimization                                           │
# │   - Automatic strategy: direct fit → chunked → streaming based on size     │
# │   - Single starting point (uses p0 or auto-estimated initial guess)        │
# │   - Best for: unimodal problems, good initial guesses, speed priority      │
# │                                                                             │
# │ workflow: "auto_global"                                                     │
# │   Memory-aware global optimization (REQUIRES BOUNDS)                        │
# │   - Multi-start Latin Hypercube Sampling or CMA-ES                          │
# │   - Automatic strategy selection based on parameter scale                   │
# │   - Use n_starts to control number of starting points (default: 10)        │
# │   - Best for: multi-modal problems, unknown initial guesses                 │
# │                                                                             │
# │ workflow: "hpc"                                                             │
# │   auto_global + checkpointing for HPC environments                          │
# │   - Fault-tolerant with automatic resume capability                         │
# │   - Multi-GPU/node support via PBS_NODEFILE or SLURM_JOB_NODELIST          │
# │   - Best for: cluster computing, very large datasets, long-running fits    │
# │                                                                             │
# └─────────────────────────────────────────────────────────────────────────────┘
#
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ COMMON PATTERNS                                                             │
# ├─────────────────────────────────────────────────────────────────────────────┤
# │                                                                             │
# │ Local optimization (fast, single start):                                    │
# │   workflow: "auto"                                                          │
# │                                                                             │
# │ Global optimization (multi-start):                                          │
# │   workflow: "auto_global"                                                   │
# │   n_starts: 10                                                              │
# │   model:                                                                    │
# │     auto_bounds: true  # or define bounds in parameters                     │
# │                                                                             │
# │ High-precision global search:                                               │
# │   workflow: "auto_global"                                                   │
# │   n_starts: 20                                                              │
# │   fitting:                                                                  │
# │     termination:                                                            │
# │       gtol: 1.0e-10                                                         │
# │       ftol: 1.0e-10                                                         │
# │                                                                             │
# │ HPC cluster with checkpointing:                                             │
# │   workflow: "hpc"                                                           │
# │   n_starts: 20                                                              │
# │   checkpointing:                                                            │
# │     enabled: true                                                           │
# │     directory: "checkpoints"                                                │
# │                                                                             │
# └─────────────────────────────────────────────────────────────────────────────┘
#
# ==============================================================================

metadata:
  workflow_name: "example_workflow"
  description: "NLSQ workflow configuration"
  version: "5.0"
  author: ""

# ==============================================================================
# WORKFLOW SELECTION
# ==============================================================================

# Primary workflow selection: "auto", "auto_global", or "hpc"
workflow: "auto"

# Number of starting points for global optimization (auto_global, hpc only)
# Ignored when workflow="auto"
n_starts: 10

# Sampling method for multi-start: "lhs" (Latin Hypercube), "sobol", "halton"
sampler: "lhs"

# Memory limit in GB for automatic strategy selection (null = auto-detect)
memory_limit_gb: null

# ==============================================================================
# DATA
# ==============================================================================
# Dataset configuration for CLI data loading.
# Supports: CSV (.csv), ASCII (.txt, .dat), NPZ (.npz), HDF5 (.h5, .hdf5)

data:
  # Path to data file (required for CLI)
  input_file: "data/experiment.csv"

  # File format: "auto" (from extension), "csv", "ascii", "npz", "hdf5"
  format: "auto"

  # Column selection (0-based index or column name for CSV with header)
  columns:
    x: 0              # Independent variable
    y: 1              # Dependent variable
    z: null           # Optional: enables 2D surface fitting
    sigma: null       # Optional: measurement uncertainties

  # CSV options
  csv:
    delimiter: ","
    header: true
    encoding: "utf-8"

  # ASCII options (.txt, .dat)
  ascii:
    delimiter: null   # null = any whitespace
    comment_char: "#"
    skip_header: 0

  # NPZ options
  npz:
    x_key: "x"
    y_key: "y"
    z_key: null
    sigma_key: null

  # HDF5 options
  hdf5:
    x_path: "/data/x"
    y_path: "/data/y"
    z_path: null
    sigma_path: null

  # Validation
  validation:
    require_finite: true
    min_points: 2

# ==============================================================================
# MODEL
# ==============================================================================
# Model function configuration.
#
# Types:
#   "builtin"    - Use nlsq.functions library
#   "custom"     - Load from external Python file
#   "polynomial" - Generate polynomial of specified degree
#
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ BUILTIN MODELS                                                              │
# ├─────────────────────────────────────────────────────────────────────────────┤
# │ linear             │ a*x + b                     │ a, b                     │
# │ exponential_decay  │ a*exp(-b*x) + c             │ a, b, c                  │
# │ exponential_growth │ a*exp(b*x) + c              │ a, b, c                  │
# │ gaussian           │ amp*exp(-(x-mu)²/(2σ²))     │ amp, mu, sigma           │
# │ sigmoid            │ L/(1+exp(-k*(x-x0))) + b    │ L, x0, k, b              │
# │ power_law          │ a*x^b                       │ a, b                     │
# └─────────────────────────────────────────────────────────────────────────────┘

model:
  # Model type: "builtin", "custom", "polynomial"
  type: "builtin"

  # Builtin model name (when type: "builtin")
  name: "exponential_decay"

  # Automatic initial parameter estimation
  auto_p0: true

  # Automatic bounds from model defaults (required for auto_global/hpc)
  auto_bounds: false

  # Custom model configuration (when type: "custom")
  custom:
    file: "models.py"
    function: "my_model"

  # Polynomial configuration (when type: "polynomial")
  polynomial:
    degree: 3

  # Manual parameter configuration
  # Required when auto_p0=false or auto_bounds=false
  # IMPORTANT: Bounds are REQUIRED for workflow="auto_global" or "hpc"
  parameters:
    - name: "amplitude"
      initial: 1.0
      bounds: [0.0, 10.0]
      fixed: false
    - name: "decay_rate"
      initial: 0.1
      bounds: [0.001, 1.0]
      fixed: false
    - name: "offset"
      initial: 0.0
      bounds: [-1.0, 1.0]
      fixed: false

# ==============================================================================
# FITTING
# ==============================================================================
# Core fitting algorithm configuration.

fitting:
  # Convergence tolerances
  termination:
    gtol: 1.0e-8       # Gradient tolerance
    ftol: 1.0e-8       # Function tolerance
    xtol: 1.0e-8       # Parameter tolerance
    max_iterations: 200
    max_function_evals: 2000

  # Robust loss function: "none", "huber", "soft_l1", "cauchy"
  robust_loss: "none"

  # Jacobian computation: "auto" uses JAX autodiff (recommended)
  jacobian_mode: "auto"

# ==============================================================================
# GLOBAL OPTIMIZATION (for workflow="auto_global" or "hpc")
# ==============================================================================
# Multi-start and CMA-ES configuration.

global_optimization:
  # Multi-start sampling options
  center_on_p0: true       # Center samples around initial guess
  scale_factor: 1.0        # Exploration region scale

  # CMA-ES configuration (used when bounds span > 3 orders of magnitude)
  cmaes:
    # Preset: "cmaes-fast" (50 gens), "cmaes" (100 gens), "cmaes-global" (200 gens)
    preset: null           # null = use settings below

    popsize: null          # null = automatic (4 + 3*log(n_params))
    max_generations: 100
    sigma: 0.5             # Initial step size
    restart_strategy: "bipop"  # "none" or "bipop"
    max_restarts: 9
    refine_with_nlsq: true # Refine best solution with TRF

    # Tolerances
    tol_fun: 1.0e-8
    tol_x: 1.0e-8

    # Reproducibility
    seed: null             # null = random

# ==============================================================================
# CHECKPOINTING (for workflow="hpc" or large datasets)
# ==============================================================================

checkpointing:
  enabled: false
  directory: "checkpoints"
  frequency: 100           # Save every N iterations

# ==============================================================================
# RUNTIME
# ==============================================================================
# Execution environment settings.

runtime:
  # Device: "cpu" or "gpu" (GPU requires Linux + NVIDIA + CUDA 12.x)
  device: "cpu"

  # Numerical precision: "float64" (recommended) or "float32"
  precision: "float64"

  # Random seed for reproducibility
  random_seed: 42

  # Verbosity: 0 (silent), 1 (progress), 2 (detailed), 3 (debug)
  verbosity: 1

  # Parallel workers
  threads: 4

# ==============================================================================
# STREAMING OPTIMIZER (Advanced)
# ==============================================================================
# Configuration for large dataset processing.
# Automatically used by "auto" and "auto_global" when datasets exceed memory.

streaming:
  # Chunk size (points per batch)
  chunk_size: 10000

  # Parameter normalization: "auto", "bounds", "p0", "none"
  normalization_strategy: "auto"

  # L-BFGS warmup phase
  warmup_iterations: 200
  max_warmup_iterations: 500

  # Defense layers prevent divergence when starting near optimum
  defense_layers:
    # Preset: "default", "strict", "relaxed", "disabled", "scientific"
    preset: "default"

    # Individual layer configuration (when preset: null)
    # layer1_warm_start:
    #   enabled: true
    #   threshold: 0.01    # Skip warmup if loss < 1% of variance
    # layer2_adaptive_lr:
    #   enabled: true
    # layer3_cost_guard:
    #   enabled: true
    #   tolerance: 0.05    # Abort if loss increases > 5%
    # layer4_step_clipping:
    #   enabled: true
    #   max_step_size: 0.1

  # Telemetry for monitoring
  telemetry:
    enabled: false
    export_format: "json"  # "json", "prometheus"

# ==============================================================================
# EXPORT
# ==============================================================================
# Output configuration.

export:
  # Results file path (required for CLI)
  results_file: "output/fit_results.json"

  # What to include in results
  include:
    parameters: true
    covariance: true
    uncertainties: true
    statistics: true
    residuals: false       # Can be large for big datasets
    fitted_values: false

  # Output formats
  formats:
    - "json"
    - "csv"

# ==============================================================================
# VISUALIZATION
# ==============================================================================
# Publication-quality figure generation.

visualization:
  enabled: true
  output_dir: "figures"

  # Style: "publication", "presentation", "nature", "science", "minimal"
  style: "publication"

  # Resolution
  dpi: 300
  figure_size: [6.0, 4.5]

  # Color scheme: "default", "colorblind", "grayscale", "high_contrast"
  color_scheme: "default"

  # Output formats
  formats:
    - "pdf"
    - "png"

  # Plot components
  main_plot:
    show_grid: true
    confidence_band:
      enabled: true
      level: 0.95

  residuals_plot:
    enabled: true

  histogram:
    enabled: true

# ==============================================================================
# BATCH PROCESSING
# ==============================================================================
# Configuration for `nlsq batch` command.

batch:
  max_workers: null        # null = auto-detect
  continue_on_error: true
  summary_file: "output/batch_summary.json"

# ==============================================================================
# LOGGING
# ==============================================================================

logging:
  log_file: "logs/workflow.log"
  console: true
  log_level: "INFO"
  rotation:
    enabled: true
    max_bytes: 10485760    # 10 MB
    backup_count: 5
