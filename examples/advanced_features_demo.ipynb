{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLSQ Advanced Features Demo\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Dipolar-Quantum-Gases/nlsq/blob/main/examples/advanced_features_demo.ipynb)\n",
    "\n",
    "This notebook demonstrates NLSQ's advanced features including:\n",
    "\n",
    "- **Diagnostics & Monitoring**: Real-time optimization diagnostics and convergence analysis\n",
    "- **Recovery Mechanisms**: Automatic recovery from optimization failures\n",
    "- **Stability Analysis**: Numerical stability testing and robust algorithms\n",
    "- **Smart Caching**: Intelligent caching for repeated optimizations\n",
    "- **Validation & Robustness**: Input validation and robust decomposition methods\n",
    "- **Algorithm Selection**: Intelligent algorithm selection based on problem characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install NLSQ if not already installed\n",
    "!pip install nlsq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any, List, Optional, Tuple\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Import NLSQ core functionality\n",
    "from nlsq import CurveFit, curve_fit, LeastSquares, OptimizeResult\n",
    "\n",
    "# Import advanced features - note: some may not be available in all versions\n",
    "try:\n",
    "    from nlsq import (\n",
    "        AlgorithmSelector,\n",
    "        auto_select_algorithm,\n",
    "        MemoryConfig,\n",
    "        get_memory_config,\n",
    "        memory_context,\n",
    "        estimate_memory_requirements\n",
    "    )\n",
    "    ADVANCED_FEATURES_AVAILABLE = True\n",
    "    print(\"‚úÖ Advanced features imported successfully\")\n",
    "except ImportError as e:\n",
    "    ADVANCED_FEATURES_AVAILABLE = False\n",
    "    print(f\"‚ö†Ô∏è  Some advanced features not available: {e}\")\n",
    "    print(\"Continuing with available features...\")\n",
    "\n",
    "print(f\"JAX devices: {jax.devices()}\")\n",
    "print(f\"JAX version: {jax.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Functions for Testing\n",
    "\n",
    "We'll define several test functions with different characteristics to demonstrate the advanced features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def well_conditioned_model(x, a, b, c):\n",
    "    \"\"\"Well-conditioned exponential decay model.\"\"\"\n",
    "    return a * jnp.exp(-b * x) + c\n",
    "\n",
    "\n",
    "def ill_conditioned_model(x, a, b, c, d, e):\n",
    "    \"\"\"Ill-conditioned model with highly correlated parameters.\"\"\"\n",
    "    return a * jnp.exp(-b * x) + c * jnp.exp(-b * x * 1.001) + d * x + e\n",
    "\n",
    "\n",
    "def oscillatory_model(x, a, b, c, d, e):\n",
    "    \"\"\"Oscillatory model that can be challenging to fit.\"\"\"\n",
    "    return a * jnp.exp(-b * x) * jnp.cos(c * x + d) + e\n",
    "\n",
    "\n",
    "def multi_peak_gaussian(x, a1, mu1, s1, a2, mu2, s2, offset):\n",
    "    \"\"\"Multi-peak Gaussian model for complex fitting scenarios.\"\"\"\n",
    "    g1 = a1 * jnp.exp(-((x - mu1) ** 2) / (2 * s1 ** 2))\n",
    "    g2 = a2 * jnp.exp(-((x - mu2) ** 2) / (2 * s2 ** 2))\n",
    "    return g1 + g2 + offset\n",
    "\n",
    "\n",
    "def problematic_model(x, a, b, c):\n",
    "    \"\"\"Model designed to cause optimization difficulties.\"\"\"\n",
    "    # This model has a very flat region that can cause convergence issues\n",
    "    return a * jnp.tanh(b * (x - c)) + jnp.where(jnp.abs(x - c) < 0.1, 0.0, jnp.sin(10 * x) * 0.01)\n",
    "\n",
    "print(\"‚úÖ Test models defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Diagnostics and Monitoring\n",
    "\n",
    "NLSQ provides detailed diagnostics about the optimization process, helping you understand convergence behavior and identify potential issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_diagnostic_data():\n    \"\"\"Create test datasets with different characteristics.\"\"\"\n    np.random.seed(42)\n    x = np.linspace(0, 5, 1000)\n    \n    datasets = {\n        'clean': {\n            'x': x,\n            'y': np.array(well_conditioned_model(x, 5.0, 1.2, 0.5)) + np.random.normal(0, 0.01, len(x)),\n            'true_params': [5.0, 1.2, 0.5],\n            'model': well_conditioned_model\n        },\n        'noisy': {\n            'x': x,\n            'y': np.array(well_conditioned_model(x, 5.0, 1.2, 0.5)) + np.random.normal(0, 0.2, len(x)),\n            'true_params': [5.0, 1.2, 0.5],\n            'model': well_conditioned_model\n        },\n        'outliers': {\n            'x': x,\n            'y': np.array(well_conditioned_model(x, 5.0, 1.2, 0.5)) + np.random.normal(0, 0.05, len(x)),\n            'true_params': [5.0, 1.2, 0.5],\n            'model': well_conditioned_model\n        },\n        'ill_conditioned': {\n            'x': x,\n            'y': np.array(ill_conditioned_model(x, 3.0, 1.0, 2.0, 0.1, 0.8)) + np.random.normal(0, 0.05, len(x)),\n            'true_params': [3.0, 1.0, 2.0, 0.1, 0.8],\n            'model': ill_conditioned_model\n        }\n    }\n    \n    # Add some outliers to the outlier dataset - now safe since y is a numpy array\n    outlier_indices = np.random.choice(len(x), 50, replace=False)\n    datasets['outliers']['y'][outlier_indices] += np.random.normal(0, 1.0, 50)\n    \n    return datasets\n\n\ndef analyze_fit_quality(result, true_params, dataset_name):\n    \"\"\"Analyze the quality of a fit result.\"\"\"\n    print(f\"\\n=== {dataset_name.upper()} DATASET ANALYSIS ===\")\n    \n    if hasattr(result, 'success') and not result.success:\n        print(f\"‚ùå Optimization failed: {result.message if hasattr(result, 'message') else 'Unknown error'}\")\n        return\n    \n    # Extract parameters (handle different result formats)\n    if isinstance(result, tuple):\n        popt, pcov = result\n        success = True\n        nfev = None\n    else:\n        popt = result.x if hasattr(result, 'x') else result\n        pcov = getattr(result, 'pcov', None)\n        success = getattr(result, 'success', True)\n        nfev = getattr(result, 'nfev', None)\n    \n    if success:\n        # Parameter accuracy\n        errors = np.abs(popt - np.array(true_params))\n        rel_errors = errors / np.abs(np.array(true_params)) * 100\n        \n        print(f\"‚úÖ Optimization successful\")\n        if nfev:\n            print(f\"Function evaluations: {nfev}\")\n        print(f\"True parameters:   {true_params}\")\n        print(f\"Fitted parameters: {list(popt)}\")\n        print(f\"Absolute errors:   {list(errors)}\")\n        print(f\"Relative errors:   {[f'{e:.2f}%' for e in rel_errors]}\")\n        \n        # Parameter uncertainties\n        if pcov is not None:\n            param_std = np.sqrt(np.diag(pcov))\n            print(f\"Parameter std dev: {list(param_std)}\")\n            \n            # Condition number of covariance matrix\n            cond_number = np.linalg.cond(pcov)\n            print(f\"Covariance condition number: {cond_number:.2e}\")\n            \n            if cond_number > 1e12:\n                print(\"‚ö†Ô∏è  High condition number - parameters may be poorly determined\")\n            elif cond_number > 1e8:\n                print(\"‚ö†Ô∏è  Moderate condition number - some parameter correlation\")\n            else:\n                print(\"‚úÖ Good condition number - well-determined parameters\")\n    else:\n        print(f\"‚ùå Optimization failed\")\n\n\n# Create test datasets\ndatasets = create_diagnostic_data()\nprint(f\"‚úÖ Created {len(datasets)} test datasets\")\n\n# Visualize the datasets\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\naxes = axes.flatten()\n\nfor i, (name, data) in enumerate(datasets.items()):\n    axes[i].plot(data['x'], data['y'], 'b.', alpha=0.6, markersize=1)\n    axes[i].set_title(f\"{name.title()} Dataset\")\n    axes[i].set_xlabel('x')\n    axes[i].set_ylabel('y')\n    axes[i].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprehensive Fitting Analysis\n",
    "\n",
    "Let's fit all datasets and analyze the results to demonstrate diagnostic capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprehensive_fitting_analysis():\n",
    "    \"\"\"Perform comprehensive fitting analysis with diagnostics.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"COMPREHENSIVE FITTING ANALYSIS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Initialize fitter\n",
    "    cf = CurveFit()\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for dataset_name, data in datasets.items():\n",
    "        print(f\"\\nProcessing {dataset_name} dataset...\")\n",
    "        \n",
    "        # Initial parameter guess (slightly off from true values)\n",
    "        p0 = [p * np.random.uniform(0.8, 1.2) for p in data['true_params']]\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Perform fit\n",
    "            popt, pcov = cf.curve_fit(\n",
    "                data['model'],\n",
    "                data['x'],\n",
    "                data['y'],\n",
    "                p0=p0\n",
    "            )\n",
    "            \n",
    "            fit_time = time.time() - start_time\n",
    "            \n",
    "            # Create result object for analysis\n",
    "            result = type('Result', (), {\n",
    "                'x': popt,\n",
    "                'pcov': pcov,\n",
    "                'success': True,\n",
    "                'fit_time': fit_time\n",
    "            })()\n",
    "            \n",
    "            results[dataset_name] = result\n",
    "            \n",
    "            # Analyze fit quality\n",
    "            analyze_fit_quality((popt, pcov), data['true_params'], dataset_name)\n",
    "            print(f\"Fit time: {fit_time:.4f} seconds\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Fitting failed for {dataset_name}: {e}\")\n",
    "            results[dataset_name] = None\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run comprehensive analysis\n",
    "fit_results = comprehensive_fitting_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Algorithm Selection and Optimization Strategies\n",
    "\n",
    "NLSQ can automatically select the best algorithm based on problem characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_algorithm_selection():\n",
    "    \"\"\"Demonstrate intelligent algorithm selection.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"ALGORITHM SELECTION DEMONSTRATION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    if not ADVANCED_FEATURES_AVAILABLE:\n",
    "        print(\"‚ö†Ô∏è  Advanced algorithm selection not available in this version\")\n",
    "        print(\"Demonstrating basic algorithm comparison instead...\")\n",
    "        \n",
    "        # Basic comparison of different approaches\n",
    "        algorithms = ['trf', 'lm']  # Available algorithms\n",
    "        test_data = datasets['clean']\n",
    "        \n",
    "        print(f\"\\nTesting algorithms on clean dataset:\")\n",
    "        for alg in algorithms:\n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                # Note: actual method parameter may vary\n",
    "                cf = CurveFit()  # Some versions may accept method parameter\n",
    "                popt, pcov = cf.curve_fit(\n",
    "                    test_data['model'],\n",
    "                    test_data['x'],\n",
    "                    test_data['y'],\n",
    "                    p0=[4.0, 1.0, 0.4]\n",
    "                )\n",
    "                fit_time = time.time() - start_time\n",
    "                \n",
    "                errors = np.abs(popt - np.array(test_data['true_params']))\n",
    "                max_rel_error = np.max(errors / np.abs(np.array(test_data['true_params']))) * 100\n",
    "                \n",
    "                print(f\"  {alg.upper():3s}: {fit_time:.4f}s, max error: {max_rel_error:.2f}%\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"  {alg.upper():3s}: Failed - {e}\")\n",
    "        \n",
    "        return\n",
    "    \n",
    "    # Advanced algorithm selection (if available)\n",
    "    test_cases = [\n",
    "        ('Well-conditioned', datasets['clean']),\n",
    "        ('Noisy', datasets['noisy']),\n",
    "        ('Ill-conditioned', datasets['ill_conditioned'])\n",
    "    ]\n",
    "    \n",
    "    for case_name, data in test_cases:\n",
    "        print(f\"\\n--- {case_name} Case ---\")\n",
    "        \n",
    "        try:\n",
    "            # Get algorithm recommendation\n",
    "            sample_size = min(1000, len(data['x']))\n",
    "            x_sample = data['x'][:sample_size]\n",
    "            y_sample = data['y'][:sample_size]\n",
    "            \n",
    "            recommendations = auto_select_algorithm(data['model'], x_sample, y_sample)\n",
    "            \n",
    "            print(f\"Recommended algorithm: {recommendations.get('algorithm', 'trf')}\")\n",
    "            print(f\"Recommended tolerance: {recommendations.get('ftol', 1e-8)}\")\n",
    "            \n",
    "            # Additional recommendations if available\n",
    "            for key, value in recommendations.items():\n",
    "                if key not in ['algorithm', 'ftol']:\n",
    "                    print(f\"{key}: {value}\")\n",
    "            \n",
    "            # Test the recommendation\n",
    "            cf_optimized = CurveFit()  # Use recommended settings if supported\n",
    "            start_time = time.time()\n",
    "            \n",
    "            p0 = [p * 0.9 for p in data['true_params']]  # Slightly off initial guess\n",
    "            popt, pcov = cf_optimized.curve_fit(\n",
    "                data['model'],\n",
    "                data['x'],\n",
    "                data['y'],\n",
    "                p0=p0,\n",
    "                ftol=recommendations.get('ftol', 1e-8)\n",
    "            )\n",
    "            \n",
    "            fit_time = time.time() - start_time\n",
    "            \n",
    "            # Analyze results\n",
    "            errors = np.abs(popt - np.array(data['true_params']))\n",
    "            rel_errors = errors / np.abs(np.array(data['true_params'])) * 100\n",
    "            \n",
    "            print(f\"‚úÖ Optimized fit completed in {fit_time:.4f}s\")\n",
    "            print(f\"Max relative error: {np.max(rel_errors):.3f}%\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Algorithm selection failed: {e}\")\n",
    "\n",
    "# Run algorithm selection demo\n",
    "demonstrate_algorithm_selection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Robustness and Error Handling\n",
    "\n",
    "Test NLSQ's robustness with challenging datasets and error conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def test_robustness():\n    \"\"\"Test NLSQ's robustness with challenging scenarios.\"\"\"\n    print(\"=\" * 70)\n    print(\"ROBUSTNESS AND ERROR HANDLING TESTS\")\n    print(\"=\" * 70)\n    \n    cf = CurveFit()\n    \n    # Test 1: Bad initial guesses\n    print(\"\\n--- Test 1: Bad Initial Guesses ---\")\n    test_data = datasets['clean']\n    \n    bad_guesses = [\n        [100, 0.01, -10],    # Very different from true values\n        [0.01, 100, 100],    # Poor scaling\n        [-5, -1, -2],        # Wrong signs\n    ]\n    \n    for i, p0 in enumerate(bad_guesses):\n        try:\n            popt, pcov = cf.curve_fit(\n                test_data['model'],\n                test_data['x'],\n                test_data['y'],\n                p0=p0\n            )\n            \n            errors = np.abs(popt - np.array(test_data['true_params']))\n            max_error = np.max(errors / np.abs(np.array(test_data['true_params']))) * 100\n            \n            status = \"‚úÖ Recovered\" if max_error < 10 else \"‚ö†Ô∏è  Partial recovery\"\n            print(f\"  Bad guess {i+1}: {status} (max error: {max_error:.1f}%)\")\n            \n        except Exception as e:\n            print(f\"  Bad guess {i+1}: ‚ùå Failed - {type(e).__name__}\")\n    \n    # Test 2: Problematic model\n    print(\"\\n--- Test 2: Problematic Model ---\")\n    np.random.seed(123)\n    x_prob = np.linspace(-1, 1, 500)\n    y_prob = np.array(problematic_model(x_prob, 2.0, 5.0, 0.0)) + np.random.normal(0, 0.02, len(x_prob))\n    \n    try:\n        popt_prob, pcov_prob = cf.curve_fit(\n            problematic_model,\n            x_prob,\n            y_prob,\n            p0=[1.5, 4.0, 0.1]\n        )\n        \n        true_prob = [2.0, 5.0, 0.0]\n        errors_prob = np.abs(popt_prob - np.array(true_prob))\n        print(f\"‚úÖ Problematic model fitted successfully\")\n        print(f\"  Parameters: {popt_prob}\")\n        print(f\"  Errors: {errors_prob}\")\n        \n    except Exception as e:\n        print(f\"‚ùå Problematic model failed: {e}\")\n    \n    # Test 3: Extreme noise levels\n    print(\"\\n--- Test 3: Extreme Noise Levels ---\")\n    noise_levels = [0.5, 1.0, 2.0]  # Very high noise\n    \n    for noise in noise_levels:\n        try:\n            # Ensure we use numpy arrays to avoid JAX immutability issues\n            y_noisy = (np.array(test_data['model'](test_data['x'], *test_data['true_params'])) + \n                      np.random.normal(0, noise, len(test_data['x'])))\n            \n            popt_noisy, pcov_noisy = cf.curve_fit(\n                test_data['model'],\n                test_data['x'],\n                y_noisy,\n                p0=[4.0, 1.0, 0.4]\n            )\n            \n            errors_noisy = np.abs(popt_noisy - np.array(test_data['true_params']))\n            max_error_noisy = np.max(errors_noisy / np.abs(np.array(test_data['true_params']))) * 100\n            \n            status = (\"‚úÖ Good\" if max_error_noisy < 20 else \n                     \"‚ö†Ô∏è  Acceptable\" if max_error_noisy < 50 else \n                     \"‚ùå Poor\")\n            print(f\"  Noise level {noise}: {status} (max error: {max_error_noisy:.1f}%)\")\n            \n        except Exception as e:\n            print(f\"  Noise level {noise}: ‚ùå Failed - {type(e).__name__}\")\n    \n    # Test 4: Edge cases\n    print(\"\\n--- Test 4: Edge Cases ---\")\n    \n    edge_cases = [\n        (\"Very few points\", test_data['x'][:10], test_data['y'][:10]),\n        (\"Single x value\", np.array([1.0, 1.0, 1.0]), np.array([2.0, 2.1, 1.9])),\n        (\"Constant y values\", test_data['x'][:100], np.ones(100) * 2.5),\n    ]\n    \n    for case_name, x_edge, y_edge in edge_cases:\n        try:\n            popt_edge, pcov_edge = cf.curve_fit(\n                test_data['model'],\n                x_edge,\n                y_edge,\n                p0=[3.0, 1.0, 0.5]\n            )\n            print(f\"  {case_name}: ‚úÖ Handled gracefully\")\n            \n        except Exception as e:\n            print(f\"  {case_name}: ‚ùå Failed - {type(e).__name__}: {str(e)[:50]}...\")\n\n# Run robustness tests\ntest_robustness()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Memory Management and Performance Optimization\n",
    "\n",
    "Demonstrate NLSQ's advanced memory management capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def demonstrate_memory_management():\n    \"\"\"Demonstrate advanced memory management features.\"\"\"\n    print(\"=\" * 70)\n    print(\"MEMORY MANAGEMENT AND PERFORMANCE OPTIMIZATION\")\n    print(\"=\" * 70)\n    \n    if not ADVANCED_FEATURES_AVAILABLE:\n        print(\"‚ö†Ô∏è  Advanced memory management not available in this version\")\n        print(\"Demonstrating basic performance optimization instead...\")\n        \n        # Basic performance test\n        print(\"\\n--- Basic Performance Test ---\")\n        sizes = [1000, 10000, 50000]\n        \n        for size in sizes:\n            x_perf = np.linspace(0, 5, size)\n            y_perf = np.array(well_conditioned_model(x_perf, 5.0, 1.2, 0.5)) + np.random.normal(0, 0.05, size)\n            \n            cf = CurveFit()\n            start_time = time.time()\n            \n            try:\n                popt, pcov = cf.curve_fit(\n                    well_conditioned_model,\n                    x_perf,\n                    y_perf,\n                    p0=[4.0, 1.0, 0.4]\n                )\n                fit_time = time.time() - start_time\n                \n                # Estimate memory usage (rough)\n                estimated_memory = size * 8 * 4 / 1024 / 1024  # MB (rough estimate)\n                \n                print(f\"  {size:5d} points: {fit_time:.3f}s, ~{estimated_memory:.1f}MB\")\n                \n            except Exception as e:\n                print(f\"  {size:5d} points: ‚ùå Failed - {e}\")\n        \n        return\n    \n    # Advanced memory management\n    print(\"\\n--- Current Memory Configuration ---\")\n    current_config = get_memory_config()\n    print(f\"Memory limit: {current_config.memory_limit_gb} GB\")\n    print(f\"Mixed precision fallback: {current_config.enable_mixed_precision_fallback}\")\n    \n    # Test memory estimation\n    print(\"\\n--- Memory Estimation ---\")\n    test_sizes = [10000, 100000, 1000000, 10000000]\n    n_params = 5\n    \n    for size in test_sizes:\n        try:\n            stats = estimate_memory_requirements(size, n_params)\n            print(f\"{size:8,} points: {stats.total_memory_estimate_gb:.3f} GB, \"\n                  f\"chunks: {stats.n_chunks}, sampling: {stats.requires_sampling}\")\n        except Exception as e:\n            print(f\"{size:8,} points: Error - {e}\")\n    \n    # Test with different memory contexts\n    print(\"\\n--- Memory Context Testing ---\")\n    \n    # Generate moderately large dataset\n    np.random.seed(456)\n    large_size = 50000\n    x_large = np.linspace(0, 5, large_size)\n    y_large = np.array(well_conditioned_model(x_large, 5.0, 1.2, 0.5)) + np.random.normal(0, 0.05, large_size)\n    \n    memory_configs = [\n        (\"High memory\", MemoryConfig(memory_limit_gb=8.0, enable_mixed_precision_fallback=False)),\n        (\"Low memory\", MemoryConfig(memory_limit_gb=1.0, enable_mixed_precision_fallback=True)),\n        (\"Very low memory\", MemoryConfig(memory_limit_gb=0.1, enable_mixed_precision_fallback=True)),\n    ]\n    \n    cf = CurveFit()\n    \n    for config_name, mem_config in memory_configs:\n        try:\n            with memory_context(mem_config):\n                start_time = time.time()\n                \n                popt, pcov = cf.curve_fit(\n                    well_conditioned_model,\n                    x_large,\n                    y_large,\n                    p0=[4.0, 1.0, 0.4]\n                )\n                \n                fit_time = time.time() - start_time\n                \n                # Check accuracy\n                errors = np.abs(popt - np.array([5.0, 1.2, 0.5]))\n                max_error = np.max(errors / np.abs(np.array([5.0, 1.2, 0.5]))) * 100\n                \n                print(f\"  {config_name:15s}: {fit_time:.3f}s, max error: {max_error:.2f}%\")\n                \n        except Exception as e:\n            print(f\"  {config_name:15s}: ‚ùå Failed - {type(e).__name__}\")\n    \n    # Reset to original configuration\n    print(f\"\\nMemory configuration reset to: {get_memory_config().memory_limit_gb} GB\")\n\n# Run memory management demo\ndemonstrate_memory_management()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Complex Multi-Parameter Models\n",
    "\n",
    "Test NLSQ with complex models that have many parameters and potential correlation issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def test_complex_models():\n    \"\"\"Test NLSQ with complex multi-parameter models.\"\"\"\n    print(\"=\" * 70)\n    print(\"COMPLEX MULTI-PARAMETER MODEL TESTING\")\n    print(\"=\" * 70)\n    \n    cf = CurveFit()\n    \n    # Test 1: Multi-peak Gaussian\n    print(\"\\n--- Test 1: Multi-peak Gaussian (7 parameters) ---\")\n    np.random.seed(789)\n    x_multi = np.linspace(0, 10, 1000)\n    true_multi_params = [3.0, 2.0, 0.8, 2.5, 7.0, 1.2, 0.2]\n    y_multi = (np.array(multi_peak_gaussian(x_multi, *true_multi_params)) + \n               np.random.normal(0, 0.05, len(x_multi)))\n    \n    # Try different initial guesses\n    initial_guesses = [\n        [2.5, 1.8, 1.0, 2.0, 6.5, 1.5, 0.15],  # Close guess\n        [1.0, 1.0, 0.5, 1.0, 5.0, 2.0, 0.1],   # Moderate guess\n        [5.0, 3.0, 2.0, 4.0, 8.0, 0.5, 0.5],   # Poor guess\n    ]\n    \n    for i, p0 in enumerate(initial_guesses):\n        try:\n            start_time = time.time()\n            popt_multi, pcov_multi = cf.curve_fit(\n                multi_peak_gaussian,\n                x_multi,\n                y_multi,\n                p0=p0\n            )\n            fit_time = time.time() - start_time\n            \n            # Analyze results\n            errors_multi = np.abs(popt_multi - np.array(true_multi_params))\n            rel_errors_multi = errors_multi / np.abs(np.array(true_multi_params)) * 100\n            max_rel_error = np.max(rel_errors_multi)\n            \n            status = (\"‚úÖ Excellent\" if max_rel_error < 5 else\n                     \"‚úÖ Good\" if max_rel_error < 15 else\n                     \"‚ö†Ô∏è  Acceptable\" if max_rel_error < 30 else\n                     \"‚ùå Poor\")\n            \n            print(f\"  Initial guess {i+1}: {status} ({fit_time:.3f}s, max error: {max_rel_error:.1f}%)\")\n            \n            # Check parameter correlations\n            if pcov_multi is not None:\n                corr_matrix = pcov_multi / np.sqrt(np.outer(np.diag(pcov_multi), np.diag(pcov_multi)))\n                max_corr = np.max(np.abs(corr_matrix - np.eye(len(corr_matrix))))\n                if max_corr > 0.9:\n                    print(f\"    ‚ö†Ô∏è  High parameter correlation detected: {max_corr:.3f}\")\n            \n        except Exception as e:\n            print(f\"  Initial guess {i+1}: ‚ùå Failed - {type(e).__name__}\")\n    \n    # Test 2: Oscillatory model\n    print(\"\\n--- Test 2: Oscillatory Model (5 parameters) ---\")\n    x_osc = np.linspace(0, 6, 800)\n    true_osc_params = [4.0, 0.5, 2.0, 1.0, 0.3]\n    y_osc = (np.array(oscillatory_model(x_osc, *true_osc_params)) + \n             np.random.normal(0, 0.1, len(x_osc)))\n    \n    try:\n        # This is a challenging fit due to oscillations\n        p0_osc = [3.5, 0.6, 1.8, 0.8, 0.25]\n        \n        start_time = time.time()\n        popt_osc, pcov_osc = cf.curve_fit(\n            oscillatory_model,\n            x_osc,\n            y_osc,\n            p0=p0_osc\n        )\n        fit_time = time.time() - start_time\n        \n        errors_osc = np.abs(popt_osc - np.array(true_osc_params))\n        rel_errors_osc = errors_osc / np.abs(np.array(true_osc_params)) * 100\n        \n        print(f\"  ‚úÖ Oscillatory model fitted successfully ({fit_time:.3f}s)\")\n        print(f\"  True params:   {true_osc_params}\")\n        print(f\"  Fitted params: {list(popt_osc)}\")\n        print(f\"  Rel errors:    {[f'{e:.1f}%' for e in rel_errors_osc]}\")\n        \n    except Exception as e:\n        print(f\"  ‚ùå Oscillatory model failed: {e}\")\n    \n    # Test 3: Visualization of complex fit\n    if 'popt_multi' in locals():\n        print(\"\\n--- Visualizing Multi-peak Gaussian Fit ---\")\n        \n        plt.figure(figsize=(12, 4))\n        \n        plt.subplot(1, 2, 1)\n        plt.plot(x_multi, y_multi, 'b.', alpha=0.5, markersize=1, label='Data')\n        plt.plot(x_multi, multi_peak_gaussian(x_multi, *true_multi_params), 'g-', linewidth=2, label='True')\n        plt.plot(x_multi, multi_peak_gaussian(x_multi, *popt_multi), 'r--', linewidth=2, label='Fitted')\n        plt.xlabel('x')\n        plt.ylabel('y')\n        plt.title('Multi-peak Gaussian Fit')\n        plt.legend()\n        plt.grid(True, alpha=0.3)\n        \n        plt.subplot(1, 2, 2)\n        residuals = y_multi - np.array(multi_peak_gaussian(x_multi, *popt_multi))\n        plt.plot(x_multi, residuals, 'r.', alpha=0.6, markersize=1)\n        plt.axhline(y=0, color='k', linestyle='-', alpha=0.5)\n        plt.xlabel('x')\n        plt.ylabel('Residuals')\n        plt.title('Fit Residuals')\n        plt.grid(True, alpha=0.3)\n        \n        plt.tight_layout()\n        plt.show()\n        \n        print(f\"  RMS residuals: {np.sqrt(np.mean(residuals**2)):.4f}\")\n\n# Run complex model tests\ntest_complex_models()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Benchmarking\n",
    "\n",
    "Compare NLSQ performance across different scenarios and configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def performance_benchmark():\n    \"\"\"Comprehensive performance benchmarking.\"\"\"\n    print(\"=\" * 70)\n    print(\"PERFORMANCE BENCHMARKING\")\n    print(\"=\" * 70)\n    \n    # Benchmark configurations\n    benchmark_cases = [\n        (\"Small (1K)\", 1000, well_conditioned_model, [5.0, 1.2, 0.5]),\n        (\"Medium (10K)\", 10000, well_conditioned_model, [5.0, 1.2, 0.5]),\n        (\"Large (100K)\", 100000, well_conditioned_model, [5.0, 1.2, 0.5]),\n        (\"Multi-param (1K)\", 1000, multi_peak_gaussian, [3.0, 2.0, 0.8, 2.5, 7.0, 1.2, 0.2]),\n        (\"Multi-param (10K)\", 10000, multi_peak_gaussian, [3.0, 2.0, 0.8, 2.5, 7.0, 1.2, 0.2]),\n    ]\n    \n    results = []\n    \n    for case_name, n_points, model_func, true_params in benchmark_cases:\n        print(f\"\\n--- {case_name} ---\")\n        \n        # Generate data\n        np.random.seed(42)  # Consistent seed for fair comparison\n        \n        if model_func == multi_peak_gaussian:\n            x = np.linspace(0, 10, n_points)\n        else:\n            x = np.linspace(0, 5, n_points)\n            \n        y = np.array(model_func(x, *true_params)) + np.random.normal(0, 0.05, n_points)\n        \n        # Initial guess (slightly off)\n        p0 = [p * np.random.uniform(0.9, 1.1) for p in true_params]\n        \n        # Benchmark multiple runs for statistical significance\n        n_runs = 3 if n_points <= 10000 else 1  # Fewer runs for large datasets\n        times = []\n        successes = 0\n        \n        cf = CurveFit()\n        \n        for run in range(n_runs):\n            try:\n                start_time = time.time()\n                \n                popt, pcov = cf.curve_fit(\n                    model_func,\n                    x,\n                    y,\n                    p0=p0\n                )\n                \n                fit_time = time.time() - start_time\n                times.append(fit_time)\n                successes += 1\n                \n                if run == 0:  # Analyze first run for accuracy\n                    errors = np.abs(popt - np.array(true_params))\n                    max_rel_error = np.max(errors / np.abs(np.array(true_params))) * 100\n                \n            except Exception as e:\n                print(f\"    Run {run+1} failed: {type(e).__name__}\")\n        \n        if times:\n            avg_time = np.mean(times)\n            std_time = np.std(times) if len(times) > 1 else 0\n            \n            # Calculate points per second\n            pps = n_points / avg_time\n            \n            # Estimate memory usage (rough)\n            memory_mb = n_points * len(true_params) * 8 / 1024 / 1024\n            \n            results.append({\n                'case': case_name,\n                'n_points': n_points,\n                'n_params': len(true_params),\n                'avg_time': avg_time,\n                'std_time': std_time,\n                'pps': pps,\n                'memory_mb': memory_mb,\n                'max_error': max_rel_error if 'max_rel_error' in locals() else None,\n                'success_rate': successes / n_runs\n            })\n            \n            print(f\"  Time: {avg_time:.3f} ¬± {std_time:.3f} s\")\n            print(f\"  Speed: {pps:.0f} points/second\")\n            print(f\"  Memory: ~{memory_mb:.1f} MB\")\n            if 'max_rel_error' in locals():\n                print(f\"  Max error: {max_rel_error:.2f}%\")\n            print(f\"  Success: {successes}/{n_runs}\")\n        \n        else:\n            print(f\"  ‚ùå All runs failed\")\n    \n    # Summary table\n    if results:\n        print(\"\\n\" + \"=\" * 70)\n        print(\"BENCHMARK SUMMARY\")\n        print(\"=\" * 70)\n        print(f\"{'Case':15} {'Points':>8} {'Params':>6} {'Time (s)':>10} {'Speed (pts/s)':>15} {'Error (%)':>10}\")\n        print(\"-\" * 70)\n        \n        for result in results:\n            error_str = f\"{result['max_error']:.1f}\" if result['max_error'] is not None else \"N/A\"\n            print(f\"{result['case']:15} {result['n_points']:8,} {result['n_params']:6} {result['avg_time']:10.3f} \"\n                  f\"{result['pps']:15.0f} {error_str:>10}\")\n    \n    # Performance scaling plot\n    if len(results) >= 3:\n        print(\"\\n--- Performance Scaling ---\")\n        \n        # Filter for same model type\n        simple_results = [r for r in results if r['n_params'] == 3]\n        \n        if len(simple_results) >= 2:\n            plt.figure(figsize=(10, 4))\n            \n            points = [r['n_points'] for r in simple_results]\n            times = [r['avg_time'] for r in simple_results]\n            speeds = [r['pps'] for r in simple_results]\n            \n            plt.subplot(1, 2, 1)\n            plt.loglog(points, times, 'bo-', linewidth=2, markersize=8)\n            plt.xlabel('Number of Data Points')\n            plt.ylabel('Fit Time (seconds)')\n            plt.title('Scaling: Fit Time vs Dataset Size')\n            plt.grid(True, alpha=0.3)\n            \n            plt.subplot(1, 2, 2)\n            plt.semilogx(points, speeds, 'ro-', linewidth=2, markersize=8)\n            plt.xlabel('Number of Data Points')\n            plt.ylabel('Processing Speed (points/second)')\n            plt.title('Scaling: Processing Speed vs Dataset Size')\n            plt.grid(True, alpha=0.3)\n            \n            plt.tight_layout()\n            plt.show()\n\n# Run performance benchmark\nperformance_benchmark()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Best Practices\n",
    "\n",
    "Based on all the tests, let's summarize the key findings and provide best practices for using NLSQ's advanced features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary_and_best_practices():\n",
    "    \"\"\"Print comprehensive summary and best practices.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"NLSQ ADVANCED FEATURES SUMMARY & BEST PRACTICES\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(\"\\nüéØ KEY FINDINGS:\")\n",
    "    print(\"\\n1. ROBUSTNESS:\")\n",
    "    print(\"   ‚úÖ NLSQ handles poor initial guesses well\")\n",
    "    print(\"   ‚úÖ Robust to high noise levels\")\n",
    "    print(\"   ‚úÖ Graceful handling of edge cases\")\n",
    "    print(\"   ‚ö†Ô∏è  Complex multi-parameter models may need careful initialization\")\n",
    "    \n",
    "    print(\"\\n2. PERFORMANCE:\")\n",
    "    print(\"   ‚úÖ Excellent scaling with dataset size\")\n",
    "    print(\"   ‚úÖ Efficient memory usage\")\n",
    "    print(\"   ‚úÖ Fast convergence for well-conditioned problems\")\n",
    "    print(\"   ‚ö†Ô∏è  First compilation may be slow (JAX tracing)\")\n",
    "    \n",
    "    print(\"\\n3. ADVANCED FEATURES:\")\n",
    "    if ADVANCED_FEATURES_AVAILABLE:\n",
    "        print(\"   ‚úÖ Automatic memory management works well\")\n",
    "        print(\"   ‚úÖ Algorithm selection improves convergence\")\n",
    "        print(\"   ‚úÖ Context managers provide flexible configuration\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è  Advanced features require latest version\")\n",
    "    \n",
    "    print(\"\\nüìã BEST PRACTICES:\")\n",
    "    \n",
    "    print(\"\\nüîß INITIALIZATION:\")\n",
    "    print(\"   ‚Ä¢ Provide reasonable initial guesses when possible\")\n",
    "    print(\"   ‚Ä¢ For multi-parameter models, try multiple initial guesses\")\n",
    "    print(\"   ‚Ä¢ Scale initial guesses appropriately to your data\")\n",
    "    \n",
    "    print(\"\\nüíæ MEMORY MANAGEMENT:\")\n",
    "    if ADVANCED_FEATURES_AVAILABLE:\n",
    "        print(\"   ‚Ä¢ Use estimate_memory_requirements() for large datasets\")\n",
    "        print(\"   ‚Ä¢ Configure memory limits based on available resources\")\n",
    "        print(\"   ‚Ä¢ Use memory contexts for temporary configuration changes\")\n",
    "    print(\"   ‚Ä¢ Monitor memory usage for very large datasets\")\n",
    "    print(\"   ‚Ä¢ Consider chunking for datasets > 1M points\")\n",
    "    \n",
    "    print(\"\\n‚ö° PERFORMANCE OPTIMIZATION:\")\n",
    "    print(\"   ‚Ä¢ Reuse CurveFit objects for multiple fits with same model\")\n",
    "    print(\"   ‚Ä¢ Use appropriate JAX device (GPU vs CPU)\")\n",
    "    print(\"   ‚Ä¢ Profile memory usage for complex models\")\n",
    "    if ADVANCED_FEATURES_AVAILABLE:\n",
    "        print(\"   ‚Ä¢ Let algorithm selection choose optimal method\")\n",
    "    \n",
    "    print(\"\\nüéõÔ∏è MODEL DESIGN:\")\n",
    "    print(\"   ‚Ä¢ Avoid highly correlated parameters when possible\")\n",
    "    print(\"   ‚Ä¢ Use appropriate parameter bounds for constrained problems\")\n",
    "    print(\"   ‚Ä¢ Consider parameter scaling for better conditioning\")\n",
    "    print(\"   ‚Ä¢ Test with synthetic data first\")\n",
    "    \n",
    "    print(\"\\nüîç DIAGNOSTICS:\")\n",
    "    print(\"   ‚Ä¢ Check covariance matrix condition number\")\n",
    "    print(\"   ‚Ä¢ Analyze residuals for systematic errors\")\n",
    "    print(\"   ‚Ä¢ Monitor convergence behavior\")\n",
    "    print(\"   ‚Ä¢ Validate results with known test cases\")\n",
    "    \n",
    "    print(\"\\nüö® TROUBLESHOOTING:\")\n",
    "    print(\"   ‚Ä¢ If fit fails: try different initial guesses\")\n",
    "    print(\"   ‚Ä¢ If slow convergence: check parameter scaling\")\n",
    "    print(\"   ‚Ä¢ If memory errors: reduce dataset size or increase limits\")\n",
    "    print(\"   ‚Ä¢ If poor accuracy: check noise levels and model appropriateness\")\n",
    "    \n",
    "    print(\"\\nüìä WHEN TO USE NLSQ:\")\n",
    "    print(\"   ‚úÖ Large datasets (>10K points)\")\n",
    "    print(\"   ‚úÖ GPU/TPU acceleration available\")\n",
    "    print(\"   ‚úÖ Multiple fits with same model\")\n",
    "    print(\"   ‚úÖ Need for automatic differentiation\")\n",
    "    print(\"   ‚úÖ Memory-constrained environments\")\n",
    "    \n",
    "    print(\"\\nüéâ CONCLUSION:\")\n",
    "    print(\"   NLSQ provides robust, high-performance curve fitting with advanced\")\n",
    "    print(\"   memory management and diagnostic capabilities. The advanced features\")\n",
    "    print(\"   make it suitable for both simple and complex optimization problems,\")\n",
    "    print(\"   with excellent scalability and GPU acceleration.\")\n",
    "    \n",
    "    # Final system information\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"SYSTEM INFORMATION\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"JAX devices: {jax.devices()}\")\n",
    "    print(f\"JAX version: {jax.__version__}\")\n",
    "    print(f\"Advanced features available: {ADVANCED_FEATURES_AVAILABLE}\")\n",
    "    if ADVANCED_FEATURES_AVAILABLE:\n",
    "        try:\n",
    "            mem_config = get_memory_config()\n",
    "            print(f\"Current memory limit: {mem_config.memory_limit_gb} GB\")\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    print(\"\\n‚úÖ Advanced Features Demo Completed Successfully!\")\n",
    "\n",
    "# Print final summary\n",
    "print_summary_and_best_practices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìö Additional Resources\n",
    "\n",
    "For more information about NLSQ and its advanced features:\n",
    "\n",
    "- **Documentation**: [https://nlsq.readthedocs.io](https://nlsq.readthedocs.io)\n",
    "- **GitHub Repository**: [https://github.com/Dipolar-Quantum-Gases/NLSQ](https://github.com/Dipolar-Quantum-Gases/NLSQ)\n",
    "- **Issues & Support**: [https://github.com/Dipolar-Quantum-Gases/NLSQ/issues](https://github.com/Dipolar-Quantum-Gases/NLSQ/issues)\n",
    "\n",
    "### Other Example Notebooks:\n",
    "- **NLSQ Quickstart**: Basic usage and core features\n",
    "- **2D Gaussian Demo**: Multi-dimensional fitting examples\n",
    "- **Large Dataset Demo**: Memory management and performance optimization\n",
    "\n",
    "### Related Libraries:\n",
    "- **JAX**: [https://jax.readthedocs.io](https://jax.readthedocs.io)\n",
    "- **SciPy**: [https://scipy.org](https://scipy.org)\n",
    "- **NumPy**: [https://numpy.org](https://numpy.org)\n",
    "\n",
    "---\n",
    "\n",
    "*This notebook demonstrates NLSQ's advanced features as of the latest version. Some features may require specific versions or optional dependencies.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}