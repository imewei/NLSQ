{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLSQ Large Dataset Fitting Demonstration\n",
    "\n",
    "This notebook demonstrates the capabilities of NLSQ for handling very large datasets with automatic memory management, chunking, and sampling strategies.\n",
    "\n",
    "## Key Features:\n",
    "- Memory estimation for datasets from 100K to 100M+ points\n",
    "- Automatic chunking for datasets that don't fit in memory\n",
    "- Sampling strategies for extremely large datasets\n",
    "- Progress reporting for long-running fits\n",
    "- Memory-aware optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Demonstration of NLSQ Large Dataset Fitting Capabilities\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "from nlsq import LargeDatasetFitter, estimate_memory_requirements, fit_large_dataset\n",
    "\n",
    "\n",
    "# Define our model functions\n",
    "def exponential_decay(x, a, b, c):\n",
    "    \"\"\"Exponential decay model with offset: y = a * exp(-b * x) + c\"\"\"\n",
    "    return a * jnp.exp(-b * x) + c\n",
    "\n",
    "\n",
    "def polynomial_model(x, a, b, c, d):\n",
    "    \"\"\"Polynomial model: y = a*x^3 + b*x^2 + c*x + d\"\"\"\n",
    "    return a * x**3 + b * x**2 + c * x + d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Memory Estimation Demo\n",
    "\n",
    "First, let's understand how much memory different dataset sizes require and what processing strategies NLSQ recommends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_memory_estimation():\n",
    "    \"\"\"Demonstrate memory estimation capabilities.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"MEMORY ESTIMATION DEMO\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Estimate requirements for different dataset sizes\n",
    "    test_cases = [\n",
    "        (100_000, 3, \"Small dataset\"),\n",
    "        (1_000_000, 3, \"Medium dataset\"),\n",
    "        (10_000_000, 3, \"Large dataset\"),\n",
    "        (50_000_000, 3, \"Very large dataset\"),\n",
    "        (100_000_000, 3, \"Extremely large dataset\"),\n",
    "    ]\n",
    "\n",
    "    for n_points, n_params, description in test_cases:\n",
    "        stats = estimate_memory_requirements(n_points, n_params)\n",
    "\n",
    "        print(f\"\\n{description} ({n_points:,} points, {n_params} parameters):\")\n",
    "        print(f\"  Memory estimate: {stats.total_memory_estimate_gb:.2f} GB\")\n",
    "        print(f\"  Chunk size: {stats.recommended_chunk_size:,}\")\n",
    "        print(f\"  Number of chunks: {stats.n_chunks}\")\n",
    "\n",
    "        if stats.requires_sampling:\n",
    "            print(\"  Strategy: Sampling recommended\")\n",
    "        elif stats.n_chunks == 1:\n",
    "            print(\"  Strategy: Single chunk (fits in memory)\")\n",
    "        else:\n",
    "            print(\"  Strategy: Chunked processing\")\n",
    "\n",
    "\n",
    "# Run the demo\n",
    "demo_memory_estimation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Large Dataset Fitting\n",
    "\n",
    "Let's demonstrate fitting a 1 million point dataset using the convenience function `fit_large_dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_basic_large_dataset_fitting():\n",
    "    \"\"\"Demonstrate basic large dataset fitting.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"BASIC LARGE DATASET FITTING DEMO\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Generate synthetic large dataset (1M points)\n",
    "    print(\"Generating 1M point exponential decay dataset...\")\n",
    "    np.random.seed(42)\n",
    "    n_points = 1_000_000\n",
    "    x_data = np.linspace(0, 5, n_points, dtype=np.float64)\n",
    "    true_params = [5.0, 1.2, 0.5]\n",
    "    noise_level = 0.05\n",
    "\n",
    "    y_true = true_params[0] * np.exp(-true_params[1] * x_data) + true_params[2]\n",
    "    y_data = y_true + np.random.normal(0, noise_level, n_points)\n",
    "\n",
    "    print(f\"Dataset: {n_points:,} points\")\n",
    "    print(\n",
    "        f\"True parameters: a={true_params[0]}, b={true_params[1]}, c={true_params[2]}\"\n",
    "    )\n",
    "\n",
    "    # Fit using convenience function\n",
    "    print(\"\\nFitting with automatic memory management...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    result = fit_large_dataset(\n",
    "        exponential_decay,\n",
    "        x_data,\n",
    "        y_data,\n",
    "        p0=[4.0, 1.0, 0.4],\n",
    "        memory_limit_gb=2.0,  # 2GB limit\n",
    "        show_progress=True,\n",
    "    )\n",
    "\n",
    "    fit_time = time.time() - start_time\n",
    "\n",
    "    if result.success:\n",
    "        fitted_params = np.array(result.popt)\n",
    "        errors = np.abs(fitted_params - np.array(true_params))\n",
    "        rel_errors = errors / np.array(true_params) * 100\n",
    "\n",
    "        print(f\"\\n✅ Fit completed in {fit_time:.2f} seconds\")\n",
    "        print(\n",
    "            f\"Fitted parameters: [{fitted_params[0]:.3f}, {fitted_params[1]:.3f}, {fitted_params[2]:.3f}]\"\n",
    "        )\n",
    "        print(f\"Absolute errors: [{errors[0]:.4f}, {errors[1]:.4f}, {errors[2]:.4f}]\")\n",
    "        print(\n",
    "            f\"Relative errors: [{rel_errors[0]:.2f}%, {rel_errors[1]:.2f}%, {rel_errors[2]:.2f}%]\"\n",
    "        )\n",
    "    else:\n",
    "        print(f\"❌ Fit failed: {result.message}\")\n",
    "\n",
    "\n",
    "# Run the demo\n",
    "demo_basic_large_dataset_fitting()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chunked Processing Demo\n",
    "\n",
    "For datasets that don't fit in memory, NLSQ automatically chunks the data and processes it in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_chunked_processing():\n",
    "    \"\"\"Demonstrate chunked processing with progress reporting.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"CHUNKED PROCESSING DEMO\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Generate a dataset that will require chunking\n",
    "    print(\"Generating 2M point polynomial dataset...\")\n",
    "    np.random.seed(123)\n",
    "    n_points = 2_000_000\n",
    "    x_data = np.linspace(-2, 2, n_points, dtype=np.float64)\n",
    "    true_params = [0.5, -1.2, 2.0, 1.5]\n",
    "    noise_level = 0.1\n",
    "\n",
    "    y_true = (\n",
    "        true_params[0] * x_data**3\n",
    "        + true_params[1] * x_data**2\n",
    "        + true_params[2] * x_data\n",
    "        + true_params[3]\n",
    "    )\n",
    "    y_data = y_true + np.random.normal(0, noise_level, n_points)\n",
    "\n",
    "    print(f\"Dataset: {n_points:,} points\")\n",
    "    print(f\"True parameters: {true_params}\")\n",
    "\n",
    "    # Create fitter with limited memory to force chunking\n",
    "    fitter = LargeDatasetFitter(memory_limit_gb=0.5)  # Small limit to force chunking\n",
    "\n",
    "    # Get processing recommendations\n",
    "    recs = fitter.get_memory_recommendations(n_points, 4)\n",
    "    print(f\"\\nProcessing strategy: {recs['processing_strategy']}\")\n",
    "    print(f\"Chunk size: {recs['recommendations']['chunk_size']:,}\")\n",
    "    print(f\"Number of chunks: {recs['recommendations']['n_chunks']}\")\n",
    "    print(\n",
    "        f\"Memory estimate: {recs['recommendations']['total_memory_estimate_gb']:.2f} GB\"\n",
    "    )\n",
    "\n",
    "    # Fit with progress reporting\n",
    "    print(\"\\nFitting with chunked processing...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    result = fitter.fit_with_progress(\n",
    "        polynomial_model, x_data, y_data, p0=[0.4, -1.0, 1.8, 1.2]\n",
    "    )\n",
    "\n",
    "    fit_time = time.time() - start_time\n",
    "\n",
    "    if result.success:\n",
    "        fitted_params = np.array(result.popt)\n",
    "        errors = np.abs(fitted_params - np.array(true_params))\n",
    "        rel_errors = errors / np.abs(np.array(true_params)) * 100\n",
    "\n",
    "        print(f\"\\n✅ Chunked fit completed in {fit_time:.2f} seconds\")\n",
    "        if hasattr(result, \"n_chunks\"):\n",
    "            print(\n",
    "                f\"Used {result.n_chunks} chunks with {result.success_rate:.1%} success rate\"\n",
    "            )\n",
    "        print(f\"Fitted parameters: {fitted_params}\")\n",
    "        print(f\"Absolute errors: {errors}\")\n",
    "        print(f\"Relative errors: {rel_errors}%\")\n",
    "    else:\n",
    "        print(f\"❌ Chunked fit failed: {result.message}\")\n",
    "\n",
    "\n",
    "# Run the demo\n",
    "demo_chunked_processing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sampling Strategy for Extremely Large Datasets\n",
    "\n",
    "For datasets with 100M+ points, sampling strategies can be more efficient than processing all data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_sampling_strategy():\n",
    "    \"\"\"Demonstrate sampling for extremely large datasets.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SAMPLING STRATEGY DEMO\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Simulate a very large dataset scenario\n",
    "    print(\"Simulating extremely large dataset (100M points)...\")\n",
    "    n_points_full = 100_000_000  # 100M points\n",
    "    true_params = [3.0, 0.8, 0.2]\n",
    "\n",
    "    # For demo purposes, generate a smaller representative sample\n",
    "    # In practice, you would have this data already or stream it\n",
    "    np.random.seed(456)\n",
    "    n_sample = 1_000_000  # 1M sample for demo\n",
    "    x_sample = np.sort(np.random.uniform(0, 5, n_sample))\n",
    "    y_sample = (\n",
    "        true_params[0] * np.exp(-true_params[1] * x_sample)\n",
    "        + true_params[2]\n",
    "        + np.random.normal(0, 0.05, n_sample)\n",
    "    )\n",
    "\n",
    "    print(f\"Full dataset size: {n_points_full:,} points (simulated)\")\n",
    "    print(f\"Demo sample size: {n_sample:,} points\")\n",
    "    print(f\"True parameters: {true_params}\")\n",
    "\n",
    "    # Check memory requirements for full dataset\n",
    "    stats = estimate_memory_requirements(n_points_full, 3)\n",
    "    print(f\"\\nFull dataset memory estimate: {stats.total_memory_estimate_gb:.2f} GB\")\n",
    "    print(f\"Sampling recommended: {stats.requires_sampling}\")\n",
    "\n",
    "    # Create fitter with sampling enabled\n",
    "    from nlsq.large_dataset import LDMemoryConfig\n",
    "\n",
    "    config = LDMemoryConfig(memory_limit_gb=4.0, enable_sampling=True)\n",
    "    fitter = LargeDatasetFitter(config=config)\n",
    "\n",
    "    print(\"\\nFitting with sampling strategy...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    # For demo, use our sample as if it were the full dataset\n",
    "    result = fitter.fit(exponential_decay, x_sample, y_sample, p0=[2.5, 1.0, 0.1])\n",
    "\n",
    "    fit_time = time.time() - start_time\n",
    "\n",
    "    if result.success:\n",
    "        fitted_params = np.array(result.popt)\n",
    "        errors = np.abs(fitted_params - np.array(true_params))\n",
    "        rel_errors = errors / np.array(true_params) * 100\n",
    "\n",
    "        print(f\"\\n✅ Sampling fit completed in {fit_time:.2f} seconds\")\n",
    "        print(f\"Fitted parameters: {fitted_params}\")\n",
    "        print(f\"Absolute errors: {errors}\")\n",
    "        print(f\"Relative errors: {rel_errors}%\")\n",
    "\n",
    "        if hasattr(result, \"was_sampled\") and result.was_sampled:\n",
    "            print(\n",
    "                f\"Used sampling: {result.sample_size:,} points from {result.original_size:,}\"\n",
    "            )\n",
    "    else:\n",
    "        print(f\"❌ Sampling fit failed: {result.message}\")\n",
    "\n",
    "\n",
    "# Run the demo\n",
    "demo_sampling_strategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Performance Comparison\n",
    "\n",
    "Let's compare different approaches for various dataset sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_approaches():\n",
    "    \"\"\"Compare different fitting approaches.\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"PERFORMANCE COMPARISON\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Test different dataset sizes\n",
    "    sizes = [10_000, 100_000, 500_000]\n",
    "\n",
    "    print(f\"\\n{'Size':>10} {'Time (s)':>12} {'Memory (GB)':>12} {'Strategy':>20}\")\n",
    "    print(\"-\" * 55)\n",
    "\n",
    "    for n in sizes:\n",
    "        # Generate data\n",
    "        np.random.seed(42)\n",
    "        x = np.linspace(0, 10, n)\n",
    "        y = 2.0 * np.exp(-0.5 * x) + 0.3 + np.random.normal(0, 0.05, n)\n",
    "\n",
    "        # Get memory estimate\n",
    "        stats = estimate_memory_requirements(n, 3)\n",
    "\n",
    "        # Determine strategy\n",
    "        if stats.n_chunks == 1:\n",
    "            strategy = \"Single chunk\"\n",
    "        elif stats.requires_sampling:\n",
    "            strategy = \"Sampling\"\n",
    "        else:\n",
    "            strategy = f\"Chunked ({stats.n_chunks} chunks)\"\n",
    "\n",
    "        # Time the fit\n",
    "        start = time.time()\n",
    "        result = fit_large_dataset(\n",
    "            exponential_decay,\n",
    "            x,\n",
    "            y,\n",
    "            p0=[2.5, 0.6, 0.2],\n",
    "            memory_limit_gb=0.5,  # Small limit to test chunking\n",
    "            show_progress=False,\n",
    "        )\n",
    "        elapsed = time.time() - start\n",
    "\n",
    "        print(\n",
    "            f\"{n:10,} {elapsed:12.3f} {stats.total_memory_estimate_gb:12.3f} {strategy:>20}\"\n",
    "        )\n",
    "\n",
    "\n",
    "# Run comparison\n",
    "compare_approaches()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Key Takeaways\n",
    "\n",
    "NLSQ provides comprehensive support for large dataset fitting:\n",
    "\n",
    "1. **Automatic Memory Management**: NLSQ automatically detects available memory and chooses the best strategy\n",
    "2. **Chunked Processing**: Datasets that don't fit in memory are automatically processed in chunks\n",
    "3. **Sampling Strategies**: For extremely large datasets (>100M points), intelligent sampling can provide accurate results\n",
    "4. **Progress Reporting**: Long-running fits provide progress updates\n",
    "5. **Memory Estimation**: Predict memory requirements before fitting\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "- Use `estimate_memory_requirements()` to understand dataset requirements\n",
    "- Use `fit_large_dataset()` for automatic handling of large datasets\n",
    "- Set appropriate `memory_limit_gb` based on your system\n",
    "- Enable sampling for datasets >100M points\n",
    "- Use progress reporting for long-running fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print final summary\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DEMO COMPLETED\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nKey takeaways:\")\n",
    "print(\"• NLSQ automatically handles memory management for large datasets\")\n",
    "print(\"• Chunked processing works for datasets that don't fit in memory\")\n",
    "print(\"• Sampling strategies can handle extremely large datasets efficiently\")\n",
    "print(\"• Progress reporting helps track long-running fits\")\n",
    "print(\"• Memory estimation helps plan processing strategies\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
