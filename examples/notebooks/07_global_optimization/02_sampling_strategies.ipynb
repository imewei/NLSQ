{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "    return a * jnp.sin(b * x + c)\ndef compute_simple_discrepancy(samples: np.ndarray) -> float:\n    \"\"\"Compute a simple discrepancy measure based on minimum neighbor distances.\n    Lower discrepancy indicates more uniform coverage.\n    \"\"\"\n    n = len(samples)\n    if n < 2:\n        return 0.0\n    distances = []\n    for i in range(n):\n        min_dist = float(\"inf\")\n        for j in range(n):\n            if i != j:\n                dist = np.linalg.norm(samples[i] - samples[j])\n                min_dist = min(min_dist, dist)\n        distances.append(min_dist)\n    d = samples.shape[1]\n    ideal_dist = (1.0 / n) ** (1.0 / d)\n    distances = np.array(distances)\n    discrepancy = np.std(distances) / ideal_dist\n    return discrepancy\ndef evaluate_starting_points(samples_unit, lb, ub, x_data, y_data, true_params, bounds):\n    \"\"\"Evaluate success rate of starting points.\n    Returns the fraction of starting points that converge to the global optimum.\n    \"\"\"\n    samples_scaled = scale_samples_to_bounds(jnp.array(samples_unit), lb, ub)\n    samples_scaled = np.array(samples_scaled)\n    y_true_pred = true_params[0] * np.sin(true_params[1] * x_data + true_params[2])\n    true_ssr = np.sum((y_data - y_true_pred) ** 2)\n    success_count = 0\n    ssrs = []\n    for p0 in samples_scaled:\n        try:\n            popt, _ = fit(\n                multimodal_model,\n                x_data,\n                y_data,\n                p0=list(p0),\n                bounds=bounds,\n                workflow=\"auto\",\n            )\n            y_pred = multimodal_model(x_data, *popt)\n            ssr = float(jnp.sum((y_data - y_pred) ** 2))\n            ssrs.append(ssr)\n            if ssr < true_ssr * 1.1:\n                success_count += 1\n        except Exception:\n            ssrs.append(float(\"inf\"))\n    success_rate = success_count / len(samples_scaled)\n    best_ssr = min(ssrs)\n    return success_rate, best_ssr, ssrs\ndef main():\n    print(\"=\" * 70)\n    print(\"Sampling Strategies for Global Optimization (v0.6.3)\")\n    print(\"=\" * 70)\n    print()\n    if QUICK:\n        print(\"Quick mode: skipping full demonstration.\")\n        print()\n        print(\"=\" * 70)\n        print(\"Summary: Sampling Strategies\")\n        print(\"=\" * 70)\n        print()\n        print(\"Sampling Strategies:\")\n        print(\"  - Random: Baseline, poor space-filling\")\n        print(\"  - LHS: Stratified random, good coverage, stochastic\")\n        print(\"  - Sobol: Quasi-random, excellent coverage, deterministic\")\n        print(\"  - Halton: Quasi-random, very good coverage, deterministic\")\n        print()\n        print(\"Key Functions:\")\n        print(\"  - latin_hypercube_sample(n_samples, n_dims)\")\n        print(\"  - sobol_sample(n_samples, n_dims)\")\n        print(\"  - halton_sample(n_samples, n_dims)\")\n        print(\"  - scale_samples_to_bounds(samples, lb, ub)\")\n        print()\n        print(\"Usage with fit():\")\n        print(\"  fit(..., workflow='auto_global', n_starts=10, sampler='lhs')\")\n        return\n    np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Generate samples using each method\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"1. Generating samples with different methods...\")\nn_samples = 50\nn_dims = 2\nrandom_samples = np.random.rand(n_samples, n_dims)\nkey = jax.random.PRNGKey(42)\nlhs_samples = latin_hypercube_sample(n_samples, n_dims, rng_key=key)\nsobol_samples = sobol_sample(n_samples, n_dims)\nhalton_samples = halton_sample(n_samples, n_dims)\nprint(f\"  Generated {n_samples} samples in {n_dims} dimensions\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Visualize 2D samples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print()\nprint(\"2. Saving 2D comparison visualization...\")\nfig, axes = plt.subplots(2, 2, figsize=(12, 12))\nsamples_dict = {\n    \"Random\": random_samples,\n    \"Latin Hypercube (LHS)\": np.array(lhs_samples),\n    \"Sobol\": np.array(sobol_samples),\n    \"Halton\": np.array(halton_samples),\n}\nfor ax, (name, samples) in zip(axes.flat, samples_dict.items(), strict=False):\n    ax.scatter(\n        samples[:, 0],\n        samples[:, 1],\n        s=40,\n        alpha=0.7,\n        edgecolors=\"black\",\n        linewidths=0.5,\n    )\n    ax.set_xlim(0, 1)\n    ax.set_ylim(0, 1)\n    ax.set_xlabel(\"Dimension 1\")\n    ax.set_ylabel(\"Dimension 2\")\n    ax.set_title(f\"{name} ({n_samples} samples)\")\n    ax.set_aspect(\"equal\")\n    ax.grid(True, alpha=0.3)\nplt.tight_layout()\nout_path = FIG_DIR / \"02_sampling_comparison_2d.png\"\nplt.savefig(out_path, dpi=300, bbox_inches=\"tight\")\nplt.close()\nprint(f\"  Saved: {out_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. LHS stratification visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print()\nprint(\"3. Saving LHS stratification visualization...\")\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\nax1 = axes[0]\nax1.scatter(random_samples[:, 0], random_samples[:, 1], s=40, alpha=0.7)\nfor i in range(n_samples + 1):\n    ax1.axvline(x=i / n_samples, color=\"gray\", alpha=0.2, linewidth=0.5)\n    ax1.axhline(y=i / n_samples, color=\"gray\", alpha=0.2, linewidth=0.5)\nax1.set_xlim(0, 1)\nax1.set_ylim(0, 1)\nax1.set_xlabel(\"Dimension 1\")\nax1.set_ylabel(\"Dimension 2\")\nax1.set_title(\"Random: Multiple samples per stratum\")\nax2 = axes[1]\nax2.scatter(\n    np.array(lhs_samples)[:, 0],\n    np.array(lhs_samples)[:, 1],\n    s=40,\n    alpha=0.7,\n    color=\"orange\",\n)\nfor i in range(n_samples + 1):\n    ax2.axvline(x=i / n_samples, color=\"gray\", alpha=0.2, linewidth=0.5)\n    ax2.axhline(y=i / n_samples, color=\"gray\", alpha=0.2, linewidth=0.5)\nax2.set_xlim(0, 1)\nax2.set_ylim(0, 1)\nax2.set_xlabel(\"Dimension 1\")\nax2.set_ylabel(\"Dimension 2\")\nax2.set_title(\"LHS: Exactly one sample per stratum per dimension\")\nplt.tight_layout()\nplt.savefig(FIG_DIR / \"02_lhs_stratification.png\", dpi=300, bbox_inches=\"tight\")\nplt.close()\nprint(f\"  Saved: {FIG_DIR / '02_lhs_stratification.png'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. Quasi-random progressive fill\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print()\nprint(\"4. Saving quasi-random progressive fill visualization...\")\nfig, axes = plt.subplots(2, 4, figsize=(16, 8))\nsample_counts = [8, 16, 32, 64]\nfor i, n in enumerate(sample_counts):\n    sobol_n = sobol_sample(n, 2)\n    axes[0, i].scatter(\n        np.array(sobol_n)[:, 0], np.array(sobol_n)[:, 1], s=30, alpha=0.8\n    )\n    axes[0, i].set_xlim(0, 1)\n    axes[0, i].set_ylim(0, 1)\n    axes[0, i].set_title(f\"Sobol: n={n}\")\n    axes[0, i].set_aspect(\"equal\")\n    axes[0, i].grid(True, alpha=0.3)\n    halton_n = halton_sample(n, 2)\n    axes[1, i].scatter(\n        np.array(halton_n)[:, 0],\n        np.array(halton_n)[:, 1],\n        s=30,\n        alpha=0.8,\n        color=\"green\",\n    )\n    axes[1, i].set_xlim(0, 1)\n    axes[1, i].set_ylim(0, 1)\n    axes[1, i].set_title(f\"Halton: n={n}\")\n    axes[1, i].set_aspect(\"equal\")\n    axes[1, i].grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig(\n    FIG_DIR / \"02_quasi_random_progressive.png\", dpi=300, bbox_inches=\"tight\"\n)\nplt.close()\nprint(f\"  Saved: {FIG_DIR / '02_quasi_random_progressive.png'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. Discrepancy comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print()\nprint(\"5. Computing discrepancy comparison...\")\nsample_sizes = [10, 20, 30, 50, 75, 100]\ndiscrepancies = {\"Random\": [], \"LHS\": [], \"Sobol\": [], \"Halton\": []}\nfor n in sample_sizes:\n    random_s = np.random.rand(n, 2)\n    lhs_s = np.array(latin_hypercube_sample(n, 2, rng_key=jax.random.PRNGKey(42)))\n    sobol_s = np.array(sobol_sample(n, 2))\n    halton_s = np.array(halton_sample(n, 2))\n    discrepancies[\"Random\"].append(compute_simple_discrepancy(random_s))\n    discrepancies[\"LHS\"].append(compute_simple_discrepancy(lhs_s))\n    discrepancies[\"Sobol\"].append(compute_simple_discrepancy(sobol_s))\n    discrepancies[\"Halton\"].append(compute_simple_discrepancy(halton_s))\nfig, ax = plt.subplots(figsize=(10, 6))\ncolors = {\"Random\": \"red\", \"LHS\": \"orange\", \"Sobol\": \"blue\", \"Halton\": \"green\"}\nmarkers = {\"Random\": \"o\", \"LHS\": \"s\", \"Sobol\": \"^\", \"Halton\": \"d\"}\nfor name, discs in discrepancies.items():\n    ax.plot(\n        sample_sizes,\n        discs,\n        marker=markers[name],\n        color=colors[name],\n        linewidth=2,\n        markersize=8,\n        label=name,\n    )\nax.set_xlabel(\"Number of Samples\")\nax.set_ylabel(\"Discrepancy (lower is better)\")\nax.set_title(\"Discrepancy Comparison: Space-Filling Quality\")\nax.legend()\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig(FIG_DIR / \"02_discrepancy_comparison.png\", dpi=300, bbox_inches=\"tight\")\nplt.close()\nprint(f\"  Saved: {FIG_DIR / '02_discrepancy_comparison.png'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6. Success rate comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print()\nprint(\"6. Computing success rate comparison...\")\nnp.random.seed(42)\nn_points = 100\nx_data = np.linspace(0, 4 * np.pi, n_points)\ntrue_params = [2.0, 1.5, 0.5]\ny_true = true_params[0] * np.sin(true_params[1] * x_data + true_params[2])\ny_data = y_true + 0.2 * np.random.randn(n_points)\nbounds = ([0.5, 0.5, -np.pi], [5.0, 3.0, np.pi])\nlb, ub = np.array(bounds[0]), np.array(bounds[1])\nn_starts_list = [5, 10] if QUICK else [5, 10, 15, 20, 25, 30]\nn_trials = 2 if QUICK else 5\nsuccess_rates = {\"Random\": [], \"LHS\": [], \"Sobol\": [], \"Halton\": []}\nfor n_starts in n_starts_list:\n    print(f\"  Evaluating n_starts = {n_starts}...\")\n    random_rates = []\n    lhs_rates = []\n    for trial in range(n_trials):\n        random_s = np.random.rand(n_starts, 3)\n        lhs_s = np.array(\n            latin_hypercube_sample(n_starts, 3, rng_key=jax.random.PRNGKey(trial))\n        )\n        rate, _, _ = evaluate_starting_points(\n            random_s, lb, ub, x_data, y_data, true_params, bounds\n        )\n        random_rates.append(rate)\n        rate, _, _ = evaluate_starting_points(\n            lhs_s, lb, ub, x_data, y_data, true_params, bounds\n        )\n        lhs_rates.append(rate)\n    success_rates[\"Random\"].append(np.mean(random_rates))\n    success_rates[\"LHS\"].append(np.mean(lhs_rates))\n    sobol_s = np.array(sobol_sample(n_starts, 3))\n    halton_s = np.array(halton_sample(n_starts, 3))\n    rate, _, _ = evaluate_starting_points(\n        sobol_s, lb, ub, x_data, y_data, true_params, bounds\n    )\n    success_rates[\"Sobol\"].append(rate)\n    rate, _, _ = evaluate_starting_points(\n        halton_s, lb, ub, x_data, y_data, true_params, bounds\n    )\n    success_rates[\"Halton\"].append(rate)\nfig, ax = plt.subplots(figsize=(10, 6))\nfor name, rates in success_rates.items():\n    ax.plot(\n        n_starts_list,\n        [r * 100 for r in rates],\n        marker=markers[name],\n        color=colors[name],\n        linewidth=2,\n        markersize=8,\n        label=name,\n    )\nax.set_xlabel(\"Number of Starting Points\")\nax.set_ylabel(\"Success Rate (%)\")\nax.set_title(\"Success Rate: Finding Global Optimum\")\nax.legend()\nax.grid(True, alpha=0.3)\nax.set_ylim(0, 105)\nplt.tight_layout()\nplt.savefig(\n    FIG_DIR / \"02_success_rate_comparison.png\", dpi=300, bbox_inches=\"tight\"\n)\nplt.close()\nprint(f\"  Saved: {FIG_DIR / '02_success_rate_comparison.png'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "7. Demonstrate samplers with fit(workflow='auto_global')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print()\nprint(\"7. Using samplers with fit(workflow='auto_global')...\")\nsamplers = [\"lhs\", \"sobol\", \"halton\"]\nresults = {}\nfor sampler in samplers:\n    popt, pcov = fit(\n        multimodal_model,\n        x_data,\n        y_data,\n        p0=[1.0, 1.0, 0.0],\n        bounds=bounds,\n        workflow=\"auto_global\",  # Global optimization\n        n_starts=10,\n        sampler=sampler,\n    )\n    y_pred = multimodal_model(x_data, *popt)\n    ssr = float(jnp.sum((y_data - y_pred) ** 2))\n    results[sampler] = {\"popt\": popt, \"ssr\": ssr}\n    print(f\"  Sampler: {sampler}\")\n    print(f\"    Parameters: a={popt[0]:.3f}, b={popt[1]:.3f}, c={popt[2]:.3f}\")\n    print(f\"    SSR: {ssr:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "    print()\n    print(\"=\" * 70)\n    print(\"Summary - Sampling Strategies (v0.6.3)\")\n    print(\"=\" * 70)\n    print()\n    print(\"Sampling Strategies:\")\n    print(\"  - Random: Baseline, poor space-filling\")\n    print(\"  - LHS: Stratified random, good coverage, stochastic (default)\")\n    print(\"  - Sobol: Quasi-random, excellent coverage, deterministic\")\n    print(\"  - Halton: Quasi-random, very good coverage, deterministic\")\n    print()\n    print(\"Key Functions:\")\n    print(\"  - latin_hypercube_sample(n_samples, n_dims)\")\n    print(\"  - sobol_sample(n_samples, n_dims)\")\n    print(\"  - halton_sample(n_samples, n_dims)\")\n    print(\"  - scale_samples_to_bounds(samples, lb, ub)\")\n    print()\n    print(\"Usage with fit():\")\n    print(\"  fit(..., workflow='auto_global', n_starts=10, sampler='lhs')\")\n    print()\n    print(\"Sampler Selection Guidelines:\")\n    print(\"  - General use: LHS (default)\")\n    print(\"  - Reproducibility needed: Sobol\")\n    print(\"  - Low dimensions (2-5): Halton\")\n    print(\"  - High dimensions (>10): LHS\")\nif __name__ == \"__main__\":\n    main()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
