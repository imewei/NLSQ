{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Start Optimization Basics\n",
    "\n",
    "This tutorial demonstrates how multi-start optimization helps find global optima\n",
    "in curve fitting problems with multiple local minima.\n",
    "\n",
    "**Features demonstrated:**\n",
    "- Local minima trap problem in nonlinear optimization\n",
    "- `GlobalOptimizationConfig` configuration\n",
    "- `curve_fit()` with `global_optimization` parameter\n",
    "- Comparison of single-start vs multi-start results\n",
    "- Visualization of loss landscape and starting point distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure matplotlib for inline plotting (MUST come before imports)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from nlsq import curve_fit, GlobalOptimizationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Problem: Local Minima Traps\n",
    "\n",
    "Many real-world curve fitting problems have multiple local minima. A standard\n",
    "single-start optimizer may converge to a suboptimal local minimum instead of\n",
    "the global optimum, depending on the initial parameter guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a multimodal sinusoidal model that has multiple local minima\n",
    "def multimodal_model(x, a, b, c, d):\n",
    "    \"\"\"Multimodal model: y = a * sin(b * x + c) + d\n",
    "    \n",
    "    This model has multiple local minima due to the periodicity of sin().\n",
    "    Different combinations of (b, c) can produce similar fits.\n",
    "    \"\"\"\n",
    "    return a * jnp.sin(b * x + c) + d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data with known true parameters\n",
    "n_samples = 200\n",
    "x_data = np.linspace(0, 4 * np.pi, n_samples)\n",
    "\n",
    "# True parameters\n",
    "true_a, true_b, true_c, true_d = 2.0, 1.5, 0.5, 1.0\n",
    "\n",
    "# Generate noisy observations\n",
    "y_true = true_a * np.sin(true_b * x_data + true_c) + true_d\n",
    "noise = 0.2 * np.random.randn(n_samples)\n",
    "y_data = y_true + noise\n",
    "\n",
    "print(f\"True parameters: a={true_a}, b={true_b}, c={true_c}, d={true_d}\")\n",
    "print(f\"Dataset: {n_samples} points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "ax.scatter(x_data, y_data, alpha=0.5, s=10, label=\"Noisy data\")\n",
    "ax.plot(x_data, y_true, \"r-\", linewidth=2, label=\"True function\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_title(\"Synthetic Data: Multimodal Sinusoidal Model\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/01_data_visualization.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Single-Start Optimization: The Local Minima Problem\n",
    "\n",
    "Let's try fitting with a poor initial guess. Single-start optimization may\n",
    "get trapped in a local minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define bounds for parameters\n",
    "# a: [0.5, 5], b: [0.5, 3], c: [-pi, pi], d: [-2, 5]\n",
    "bounds = ([0.5, 0.5, -np.pi, -2.0], [5.0, 3.0, np.pi, 5.0])\n",
    "\n",
    "# Try several different initial guesses with single-start optimization\n",
    "initial_guesses = [\n",
    "    [1.0, 0.8, 0.0, 0.5],   # Poor guess 1\n",
    "    [3.0, 2.5, 2.0, 2.0],   # Poor guess 2\n",
    "    [1.5, 1.2, -1.0, 0.0],  # Poor guess 3\n",
    "]\n",
    "\n",
    "single_start_results = []\n",
    "\n",
    "print(\"Single-start optimization results:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, p0 in enumerate(initial_guesses):\n",
    "    try:\n",
    "        popt, pcov = curve_fit(\n",
    "            multimodal_model,\n",
    "            x_data,\n",
    "            y_data,\n",
    "            p0=p0,\n",
    "            bounds=bounds,\n",
    "        )\n",
    "        # Calculate sum of squared residuals\n",
    "        y_pred = multimodal_model(x_data, *popt)\n",
    "        ssr = float(jnp.sum((y_data - y_pred) ** 2))\n",
    "        single_start_results.append({\"p0\": p0, \"popt\": popt, \"ssr\": ssr})\n",
    "        print(f\"Guess {i+1}: p0={p0}\")\n",
    "        print(f\"  Result: a={popt[0]:.3f}, b={popt[1]:.3f}, c={popt[2]:.3f}, d={popt[3]:.3f}\")\n",
    "        print(f\"  SSR: {ssr:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Guess {i+1}: Failed - {e}\")\n",
    "        single_start_results.append({\"p0\": p0, \"popt\": None, \"ssr\": float(\"inf\")})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find best and worst results from single-start\n",
    "valid_results = [r for r in single_start_results if r[\"popt\"] is not None]\n",
    "if valid_results:\n",
    "    best_single = min(valid_results, key=lambda x: x[\"ssr\"])\n",
    "    worst_single = max(valid_results, key=lambda x: x[\"ssr\"])\n",
    "    print(f\"\\nBest single-start SSR: {best_single['ssr']:.4f}\")\n",
    "    print(f\"Worst single-start SSR: {worst_single['ssr']:.4f}\")\n",
    "    print(f\"\\nVariability in results shows sensitivity to initial guess!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Multi-Start Optimization: Finding the Global Optimum\n",
    "\n",
    "Multi-start optimization explores the parameter space from multiple starting\n",
    "points, significantly increasing the chance of finding the global optimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure multi-start optimization\n",
    "global_config = GlobalOptimizationConfig(\n",
    "    n_starts=10,              # Number of starting points\n",
    "    sampler=\"lhs\",            # Latin Hypercube Sampling for even coverage\n",
    "    center_on_p0=True,        # Center samples around initial guess\n",
    "    scale_factor=1.0,         # Exploration scale\n",
    ")\n",
    "\n",
    "print(\"GlobalOptimizationConfig:\")\n",
    "print(f\"  n_starts: {global_config.n_starts}\")\n",
    "print(f\"  sampler: {global_config.sampler}\")\n",
    "print(f\"  center_on_p0: {global_config.center_on_p0}\")\n",
    "print(f\"  scale_factor: {global_config.scale_factor}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the first (poor) initial guess to show multi-start improvement\n",
    "p0_poor = [1.0, 0.8, 0.0, 0.5]\n",
    "\n",
    "# Fit with multi-start optimization\n",
    "popt_multi, pcov_multi = curve_fit(\n",
    "    multimodal_model,\n",
    "    x_data,\n",
    "    y_data,\n",
    "    p0=p0_poor,\n",
    "    bounds=bounds,\n",
    "    multistart=True,\n",
    "    n_starts=10,\n",
    "    sampler=\"lhs\",\n",
    ")\n",
    "\n",
    "# Calculate SSR for multi-start result\n",
    "y_pred_multi = multimodal_model(x_data, *popt_multi)\n",
    "ssr_multi = float(jnp.sum((y_data - y_pred_multi) ** 2))\n",
    "\n",
    "print(\"\\nMulti-start optimization result:\")\n",
    "print(f\"  Result: a={popt_multi[0]:.3f}, b={popt_multi[1]:.3f}, c={popt_multi[2]:.3f}, d={popt_multi[3]:.3f}\")\n",
    "print(f\"  SSR: {ssr_multi:.4f}\")\n",
    "print(f\"\\nTrue params: a={true_a}, b={true_b}, c={true_c}, d={true_d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare single-start (from same initial guess) vs multi-start\n",
    "popt_single, _ = curve_fit(\n",
    "    multimodal_model,\n",
    "    x_data,\n",
    "    y_data,\n",
    "    p0=p0_poor,\n",
    "    bounds=bounds,\n",
    ")\n",
    "y_pred_single = multimodal_model(x_data, *popt_single)\n",
    "ssr_single = float(jnp.sum((y_data - y_pred_single) ** 2))\n",
    "\n",
    "print(\"\\nComparison (same initial guess):\")\n",
    "print(f\"  Single-start SSR: {ssr_single:.4f}\")\n",
    "print(f\"  Multi-start SSR:  {ssr_multi:.4f}\")\n",
    "if ssr_multi < ssr_single:\n",
    "    improvement = (1 - ssr_multi / ssr_single) * 100\n",
    "    print(f\"  Improvement: {improvement:.1f}% lower SSR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualization: Comparing Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left plot: Data with both fits\n",
    "ax1 = axes[0]\n",
    "ax1.scatter(x_data, y_data, alpha=0.4, s=15, label=\"Data\", color=\"gray\")\n",
    "ax1.plot(x_data, y_true, \"k--\", linewidth=2, label=\"True function\", alpha=0.7)\n",
    "ax1.plot(x_data, y_pred_single, \"b-\", linewidth=2, label=f\"Single-start (SSR={ssr_single:.2f})\")\n",
    "ax1.plot(x_data, y_pred_multi, \"r-\", linewidth=2, label=f\"Multi-start (SSR={ssr_multi:.2f})\")\n",
    "ax1.set_xlabel(\"x\")\n",
    "ax1.set_ylabel(\"y\")\n",
    "ax1.set_title(\"Single-Start vs Multi-Start Comparison\")\n",
    "ax1.legend()\n",
    "\n",
    "# Right plot: Residuals comparison\n",
    "ax2 = axes[1]\n",
    "residuals_single = y_data - y_pred_single\n",
    "residuals_multi = y_data - y_pred_multi\n",
    "ax2.scatter(x_data, residuals_single, alpha=0.5, s=15, label=\"Single-start\", color=\"blue\")\n",
    "ax2.scatter(x_data, residuals_multi, alpha=0.5, s=15, label=\"Multi-start\", color=\"red\")\n",
    "ax2.axhline(y=0, color=\"k\", linestyle=\"--\", alpha=0.5)\n",
    "ax2.set_xlabel(\"x\")\n",
    "ax2.set_ylabel(\"Residual\")\n",
    "ax2.set_title(\"Residuals Comparison\")\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/01_comparison.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Loss Landscape Visualization\n",
    "\n",
    "Let's visualize the loss landscape to understand why multi-start helps.\n",
    "We'll fix two parameters and scan over the other two to see the multiple minima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create loss landscape by scanning over (b, c) while fixing (a, d) at true values\n",
    "b_range = np.linspace(0.5, 3.0, 50)\n",
    "c_range = np.linspace(-np.pi, np.pi, 50)\n",
    "B, C = np.meshgrid(b_range, c_range)\n",
    "\n",
    "# Calculate SSR for each (b, c) combination\n",
    "loss_landscape = np.zeros_like(B)\n",
    "for i in range(len(c_range)):\n",
    "    for j in range(len(b_range)):\n",
    "        y_pred = true_a * np.sin(B[i, j] * x_data + C[i, j]) + true_d\n",
    "        loss_landscape[i, j] = np.sum((y_data - y_pred) ** 2)\n",
    "\n",
    "# Log-transform for better visualization\n",
    "loss_log = np.log10(loss_landscape + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize loss landscape\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Contour plot\n",
    "contour = ax.contourf(B, C, loss_log, levels=30, cmap=\"viridis\")\n",
    "plt.colorbar(contour, ax=ax, label=\"log10(SSR + 1)\")\n",
    "\n",
    "# Mark true parameters\n",
    "ax.scatter([true_b], [true_c], color=\"white\", marker=\"*\", s=200, \n",
    "           label=\"True parameters\", edgecolors=\"black\", linewidths=1)\n",
    "\n",
    "# Mark single-start result\n",
    "ax.scatter([popt_single[1]], [popt_single[2]], color=\"blue\", marker=\"o\", s=100,\n",
    "           label=\"Single-start result\", edgecolors=\"white\", linewidths=1)\n",
    "\n",
    "# Mark multi-start result\n",
    "ax.scatter([popt_multi[1]], [popt_multi[2]], color=\"red\", marker=\"s\", s=100,\n",
    "           label=\"Multi-start result\", edgecolors=\"white\", linewidths=1)\n",
    "\n",
    "ax.set_xlabel(\"b (frequency)\")\n",
    "ax.set_ylabel(\"c (phase)\")\n",
    "ax.set_title(\"Loss Landscape (a, d fixed at true values)\\nMultiple local minima visible\")\n",
    "ax.legend(loc=\"upper right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/01_loss_landscape.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Starting Point Distribution\n",
    "\n",
    "Latin Hypercube Sampling (LHS) provides better coverage of the parameter space\n",
    "compared to random sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate samples using LHS to visualize the distribution\n",
    "from nlsq.global_optimization import latin_hypercube_sample, scale_samples_to_bounds\n",
    "import jax\n",
    "\n",
    "# Generate 20 starting points for visualization\n",
    "n_samples_viz = 20\n",
    "n_params = 4\n",
    "\n",
    "# Generate LHS samples in [0, 1]^d\n",
    "key = jax.random.PRNGKey(42)\n",
    "lhs_samples = latin_hypercube_sample(n_samples_viz, n_params, rng_key=key)\n",
    "\n",
    "# Scale to bounds\n",
    "lb = np.array([0.5, 0.5, -np.pi, -2.0])\n",
    "ub = np.array([5.0, 3.0, np.pi, 5.0])\n",
    "scaled_samples = scale_samples_to_bounds(lhs_samples, lb, ub)\n",
    "\n",
    "print(f\"Generated {n_samples_viz} LHS samples in 4D parameter space\")\n",
    "print(f\"Sample shape: {scaled_samples.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 2D projection of starting points (b vs c)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Left: LHS samples on loss landscape\n",
    "ax1 = axes[0]\n",
    "contour = ax1.contourf(B, C, loss_log, levels=30, cmap=\"viridis\", alpha=0.7)\n",
    "ax1.scatter(scaled_samples[:, 1], scaled_samples[:, 2], color=\"yellow\", \n",
    "            marker=\"o\", s=80, label=\"LHS starting points\", edgecolors=\"black\", linewidths=1)\n",
    "ax1.scatter([true_b], [true_c], color=\"white\", marker=\"*\", s=200, \n",
    "            label=\"True parameters\", edgecolors=\"black\", linewidths=1)\n",
    "ax1.set_xlabel(\"b (frequency)\")\n",
    "ax1.set_ylabel(\"c (phase)\")\n",
    "ax1.set_title(\"LHS Starting Points on Loss Landscape\")\n",
    "ax1.legend()\n",
    "\n",
    "# Right: All 2D projections\n",
    "ax2 = axes[1]\n",
    "param_names = [\"a\", \"b\", \"c\", \"d\"]\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, 6))\n",
    "\n",
    "plot_idx = 0\n",
    "for i in range(n_params):\n",
    "    for j in range(i + 1, n_params):\n",
    "        ax2.scatter(scaled_samples[:, i], scaled_samples[:, j], \n",
    "                   alpha=0.6, s=30, color=colors[plot_idx],\n",
    "                   label=f\"{param_names[i]} vs {param_names[j]}\")\n",
    "        plot_idx += 1\n",
    "\n",
    "ax2.set_xlabel(\"Parameter value (normalized)\")\n",
    "ax2.set_ylabel(\"Parameter value (normalized)\")\n",
    "ax2.set_title(\"LHS Coverage: All 2D Projections\")\n",
    "ax2.legend(loc=\"upper right\", fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/01_starting_points.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Key Takeaways\n",
    "\n",
    "1. **Local minima are common** in nonlinear optimization, especially with periodic functions.\n",
    "\n",
    "2. **Single-start optimization** is sensitive to initial guess and may converge to local minima.\n",
    "\n",
    "3. **Multi-start optimization** explores multiple starting points, significantly increasing the\n",
    "   chance of finding the global optimum.\n",
    "\n",
    "4. **GlobalOptimizationConfig** controls:\n",
    "   - `n_starts`: Number of starting points (more = better coverage, slower)\n",
    "   - `sampler`: Sampling strategy ('lhs', 'sobol', 'halton')\n",
    "   - `center_on_p0`: Whether to center around initial guess\n",
    "   - `scale_factor`: Exploration range\n",
    "\n",
    "5. **Latin Hypercube Sampling (LHS)** provides stratified coverage, ensuring the starting\n",
    "   points are evenly distributed across the parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"Summary\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"True parameters: a={true_a}, b={true_b}, c={true_c}, d={true_d}\")\n",
    "print(f\"\\nSingle-start result (poor initial guess):\")\n",
    "print(f\"  Parameters: a={popt_single[0]:.3f}, b={popt_single[1]:.3f}, c={popt_single[2]:.3f}, d={popt_single[3]:.3f}\")\n",
    "print(f\"  SSR: {ssr_single:.4f}\")\n",
    "print(f\"\\nMulti-start result (10 starts, LHS):\")\n",
    "print(f\"  Parameters: a={popt_multi[0]:.3f}, b={popt_multi[1]:.3f}, c={popt_multi[2]:.3f}, d={popt_multi[3]:.3f}\")\n",
    "print(f\"  SSR: {ssr_multi:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
