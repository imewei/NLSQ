{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# NLSQ Troubleshooting Guide\n\n**Level**: All Levels\n**Time**: Reference guide (browse as needed)\n**Prerequisites**: NLSQ Quickstart\n\n## Overview\n\nThis guide covers **common issues** encountered when using NLSQ and provides **practical solutions**. Each problem includes:\n- Clear symptoms and error messages\n- Root cause explanation\n- Step-by-step fixes\n- Working code examples\n\n### Quick Navigation\n\n1. **Convergence Failures**: \"Optimal parameters not found\", max iterations reached\n2. **Poor Fit Quality**: High residuals, wrong parameter values\n3. **Numerical Issues**: NaN, inf, JAX errors\n4. **Performance Problems**: Slow compilation, memory errors\n5. **Error Messages**: Decoding common NLSQ/JAX errors\n6. **Best Practices**: Preventing issues before they occur"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure matplotlib for inline plotting in VS Code/Jupyter\n",
    "# MUST come before importing matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\n\n# Configure matplotlib for inline plotting in VS Code/Jupyter\n# MUST come before importing matplotlib\n\nimport warnings\n\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom nlsq import CurveFit\n\n# Show all warnings (helpful for debugging)\nwarnings.filterwarnings(\"default\")\n\nprint(\"‚úì Setup complete\")\nprint(f\"  JAX version: {jax.__version__}\")\nprint(f\"  JAX backend: {jax.default_backend()}\")\n\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Issue 1: Convergence Failures\n\n### Symptoms\n- Error: `OptimizeWarning: Optimal parameters not found`\n- Error: `RuntimeError: Fitting failed to converge`\n- Warning: `Maximum number of iterations reached`\n- Parameters unchanged or nonsensical values\n\n### Root Causes\n1. **Poor initial guess** (`p0` far from true values)\n2. **Inadequate iteration limit** (complex fits need more iterations)\n3. **Ill-conditioned problem** (parameters on very different scales)\n4. **Local minima** (non-convex optimization landscape)\n\n### Solutions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\n\n# Demonstrate convergence failure and fix\n\n# Generate test data\nx_data = np.linspace(0, 10, 50)\ny_data = 5.0 * np.exp(-0.5 * x_data) + np.random.normal(0, 0.1, 50)\n\n\ndef exponential_decay(x, a, b):\n    return a * jnp.exp(-b * x)\n\n\n# Create fresh CurveFit instance (avoid lingering state from previous demos)\ncf = CurveFit()\n\n# PROBLEM: Bad initial guess\nprint(\"‚ùå PROBLEM: Poor initial guess\")\ntry:\n    p0_bad = [0.1, 10.0]  # Far from true values [5.0, 0.5]\n    popt_bad, _ = cf.curve_fit(\n        exponential_decay, jnp.array(x_data), jnp.array(y_data), p0=p0_bad, maxiter=100\n    )\n    print(f\"  Fitted params: a={popt_bad[0]:.2f}, b={popt_bad[1]:.2f}\")\n    print(\"  True params: a=5.0, b=0.5 (likely poor fit!)\\n\")\nexcept Exception as e:\n    print(f\"  Error: {e}\\n\")\n\n# SOLUTION 1: Better initial guess\nprint(\"‚úì SOLUTION 1: Improve initial guess\")\n# Strategy: Estimate from data\na_guess = y_data[0]  # Initial value (t=0)\nb_guess = -np.log(y_data[-1] / y_data[0]) / (x_data[-1] - x_data[0])  # Decay rate\np0_good = [a_guess, b_guess]\nprint(f\"  Estimated p0: a={p0_good[0]:.2f}, b={p0_good[1]:.2f}\")\n\npopt_good, _ = cf.curve_fit(\n    exponential_decay, jnp.array(x_data), jnp.array(y_data), p0=p0_good\n)\nprint(f\"  Fitted params: a={popt_good[0]:.2f}, b={popt_good[1]:.2f}\")\nprint(\"  True params: a=5.0, b=0.5 ‚úì\\n\")\n\n# SOLUTION 2: Increase iteration limit\nprint(\"‚úì SOLUTION 2: Increase maxiter for complex problems\")\npopt_iter, _ = cf.curve_fit(\n    exponential_decay,\n    jnp.array(x_data),\n    jnp.array(y_data),\n    p0=[1.0, 1.0],\n    maxiter=1000,  # Default is often 200-400\n)\nprint(f\"  Fitted with maxiter=1000: a={popt_iter[0]:.2f}, b={popt_iter[1]:.2f} ‚úì\")\n\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Issue 2: Poor Fit Quality\n\n### Symptoms\n- Fit converges but doesn't match data well\n- High chi-squared or RMSE\n- Parameters don't make physical sense\n- Residuals show clear patterns\n\n### Root Causes\n1. **Wrong model** (functional form doesn't match data)\n2. **Insufficient model complexity** (missing terms)\n3. **Bounds too restrictive** (excluding true parameters)\n4. **Outliers** dominating the fit\n\n### Solutions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\n\n# Demonstrate poor fit quality and fixes\n\n# Generate data with offset (y = a*exp(-b*x) + c)\nx_data = np.linspace(0, 5, 40)\ny_true = 3.0 * np.exp(-0.8 * x_data) + 1.5  # Has offset!\ny_data = y_true + np.random.normal(0, 0.1, len(x_data))\n\n# PROBLEM: Wrong model (missing offset term)\nprint(\"‚ùå PROBLEM: Model mismatch (missing offset)\")\n\n\ndef exp_no_offset(x, a, b):\n    return a * jnp.exp(-b * x)\n\n\npopt_wrong, _ = cf.curve_fit(\n    exp_no_offset, jnp.array(x_data), jnp.array(y_data), p0=[3.0, 0.8]\n)\ny_fit_wrong = exp_no_offset(jnp.array(x_data), *popt_wrong)\nrmse_wrong = np.sqrt(np.mean((y_data - y_fit_wrong) ** 2))\nprint(f\"  RMSE with wrong model: {rmse_wrong:.3f} (high!)\\n\")\n\n# SOLUTION: Add missing offset term\nprint(\"‚úì SOLUTION: Use correct model with offset\")\n\n\ndef exp_with_offset(x, a, b, c):\n    return a * jnp.exp(-b * x) + c\n\n\npopt_correct, _ = cf.curve_fit(\n    exp_with_offset, jnp.array(x_data), jnp.array(y_data), p0=[3.0, 0.8, 1.0]\n)\ny_fit_correct = exp_with_offset(jnp.array(x_data), *popt_correct)\nrmse_correct = np.sqrt(np.mean((y_data - y_fit_correct) ** 2))\nprint(\n    f\"  Fitted: a={popt_correct[0]:.2f}, b={popt_correct[1]:.2f}, c={popt_correct[2]:.2f}\"\n)\nprint(f\"  RMSE with correct model: {rmse_correct:.3f} (much better!) ‚úì\")\nprint(f\"  Improvement: {(rmse_wrong - rmse_correct) / rmse_wrong * 100:.1f}%\")\n\n\n# Demonstrate bounds issues\n\n# PROBLEM: Bounds excluding true parameters\nprint(\"\\n‚ùå PROBLEM: Overly restrictive bounds\")\nbounds_wrong = ([0, 0, 0], [2.0, 1.0, 1.0])  # c is actually 1.5!\n\npopt_bounded, _ = cf.curve_fit(\n    exp_with_offset,\n    jnp.array(x_data),\n    jnp.array(y_data),\n    p0=[1.9, 0.5, 0.5],  # Adjusted to be strictly within bounds\n    bounds=bounds_wrong,\n)\nprint(\n    f\"  Fitted with tight bounds: a={popt_bounded[0]:.2f}, b={popt_bounded[1]:.2f}, c={popt_bounded[2]:.2f}\"\n)\nprint(\"  True offset c=1.5, but bounds limited to c ‚â§ 1.0!\\n\")\n\n# SOLUTION: Relax bounds or remove them\nprint(\"‚úì SOLUTION: Use wider bounds or let optimizer explore\")\nbounds_good = ([0, 0, 0], [10.0, 5.0, 5.0])  # More generous\n\npopt_unbounded, _ = cf.curve_fit(\n    exp_with_offset,\n    jnp.array(x_data),\n    jnp.array(y_data),\n    p0=[2.0, 0.5, 1.0],\n    bounds=bounds_good,\n)\nprint(\n    f\"  Fitted with relaxed bounds: a={popt_unbounded[0]:.2f}, b={popt_unbounded[1]:.2f}, c={popt_unbounded[2]:.2f} ‚úì\"\n)\n\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Issue 3: Numerical Instability (NaN, Inf)\n\n### Symptoms\n- Error: `ValueError: array must not contain infs or NaNs`\n- Warning: `RuntimeWarning: overflow encountered in exp`\n- Parameters become NaN or Inf during optimization\n\n### Root Causes\n1. **Overflow** in exponentials (`exp(large_number)` ‚Üí inf)\n2. **Underflow** in divisions (divide by zero)\n3. **Poor parameter scaling** (parameters differ by orders of magnitude)\n4. **Invalid operations** (sqrt of negative, log of zero)\n\n### Solutions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\n\n# Demonstrate numerical instability and fixes\n\n# PROBLEM: Exponential overflow\nprint(\"‚ùå PROBLEM: Exponential overflow\")\n\nx_large = np.linspace(0, 100, 50)\ny_data_large = 1.0 / (1.0 + np.exp(-0.1 * (x_large - 50))) + np.random.normal(\n    0, 0.01, 50\n)\n\n\ndef logistic_unstable(x, L, k, x0):\n    # UNSTABLE: exp can overflow for large k*(x-x0)\n    return L / (1.0 + jnp.exp(-k * (x - x0)))\n\n\ntry:\n    popt, _ = cf.curve_fit(\n        logistic_unstable,\n        jnp.array(x_large),\n        jnp.array(y_data_large),\n        p0=[1.0, 1.0, 50.0],\n    )\n    print(f\"  Fitted (might have issues): {popt}\")\nexcept Exception as e:\n    print(f\"  Error: {type(e).__name__}: {e}\\n\")\n\n# SOLUTION 1: Use numerically stable formulation\nprint(\"‚úì SOLUTION 1: Rewrite function to avoid overflow\")\n\n\ndef logistic_stable(x, L, k, x0):\n    # STABLE: Uses log-space computations internally\n    z = -k * (x - x0)\n    # Use jnp.where to handle both overflow regions\n    return jnp.where(\n        z > 0,\n        L / (1.0 + jnp.exp(z)),  # Safe when z > 0\n        L * jnp.exp(-z) / (jnp.exp(-z) + 1.0),  # Safe when z ‚â§ 0\n    )\n\n\npopt_stable, _ = cf.curve_fit(\n    logistic_stable, jnp.array(x_large), jnp.array(y_data_large), p0=[1.0, 0.1, 50.0]\n)\nprint(\n    f\"  Fitted with stable version: L={popt_stable[0]:.2f}, k={popt_stable[1]:.2f}, x0={popt_stable[2]:.1f} ‚úì\\n\"\n)\n\n# SOLUTION 2: Parameter rescaling\nprint(\"‚úì SOLUTION 2: Rescale parameters to similar magnitudes\")\n\n\ndef model_rescaled(x, L, k_scaled, x0):\n    # k_scaled = k * 100 (so we fit k in [0.01, 1] instead of [0.0001, 0.01])\n    k = k_scaled / 100.0\n    return logistic_stable(x, L, k, x0)\n\n\npopt_rescaled, _ = cf.curve_fit(\n    model_rescaled, jnp.array(x_large), jnp.array(y_data_large), p0=[1.0, 10.0, 50.0]\n)\nprint(\n    f\"  Fitted with rescaling: L={popt_rescaled[0]:.2f}, k={popt_rescaled[1] / 100:.3f}, x0={popt_rescaled[2]:.1f} ‚úì\"\n)\n\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Issue 4: Performance Problems\n\n### Symptoms\n- Very slow first call (compilation time)\n- Subsequent calls still slow\n- Memory errors with large datasets\n- Error: `RESOURCE_EXHAUSTED: Out of memory`\n\n### Root Causes\n1. **JIT compilation overhead** (JAX compiles on first call)\n2. **Large data** (millions of points)\n3. **Complex models** (many nested operations)\n4. **Unnecessary recompilation** (changing array shapes)\n\n### Solutions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\n\n# Performance optimization tips\n\nimport time\n\n# PROBLEM: Slow first call (JIT compilation)\nprint(\"‚Ñπ UNDERSTANDING: JIT compilation (first call slow, then fast)\")\n\nx_perf = jnp.linspace(0, 10, 1000)\ny_perf = 2.0 * jnp.sin(x_perf) + np.random.normal(0, 0.1, 1000)\n\n\ndef sine_model(x, a, b):\n    return a * jnp.sin(b * x)\n\n\n# First call: includes compilation time\nstart = time.time()\npopt1, _ = cf.curve_fit(sine_model, x_perf, y_perf, p0=[1.0, 1.0])\ntime1 = time.time() - start\n\n# Second call: already compiled\nstart = time.time()\npopt2, _ = cf.curve_fit(sine_model, x_perf, y_perf, p0=[1.5, 0.8])\ntime2 = time.time() - start\n\nprint(f\"  First call: {time1 * 1000:.1f} ms (includes compilation)\")\nprint(f\"  Second call: {time2 * 1000:.1f} ms (cached, faster!)\")\nprint(f\"  Speedup: {time1 / time2:.1f}x\\n\")\n\n# SOLUTION 1: Pre-compile with dummy call\nprint(\"‚úì SOLUTION 1: Warm up JIT cache with dummy call\")\ncf_new = CurveFit()\n# Dummy call with data to compile (use full dataset for stable fit)\nwith contextlib.suppress(Exception):\n    # Compilation happened even if fit didn't fully converge\n    _ = cf_new.curve_fit(sine_model, x_perf, y_perf, p0=[1.0, 1.0], max_nfev=50)\nprint(\"  JIT cache warmed up (subsequent calls will be fast) ‚úì\\n\")\n\n# SOLUTION 2: Use consistent array shapes (avoid recompilation)\nprint(\"‚úì SOLUTION 2: Keep array shapes consistent\")\nprint(\"  ‚ùå Bad: Changing shapes triggers recompilation\")\nprint(\"     fit(x[:100], ...)  # Compiles for shape (100,)\")\nprint(\"     fit(x[:200], ...)  # Recompiles for shape (200,) ‚ö†\")\nprint(\"  ‚úì Good: Same shapes reuse compilation\")\nprint(\"     fit(x, ...)  # Compiles for shape (1000,)\")\nprint(\"     fit(x, ...)  # Reuses compilation ‚úì\")\n\n\n# Handling large datasets\n\nprint(\"\\n‚úì SOLUTION 3: Strategies for large datasets (>1M points)\\n\")\n\nprint(\"Option A: Downsample data (if appropriate)\")\nprint(\"  # For smooth, oversampled data\")\nprint(\"  stride = 10\")\nprint(\"  x_sub = x_data[::stride]  # Every 10th point\")\nprint(\"  y_sub = y_data[::stride]\")\nprint(\"  popt, pcov = cf.curve_fit(model, x_sub, y_sub, ...)\\n\")\n\nprint(\"Option B: Binning/averaging\")\nprint(\"  # Combine neighboring points\")\nprint(\"  n_bins = 10000\")\nprint(\"  x_binned = np.array([x_data[i::n_bins].mean() for i in range(n_bins)])\")\nprint(\"  y_binned = np.array([y_data[i::n_bins].mean() for i in range(n_bins)])\\n\")\n\nprint(\"Option C: Use float32 instead of float64\")\nprint(\"  # Halves memory usage, often sufficient precision\")\nprint(\"  x_data = jnp.array(x_data, dtype=jnp.float32)\")\nprint(\"  y_data = jnp.array(y_data, dtype=jnp.float32)\\n\")\n\nprint(\"Option D: Streaming/chunked fitting (advanced)\")\nprint(\"  # For distributed data, fit chunks separately then combine\")\nprint(\"  # See: examples/streaming/ directory\")\n\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Issue 5: Common Error Messages\n\n### Quick Reference"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\n\n# Common error messages and solutions\n\nprint(\"‚îÅ\" * 80)\nprint(\"COMMON NLSQ/JAX ERROR MESSAGES & SOLUTIONS\")\nprint(\"‚îÅ\" * 80)\nprint()\n\nerrors = [\n    {\n        \"error\": \"ValueError: operands could not be broadcast together\",\n        \"cause\": \"Array shape mismatch (x and y different lengths)\",\n        \"fix\": \"Check len(x) == len(y), reshape arrays if needed\",\n    },\n    {\n        \"error\": \"TypeError: Argument 'x' of type <class 'list'> is not a valid JAX type\",\n        \"cause\": \"Passing Python list instead of JAX/NumPy array\",\n        \"fix\": \"Convert to array: jnp.array(x) or np.array(x)\",\n    },\n    {\n        \"error\": \"LinAlgError: Singular matrix\",\n        \"cause\": \"Covariance matrix is singular (parameters not identifiable)\",\n        \"fix\": \"Reduce model complexity, check for redundant parameters, add bounds\",\n    },\n    {\n        \"error\": \"ValueError: array must not contain infs or NaNs\",\n        \"cause\": \"Model produces NaN/Inf during optimization\",\n        \"fix\": \"Check model for overflow (exp, division), add bounds, rescale params\",\n    },\n    {\n        \"error\": \"IndexError: tuple index out of range\",\n        \"cause\": \"Accessing parameters that don't exist\",\n        \"fix\": \"Check p0 length matches number of parameters in model\",\n    },\n    {\n        \"error\": \"RuntimeError: Fitting failed to converge\",\n        \"cause\": \"Optimizer couldn't find minimum\",\n        \"fix\": \"Improve p0, increase maxiter, relax bounds, check model\",\n    },\n    {\n        \"error\": \"ValueError: `bounds` must have shape (2, n_params)\",\n        \"cause\": \"Incorrect bounds format\",\n        \"fix\": \"Use bounds=([lower1, lower2], [upper1, upper2])\",\n    },\n    {\n        \"error\": \"JAX tracer error / ConcretizationTypeError\",\n        \"cause\": \"Using if/while with traced values inside JIT\",\n        \"fix\": \"Use jnp.where, jax.lax.cond, or static_argnums\",\n    },\n]\n\nfor i, err in enumerate(errors, 1):\n    print(f\"{i}. ERROR: {err['error']}\")\n    print(f\"   Cause: {err['cause']}\")\n    print(f\"   Fix:   {err['fix']}\")\n    print()\n\nprint(\"‚îÅ\" * 80)\n\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Issue 6: Best Practices Checklist\n\n### Pre-Flight Checklist (Before Fitting)\n\nUse this checklist to prevent common issues:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\n\n# Pre-fitting checklist\n\nprint(\"‚îÅ\" * 80)\nprint(\"NLSQ PRE-FITTING CHECKLIST\")\nprint(\"‚îÅ\" * 80)\nprint()\n\nchecklist = [\n    (\n        \"Data Validation\",\n        [\n            \"Arrays are JAX/NumPy arrays (not lists)\",\n            \"x and y have same length\",\n            \"No NaN or Inf values in data\",\n            \"Sufficient data points (at least 10x number of parameters)\",\n            \"Data spans appropriate range for model\",\n        ],\n    ),\n    (\n        \"Model Definition\",\n        [\n            \"Function uses jnp (not np) for JAX compatibility\",\n            \"No Python if/while statements (use jnp.where, jax.lax.cond)\",\n            \"Model is numerically stable (check for overflow/underflow)\",\n            \"Function signature: model(x, param1, param2, ...)\",\n        ],\n    ),\n    (\n        \"Initial Guess (p0)\",\n        [\n            \"p0 length matches number of parameters\",\n            \"Values are reasonable estimates (not random)\",\n            \"Test: plot model(x, *p0) vs. data to verify\",\n            \"Parameters on similar scales (or use rescaling)\",\n        ],\n    ),\n    (\n        \"Bounds (if used)\",\n        [\n            \"Format: bounds=([lower1, lower2], [upper1, upper2])\",\n            \"Bounds include true parameter values\",\n            \"Not overly restrictive (allow optimizer to explore)\",\n            \"Physical constraints enforced (e.g., positive rates)\",\n        ],\n    ),\n    (\n        \"Optimization Settings\",\n        [\n            \"maxiter sufficient for problem complexity (default: 200-400)\",\n            \"Consider sigma if uncertainties are heteroscedastic\",\n            \"Use absolute_sigma=True if sigma values are reliable\",\n        ],\n    ),\n]\n\nfor category, items in checklist:\n    print(f\"üìã {category}:\")\n    for item in items:\n        print(f\"   ‚òê {item}\")\n    print()\n\nprint(\"‚îÅ\" * 80)\nprint(\"After fitting, always:\")\nprint(\"  1. Check convergence (no warnings)\")\nprint(\"  2. Plot residuals (should be random)\")\nprint(\"  3. Verify parameter uncertainties are reasonable\")\nprint(\"  4. Test on held-out data if available\")\nprint(\"‚îÅ\" * 80)\n\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Diagnostic Workflow\n\nWhen troubleshooting a fit that's not working:"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "\n\n# Step-by-step diagnostic workflow\n\nprint(\"‚îÅ\" * 80)\nprint(\"DIAGNOSTIC WORKFLOW FOR FAILED FITS\")\nprint(\"‚îÅ\" * 80)\nprint()\n\nworkflow = [\n    (\n        \"Step 1: Visualize the problem\",\n        [\n            \"plt.plot(x_data, y_data, 'o', label='Data')\",\n            \"plt.plot(x_data, model(x_data, *p0), '-', label='Initial guess')\",\n            \"plt.legend()\",\n            \"# Does p0 give reasonable shape? If not, fix p0 first!\",\n        ],\n    ),\n    (\n        \"Step 2: Test model function\",\n        [\n            \"# Call model directly to check for errors\",\n            \"y_test = model(jnp.array(x_data), *p0)\",\n            \"print(f'Model output: min={y_test.min()}, max={y_test.max()}')\",\n            \"# Check for NaN, Inf, unexpected values\",\n        ],\n    ),\n    (\n        \"Step 3: Simplify the problem\",\n        [\n            \"# Try fitting with subset of data\",\n            \"x_sub, y_sub = x_data[:20], y_data[:20]\",\n            \"# Try simpler model (fewer parameters)\",\n            \"# Remove bounds temporarily\",\n        ],\n    ),\n    (\n        \"Step 4: Check convergence details\",\n        [\n            \"# Use full_output=True for diagnostics\",\n            \"popt, pcov, infodict = cf.curve_fit(model, x, y, p0=p0, full_output=True)\",\n            \"print(infodict)  # Inspect iteration count, message, etc.\",\n        ],\n    ),\n    (\n        \"Step 5: Try alternative approaches\",\n        [\n            \"# Different initial guesses (grid search)\",\n            \"# Different optimization method (if available)\",\n            \"# Add constraints via bounds\",\n            \"# Reformulate model (e.g., log-space)\",\n        ],\n    ),\n]\n\nfor step, code_lines in workflow:\n    print(f\"\\n{step}\")\n    print(\"-\" * 60)\n    for line in code_lines:\n        print(f\"  {line}\")\n\nprint(\"\\n\" + \"‚îÅ\" * 80)\nprint(\"If still stuck: Check NLSQ GitHub issues or ask for help!\")\nprint(\"  https://github.com/your-nlsq-repo/issues\")\nprint(\"‚îÅ\" * 80)\n\n"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Summary: Quick Problem Solver\n\n| **Symptom** | **Most Likely Cause** | **Quick Fix** |\n|-------------|----------------------|---------------|\n| \"Optimal parameters not found\" | Poor initial guess | Improve p0, visualize model(x, *p0) |\n| High residuals, poor fit | Wrong model | Add missing terms, check functional form |\n| NaN or Inf errors | Numerical overflow | Rescale parameters, rewrite model |\n| Very slow (>10s) | JIT compilation | Normal for first call, faster after |\n| Memory error | Too much data | Downsample, use float32, or chunk |\n| Singular matrix | Redundant parameters | Simplify model, add bounds |\n| Parameters hit bounds | Bounds too tight | Relax bounds or remove them |\n| JAX tracer error | if/while in model | Use jnp.where or jax.lax.cond |\n\n### Pro Tips\n\n1. **Always visualize** before fitting: `plt.plot(x, model(x, *p0))`\n2. **Start simple**: Fit with fewer parameters, then add complexity\n3. **Check units**: Ensure x, y, and parameters are in sensible ranges\n4. **Use physics**: Prior knowledge helps constrain bounds and p0\n5. **Read warnings**: They usually tell you exactly what's wrong\n\n### Additional Resources\n\n- **NLSQ Documentation**: https://nlsq.readthedocs.io/\n- **Advanced Examples**: `examples/advanced_features_demo.ipynb`\n- **JAX Debugging**: https://jax.readthedocs.io/en/latest/debugging/\n- **Gallery Examples**: `examples/gallery/` (domain-specific use cases)\n\n---\n\n**Remember**: Most fitting issues stem from poor initial guesses, wrong models, or numerical instability. Address these systematically using the diagnostic workflow above."
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
