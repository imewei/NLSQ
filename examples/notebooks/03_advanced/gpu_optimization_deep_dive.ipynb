{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4de9e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T21:10:22.429018Z",
     "iopub.status.busy": "2025-11-18T21:10:22.428524Z",
     "iopub.status.idle": "2025-11-18T21:10:22.742525Z",
     "shell.execute_reply": "2025-11-18T21:10:22.741630Z"
    }
   },
   "outputs": [],
   "source": [
    "# Configure matplotlib for inline plotting in VS Code/Jupyter\n",
    "# MUST come before importing matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bfb351",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T21:10:22.744800Z",
     "iopub.status.busy": "2025-11-18T21:10:22.744465Z",
     "iopub.status.idle": "2025-11-18T21:10:23.328268Z",
     "shell.execute_reply": "2025-11-18T21:10:23.327512Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from jax import grad, jit, vmap\n",
    "\n",
    "from nlsq import CurveFit, curve_fit\n",
    "\n",
    "\n",
    "# Define the exponential model\n",
    "def exponential_model(x, a, b):\n",
    "    return a * jnp.exp(-b * x)\n",
    "\n",
    "# Generate batch data for demonstrations\n",
    "np.random.seed(42)\n",
    "n_datasets = 1000\n",
    "n_points = 100\n",
    "x_batch_data = np.linspace(0, 5, n_points)\n",
    "y_batch_data = np.array([\n",
    "    3.0 * np.exp(-0.5 * x_batch_data) + np.random.normal(0, 0.1, n_points)\n",
    "    for _ in range(n_datasets)\n",
    "])\n",
    "\n",
    "print(f\"JAX backend: {jax.devices()[0].platform}\")\n",
    "print(f\"Generated {n_datasets} synthetic datasets with {n_points} points each\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ede082",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T21:10:23.330445Z",
     "iopub.status.busy": "2025-11-18T21:10:23.330198Z",
     "iopub.status.idle": "2025-11-18T21:10:35.599668Z",
     "shell.execute_reply": "2025-11-18T21:10:35.599268Z"
    }
   },
   "outputs": [],
   "source": [
    "# Measure sequential fitting time (for comparison)\n",
    "print(\"Timing sequential fits (100 datasets for estimate)...\")\n",
    "\n",
    "def fit_one_sequential(y_data):\n",
    "    params = jnp.array([3.0, 0.5])\n",
    "    def loss(p):\n",
    "        return jnp.sum((y_data - exponential_model(x_batch_data, *p)) ** 2)\n",
    "    for _ in range(20):\n",
    "        g = jax.grad(loss)(params)\n",
    "        params = params - 0.05 * g\n",
    "    return params\n",
    "\n",
    "# Time 100 sequential fits\n",
    "start = time.time()\n",
    "for i in range(100):\n",
    "    _ = fit_one_sequential(y_batch_data[i])\n",
    "time_sequential = time.time() - start\n",
    "\n",
    "print(f\"Time for 100 sequential fits: {time_sequential:.3f}s\")\n",
    "print(f\"Average time per fit: {time_sequential / 100 * 1000:.3f}ms\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f1b7dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T21:10:35.601202Z",
     "iopub.status.busy": "2025-11-18T21:10:35.601098Z",
     "iopub.status.idle": "2025-11-18T21:10:36.550397Z",
     "shell.execute_reply": "2025-11-18T21:10:36.549853Z"
    }
   },
   "outputs": [],
   "source": [
    "def fit_one_dataset(y_single):\n",
    "    \"\"\"Fit a single dataset using gradient descent.\"\"\"\n",
    "    params = jnp.array([3.0, 0.5])\n",
    "    def loss(p):\n",
    "        return jnp.sum((y_single - exponential_model(x_batch_data, *p)) ** 2)\n",
    "    for _ in range(20):\n",
    "        g = jax.grad(loss)(params)\n",
    "        params = params - 0.05 * g\n",
    "    return params\n",
    "\n",
    "fit_batch = jit(vmap(fit_one_dataset))\n",
    "_ = fit_batch(y_batch_data[:10])\n",
    "start = time.time()\n",
    "results_batch = fit_batch(y_batch_data)\n",
    "results_batch[0].block_until_ready()\n",
    "time_batch = time.time() - start\n",
    "print(\n",
    "    f\"  Time for {n_datasets} datasets: {time_batch * 1000:.0f} ms ({time_batch * 1000 / n_datasets:.3f} ms/fit)\"\n",
    ")\n",
    "print(f\"  Throughput: {n_datasets / time_batch:.0f} fits/second\")\n",
    "print()\n",
    "estimated_sequential_time = time_sequential * n_datasets / 100\n",
    "speedup = estimated_sequential_time / time_batch\n",
    "print(f\"Speedup: {speedup:.0f}x faster with vmap + JIT ✓\")\n",
    "print()\n",
    "print(\"Key insight: vmap parallelizes across datasets, JIT compiles once\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85b6ea5",
   "metadata": {},
   "source": [
    "Part 4: Memory Optimization\n",
    "\n",
    "Avoiding out-of-memory (OOM) errors with large datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3912ff7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T21:10:36.551941Z",
     "iopub.status.busy": "2025-11-18T21:10:36.551831Z",
     "iopub.status.idle": "2025-11-18T21:10:36.608144Z",
     "shell.execute_reply": "2025-11-18T21:10:36.607629Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Memory Optimization Strategies:\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"1. Use float32 instead of float64:\")\n",
    "x_f64 = jnp.array([1.0, 2.0, 3.0], dtype=jnp.float64)\n",
    "x_f32 = jnp.array([1.0, 2.0, 3.0], dtype=jnp.float32)\n",
    "print(f\"   float64 memory: {x_f64.nbytes} bytes per element\")\n",
    "print(f\"   float32 memory: {x_f32.nbytes} bytes per element\")\n",
    "print(f\"   Savings: {(1 - x_f32.nbytes / x_f64.nbytes) * 100:.0f}%\")\n",
    "print(\"   → Use float32 unless high precision is critical\\n\")\n",
    "print(\"2. Process data in chunks (streaming):\")\n",
    "print(\"   # For very large datasets (millions of points)\")\n",
    "print(\"   chunk_size = 100000\")\n",
    "print(\"   for i in range(0, len(data), chunk_size):\")\n",
    "print(\"       chunk = data[i:i+chunk_size]\")\n",
    "print(\"       result = fit(chunk)\")\n",
    "print(\"       results.append(result)\\n\")\n",
    "print(\"3. Clear JAX cache if needed:\")\n",
    "print(\"   from jax import clear_caches\")\n",
    "print(\"   clear_caches()  # Frees compilation cache\\n\")\n",
    "print(\"4. Monitor memory usage:\")\n",
    "def get_array_memory_mb(arr):\n",
    "    return arr.nbytes / (1024**2)\n",
    "large_array = jnp.ones((10000, 1000), dtype=jnp.float32)\n",
    "print(\n",
    "    f\"   Example: {large_array.shape} array uses {get_array_memory_mb(large_array):.1f} MB\"\n",
    ")\n",
    "print()\n",
    "print(\"5. Typical memory requirements:\")\n",
    "print(\"   10K points:     ~0.1 MB (negligible)\")\n",
    "print(\"   1M points:      ~10 MB (easy)\")\n",
    "print(\"   100M points:    ~1 GB (manageable)\")\n",
    "print(\"   1B points:      ~10 GB (need chunking or distributed)\")\n",
    "print()\n",
    "print(\"→ For datasets >100M points, use chunked processing or streaming\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7ce175",
   "metadata": {},
   "source": [
    "Part 5: Performance Benchmarking\n",
    "\n",
    "Systematic performance measurement and optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d75a73",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T21:10:36.609782Z",
     "iopub.status.busy": "2025-11-18T21:10:36.609633Z",
     "iopub.status.idle": "2025-11-18T21:10:44.219386Z",
     "shell.execute_reply": "2025-11-18T21:10:44.218934Z"
    }
   },
   "outputs": [],
   "source": [
    "def benchmark_nlsq(n_points_list, n_params=2, n_runs=5):\n",
    "    \"\"\"Benchmark NLSQ across different problem sizes.\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_points_list : list\n",
    "        List of dataset sizes to test\n",
    "    n_params : int\n",
    "        Number of parameters to fit\n",
    "    n_runs : int\n",
    "        Number of runs to average\n",
    "    Returns\n",
    "    -------\n",
    "    results : dict\n",
    "        Benchmark results\n",
    "    \"\"\"\n",
    "    results = {\"n_points\": [], \"mean_time_ms\": [], \"std_time_ms\": []}\n",
    "    cf_bench = CurveFit()\n",
    "    for n_points in n_points_list:\n",
    "        x = jnp.linspace(0, 5, n_points)\n",
    "        y = 3.0 * jnp.exp(-0.5 * x) + np.random.normal(0, 0.1, n_points)\n",
    "        _ = cf_bench.curve_fit(exponential_model, x, y, p0=[2.0, 0.3], maxiter=20)\n",
    "        times = []\n",
    "        for _ in range(n_runs):\n",
    "            start = time.time()\n",
    "            popt, _ = cf_bench.curve_fit(\n",
    "                exponential_model, x, y, p0=[2.0, 0.3], maxiter=20\n",
    "            )\n",
    "            times.append((time.time() - start) * 1000)\n",
    "        results[\"n_points\"].append(n_points)\n",
    "        results[\"mean_time_ms\"].append(np.mean(times))\n",
    "        results[\"std_time_ms\"].append(np.std(times))\n",
    "    return results\n",
    "print(\"Running comprehensive benchmark...\")\n",
    "print(\"(This may take 30-60 seconds)\")\n",
    "print()\n",
    "sizes = [100, 500, 1000, 5000, 10000]\n",
    "bench_results = benchmark_nlsq(sizes, n_runs=5)\n",
    "print(\"Benchmark Results:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'N Points':<12} {'Mean Time (ms)':<20} {'Throughput (fits/s)'}\")\n",
    "print(\"-\" * 60)\n",
    "for i, n in enumerate(bench_results[\"n_points\"]):\n",
    "    mean_t = bench_results[\"mean_time_ms\"][i]\n",
    "    std_t = bench_results[\"std_time_ms\"][i]\n",
    "    throughput = 1000 / mean_t\n",
    "    print(f\"{n:<12} {mean_t:>8.2f} ± {std_t:<8.2f} {throughput:>12.1f}\")\n",
    "print()\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "ax1.errorbar(\n",
    "    bench_results[\"n_points\"],\n",
    "    bench_results[\"mean_time_ms\"],\n",
    "    yerr=bench_results[\"std_time_ms\"],\n",
    "    marker=\"o\",\n",
    "    capsize=5,\n",
    "    label=\"NLSQ\",\n",
    ")\n",
    "ax1.set_xlabel(\"Number of Data Points\")\n",
    "ax1.set_ylabel(\"Time (ms)\")\n",
    "ax1.set_title(\"Performance Scaling\")\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "ax2.loglog(bench_results[\"n_points\"], bench_results[\"mean_time_ms\"], \"o-\", label=\"NLSQ\")\n",
    "ax2.set_xlabel(\"Number of Data Points\")\n",
    "ax2.set_ylabel(\"Time (ms)\")\n",
    "ax2.set_title(\"Scaling Behavior (log-log)\")\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3, which=\"both\")\n",
    "plt.tight_layout()\n",
    "# Create figures directory relative to notebook location\n",
    "fig_dir = Path(\"figures\") / \"gpu_optimization_deep_dive\"\n",
    "fig_dir.mkdir(parents=True, exist_ok=True)\n",
    "plt.savefig(fig_dir / \"fig_01.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "print(\"Interpretation:\")\n",
    "print(\"  - Nearly flat scaling: Well-optimized (GPU benefits)\")\n",
    "print(\"  - Linear scaling: Expected for iterative optimization\")\n",
    "print(\"  - Superlinear scaling: May indicate memory issues or poor caching\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae256ce6",
   "metadata": {},
   "source": [
    "Summary and Best Practices\n",
    "\n",
    "Performance Optimization Checklist\n",
    "\n",
    "**For Maximum Speed:**\n",
    "\n",
    "1. ✅ **Use GPU** if available (5-50x speedup for large problems)\n",
    "2. ✅ **Keep array shapes consistent** to avoid recompilation\n",
    "3. ✅ **Use float32** unless high precision is needed (2x memory savings)\n",
    "4. ✅ **Batch process** with `vmap` for multiple datasets (10-100x faster)\n",
    "5. ✅ **Warm up JIT** with small dataset before benchmarking\n",
    "6. ✅ **Use `block_until_ready()`** when timing (JAX is async)\n",
    "\n",
    "**For Large Datasets:**\n",
    "\n",
    "1. ✅ **Chunk data** if >100M points\n",
    "2. ✅ **Monitor memory** usage\n",
    "3. ✅ **Consider downsampling** for smooth, oversampled data\n",
    "4. ✅ **Use streaming** for datasets that don't fit in memory\n",
    "\n",
    "Performance Expectations\n",
    "\n",
    "| **Scenario** | **Typical Time** | **Optimization** |\n",
    "|--------------|------------------|------------------|\n",
    "| First call (cold start) | 0.5-2 seconds | Expected (JIT compilation) |\n",
    "| Subsequent calls (warm) | 1-50 ms | Cached compilation |\n",
    "| Large dataset (10K points) | 5-100 ms | Use GPU if available |\n",
    "| Batch (1000 fits) | 100-5000 ms | Use vmap for parallelization |\n",
    "| Huge dataset (1M points) | 50-500 ms | GPU + chunking |\n",
    "\n",
    "Troubleshooting Performance Issues\n",
    "\n",
    "**Problem**: First call is slow (>5 seconds)\n",
    "- **Solution**: Normal for JIT. Subsequent calls will be fast.\n",
    "\n",
    "**Problem**: All calls are slow (>1 second for small data)\n",
    "- **Solution**: Check if recompiling each time (varying shapes/dtypes)\n",
    "\n",
    "**Problem**: Out of memory errors\n",
    "- **Solution**: Use float32, chunk data, or downsample\n",
    "\n",
    "**Problem**: GPU not being used\n",
    "- **Solution**: Check `jax.devices()`, install jax[cuda] or jax[rocm]\n",
    "\n",
    "**Problem**: Batch processing not faster than sequential\n",
    "- **Solution**: Problem may be too small, try larger batches or datasets\n",
    "\n",
    "Advanced Profiling\n",
    "\n",
    "For detailed profiling:\n",
    "\n",
    "```python\n",
    "JAX profiling (requires jax[profiling])\n",
    "import jax.profiler\n",
    "\n",
    "Profile a code block\n",
    "with jax.profiler.trace(\"/tmp/jax-trace\", create_perfetto_link=True):\n",
    "Your NLSQ code here\n",
    "popt, pcov = cf.curve_fit(model, x, y, p0=...)\n",
    "\n",
    "Opens profiling UI in browser\n",
    "```\n",
    "\n",
    "Production Recommendations\n",
    "\n",
    "```python\n",
    "Example: Optimized production setup\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from nlsq import CurveFit\n",
    "\n",
    "Configure JAX for production\n",
    "jax.config.update('jax_enable_x64', False)  # Use float32\n",
    "\n",
    "Pre-warm JIT cache at startup\n",
    "cf = CurveFit()\n",
    "x_dummy = jnp.linspace(0, 1, 100)\n",
    "y_dummy = jnp.ones(100)\n",
    "_ = cf.curve_fit(model, x_dummy, y_dummy, p0=initial_guess)\n",
    "\n",
    "Now ready for fast production fitting\n",
    "```\n",
    "\n",
    "Next Steps\n",
    "\n",
    "- **Scale up**: Try batch processing 10,000+ datasets with vmap\n",
    "- **Optimize models**: Simplify model functions for faster evaluation\n",
    "- **Profile**: Use JAX profiler to identify bottlenecks\n",
    "- **Distribute**: For massive scale, consider JAX's `pmap` for multi-GPU\n",
    "\n",
    "References\n",
    "\n",
    "1. **JAX Performance**: https://jax.readthedocs.io/en/latest/notebooks/thinking_in_jax.html\n",
    "2. **JAX Profiling**: https://jax.readthedocs.io/en/latest/profiling.html\n",
    "3. **GPU Acceleration**: https://jax.readthedocs.io/en/latest/gpu_performance_tips.html\n",
    "4. **Related examples**:\n",
    "- `custom_algorithms_advanced.ipynb` - vmap for batch fitting\n",
    "- `troubleshooting_guide.ipynb` - Performance debugging\n",
    "\n",
    "---\n",
    "\n",
    "**Remember**: Premature optimization is the root of all evil. Profile first, optimize what matters!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
