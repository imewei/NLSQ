{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gpu Optimization Deep Dive\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\nConverted from gpu_optimization_deep_dive.ipynb\n\nThis script was automatically generated from a Jupyter notebook.\nPlots are saved to the figures/ directory instead of displayed inline.\n\"\"\"\n\n# ======================================================================\n# # GPU Optimization and Performance Deep Dive\n#\n# **Level**: Advanced\n# **Time**: 50-70 minutes\n# **Prerequisites**: NLSQ Quickstart, JAX basics\n#\n# ## Overview\n#\n# This tutorial covers **performance optimization** for NLSQ, focusing on:\n# - JAX JIT compilation and profiling\n# - GPU acceleration strategies\n# - Memory optimization\n# - Batch processing for maximum throughput\n#\n# ### What You'll Learn\n#\n# 1. **JAX Profiling**: Identifying bottlenecks with JAX tools\n# 2. **JIT Compilation**: Understanding and optimizing compilation\n# 3. **GPU Acceleration**: When and how to leverage GPUs\n# 4. **Memory Management**: Avoiding OOM errors\n# 5. **Batch Strategies**: Processing thousands of fits efficiently\n# 6. **Benchmarking**: Measuring and comparing performance\n#\n# ### Performance Targets\n#\n# Typical NLSQ performance (depends on hardware, problem size):\n# - **Cold start (first call)**: 0.5-2 seconds (includes JIT compilation)\n# - **Warm calls (cached)**: 1-50 ms per fit\n# - **GPU speedup**: 5-50x for large batches vs CPU\n# - **Batch throughput**: 100-10,000 fits/second (GPU, batched)\n#\n# ### Hardware Requirements\n#\n# This notebook runs on CPU or GPU. GPU examples automatically fall back to CPU if no GPU is available.\n# ======================================================================\n# Configure matplotlib for inline plotting in VS Code/Jupyter\n# MUST come before importing matplotlib\nimport os\nimport time\nfrom pathlib import Path\n\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom jax import jit, vmap\n\nfrom nlsq import CurveFit\n\n# Detect available devices\ndevices = jax.devices()\nhas_gpu = any(\"gpu\" in str(d).lower() for d in devices)\n\nQUICK = os.environ.get(\"NLSQ_EXAMPLES_QUICK\") == \"1\"\nMAX_SAMPLES = int(os.environ.get(\"NLSQ_EXAMPLES_MAX_SAMPLES\", \"300000\"))\n\n\ndef cap_samples(n: int) -> int:\n    return min(n, MAX_SAMPLES) if QUICK else n\n\n\nprint(\"Hardware Configuration:\")\nprint(f\"  JAX version: {jax.__version__}\")\nprint(f\"  Default backend: {jax.default_backend()}\")\nprint(f\"  Available devices: {devices}\")\nprint(f\"  GPU available: {'\u2713 Yes' if has_gpu else '\u2717 No (will use CPU)'}\")\nprint()\n\nif has_gpu:\n    print(\"GPU detected - examples will show GPU acceleration\")\nelse:\n    print(\"Running on CPU - GPU examples will still work but won't show speedup\")\n    print(\"To use GPU: Install jax[cuda] or jax[rocm] depending on your hardware\")\n\n\n# ======================================================================\n# ## Part 1: JIT Compilation Basics\n#\n# Understanding JAX's Just-In-Time (JIT) compilation is crucial for performance.\n# ======================================================================\n\n\n# Demonstrating JIT compilation overhead and benefits\n\n\n# Simple model\ndef exponential_model(x, a, b):\n    return a * jnp.exp(-b * x)\n\n\n# Test data\nx_test = jnp.linspace(0, 5, cap_samples(1000))\ny_test = exponential_model(x_test, 3.0, 0.5) + np.random.normal(0, 0.1, len(x_test))\n\ncf = CurveFit()\n\nprint(\"JIT Compilation Analysis:\")\nprint(\"=\" * 60)\n\n# First call: includes compilation time\nstart = time.time()\npopt1, _ = cf.curve_fit(exponential_model, x_test, y_test, p0=[2.0, 0.3])\ntime_first = (time.time() - start) * 1000  # ms\n\n# Second call: uses cached compilation\nstart = time.time()\npopt2, _ = cf.curve_fit(exponential_model, x_test, y_test, p0=[2.5, 0.4])\ntime_second = (time.time() - start) * 1000  # ms\n\n# Third call: still cached\nstart = time.time()\npopt3, _ = cf.curve_fit(exponential_model, x_test, y_test, p0=[3.0, 0.5])\ntime_third = (time.time() - start) * 1000  # ms\n\nprint(f\"First call (cold):  {time_first:.1f} ms (includes JIT compilation)\")\nprint(f\"Second call (warm): {time_second:.1f} ms (cached)\")\nprint(f\"Third call (warm):  {time_third:.1f} ms (cached)\")\nprint()\nprint(f\"Speedup after JIT:  {time_first / time_second:.1f}x\")\nprint(\n    f\"Compilation overhead: {time_first - time_second:.1f} ms ({(time_first - time_second) / time_first * 100:.1f}% of first call)\"\n)\nprint()\nprint(\"Key insight: First call is slow due to JIT compilation.\")\nprint(\"            Subsequent calls are much faster (10-100x).\")\n\n\n# Understanding what triggers recompilation\n\nprint(\"Recompilation Triggers:\")\nprint(\"=\" * 60)\n\n# Trigger 1: Different array shapes\nprint(\"\\n1. Changing array shapes triggers recompilation:\")\n\nx_100 = jnp.linspace(0, 5, 100)\ny_100 = exponential_model(x_100, 3.0, 0.5) + np.random.normal(0, 0.1, 100)\n\nx_200 = jnp.linspace(0, 5, cap_samples(200))\ny_200 = exponential_model(x_200, 3.0, 0.5) + np.random.normal(0, 0.1, cap_samples(200))\n\ncf_new = CurveFit()\n\nstart = time.time()\ncf_new.curve_fit(exponential_model, x_100, y_100, p0=[2.0, 0.3])\ntime_100 = (time.time() - start) * 1000\n\nstart = time.time()\ncf_new.curve_fit(exponential_model, x_200, y_200, p0=[2.0, 0.3])  # Different shape!\ntime_200 = (time.time() - start) * 1000\n\nstart = time.time()\ncf_new.curve_fit(exponential_model, x_200, y_200, p0=[2.5, 0.4])  # Same shape\ntime_200_cached = (time.time() - start) * 1000\n\nprint(f\"  Fit with shape (100,): {time_100:.1f} ms (first compile)\")\nprint(f\"  Fit with shape (200,): {time_200:.1f} ms (recompiled!)\")\nprint(f\"  Fit with shape (200,): {time_200_cached:.1f} ms (cached) \u2713\")\nprint()\nprint(\"  \u2192 Keep array shapes consistent to avoid recompilation\")\n\n# Trigger 2: Different dtypes\nprint(\"\\n2. Changing dtypes triggers recompilation:\")\nprint(\"  float32 vs float64 will trigger separate compilations\")\nprint(\"  \u2192 Use consistent dtype (float32 for GPU, float64 for high precision)\")\n\n# Trigger 3: Different parameter counts\nprint(\"\\n3. Different model signatures trigger recompilation:\")\nprint(\"  model(x, a, b) vs model(x, a, b, c) are compiled separately\")\nprint(\"  \u2192 Expected - different models need different compilations\")\n\n\n# ======================================================================\n# ## Part 2: GPU Acceleration\n#\n# Leverage GPU for massive speedups on large problems.\n# ======================================================================\n\n\n# CPU vs GPU performance comparison\n\n# Large dataset (GPU shines here)\nn_points = cap_samples(10000)\nx_large = jnp.linspace(0, 10, n_points)\ny_large = (\n    3.0 * jnp.exp(-0.5 * x_large)\n    + 2.0 * jnp.sin(x_large)\n    + np.random.normal(0, 0.1, n_points)\n)\n\n\ndef complex_model(x, a, b, c, d):\n    return a * jnp.exp(-b * x) + c * jnp.sin(d * x)\n\n\nprint(f\"GPU Acceleration Benchmark (n_points={n_points}):\")\nprint(\"=\" * 60)\n\n# Ensure compilation is done (use same settings as benchmark for consistency)\ncf_gpu = CurveFit()\ntry:\n    _ = cf_gpu.curve_fit(\n        complex_model,\n        x_large[:100],\n        y_large[:100],\n        p0=[3, 0.5, 2, 1],\n        maxiter=20 if QUICK else 50,\n        max_nfev=200 if QUICK else 1000,\n    )\nexcept Exception as exc:\n    print(f\"Warmup fit skipped: {exc}\")\n\n# Benchmark: 10 fits (reduced in quick mode)\nn_runs = 3 if QUICK else 10\ntimes = []\n\nfor i in range(n_runs):\n    # Slightly vary initial guess to avoid trivial caching\n    p0 = [3.0 + i * 0.1, 0.5, 2.0, 1.0]\n    start = time.time()\n    popt, _ = cf_gpu.curve_fit(\n        complex_model, x_large, y_large, p0=p0, maxiter=20 if QUICK else 50\n    )\n    times.append((time.time() - start) * 1000)\n\nmean_time = np.mean(times)\nstd_time = np.std(times)\n\nprint(f\"\\nDevice: {jax.devices()[0]}\")\nprint(f\"Average fit time: {mean_time:.1f} \u00b1 {std_time:.1f} ms\")\nprint(f\"Throughput: {1000 / mean_time:.1f} fits/second\")\nprint()\n\nif has_gpu:\n    print(\"\u2713 Running on GPU - performance is optimized\")\n    print(\"  Expected speedup vs CPU: 5-20x for this problem size\")\nelse:\n    print(\"Running on CPU - results are valid but slower than GPU\")\n    print(\"  With GPU: Expect 5-50x speedup for large datasets\")\n\n\n# ======================================================================\n# ## Part 3: Batch Processing Strategies\n#\n# Process thousands of fits efficiently with vectorization.\n# ======================================================================\n\n\n# Batch processing with vmap for maximum throughput\n\nprint(\"Batch Processing Benchmark:\")\nprint(\"=\" * 60)\n\n# Generate batch of datasets\nn_datasets = min(20, cap_samples(200)) if QUICK else max(10, cap_samples(1000))\nn_points_per_dataset = 30 if QUICK else 50\n\nx_batch_data = jnp.linspace(0, 5, n_points_per_dataset)\n\n# Random true parameters for each dataset\nnp.random.seed(42)\na_true_batch = np.random.uniform(2, 4, n_datasets)\nb_true_batch = np.random.uniform(0.3, 0.7, n_datasets)\n\ny_batch_data = jnp.array(\n    [\n        a * jnp.exp(-b * x_batch_data) + np.random.normal(0, 0.05, n_points_per_dataset)\n        for a, b in zip(a_true_batch, b_true_batch, strict=True)\n    ]\n)\n\nprint(f\"Batch size: {n_datasets} datasets\")\nprint(f\"Points per dataset: {n_points_per_dataset}\")\nprint(f\"Total data points: {n_datasets * n_points_per_dataset:,}\")\nprint()\n\n# Method 1: Sequential (slow)\nprint(\"Method 1: Sequential fitting (baseline)\")\nstart = time.time()\nresults_sequential = []\ncf_seq = CurveFit()\nsequential_runs = min(20 if QUICK else 100, n_datasets)\nfor i in range(sequential_runs):  # Only fit a subset for speed\n    popt, _ = cf_seq.curve_fit(\n        exponential_model, x_batch_data, y_batch_data[i], p0=[3.0, 0.5], maxiter=30\n    )\n    results_sequential.append(popt)\ntime_sequential = time.time() - start\n\nprint(\n    f\"  Time for {sequential_runs} datasets: {time_sequential * 1000:.0f} ms \"\n    f\"({time_sequential * 1000 / sequential_runs:.1f} ms/fit)\"\n)\nprint(\n    f\"  Estimated time for {n_datasets}: {time_sequential * n_datasets / sequential_runs:.1f} s\"\n)\nprint()\n\n# Method 2: Vectorized with vmap (fast)\nprint(\"Method 2: Batched fitting with vmap (optimized)\")\n\n\n# Simplified optimizer for vectorization\ndef fit_one_dataset(y_single):\n    \"\"\"Fit single dataset (simplified gradient descent).\"\"\"\n    params = jnp.array([3.0, 0.5])\n\n    def loss(p):\n        return jnp.sum((y_single - exponential_model(x_batch_data, *p)) ** 2)\n\n    # A few gradient descent steps for demonstration\n    for _ in range(5 if QUICK else 20):\n        g = jax.grad(loss)(params)\n        params = params - 0.05 * g\n    return params\n\n\n# Vectorize over batch dimension\nfit_batch = jit(vmap(fit_one_dataset))\n\n# Warm up JIT\nwarmup_size = min(10, n_datasets)\n_ = fit_batch(y_batch_data[:warmup_size])\n\n# Benchmark\nstart = time.time()\nresults_batch = fit_batch(y_batch_data)\n# Block until computation completes (JAX is async)\nresults_batch[0].block_until_ready()\ntime_batch = time.time() - start\n\nprint(\n    f\"  Time for {n_datasets} datasets: {time_batch * 1000:.0f} ms ({time_batch * 1000 / n_datasets:.3f} ms/fit)\"\n)\nprint(f\"  Throughput: {n_datasets / time_batch:.0f} fits/second\")\nprint()\n\n# Speedup\nestimated_sequential_time = time_sequential * n_datasets / sequential_runs\nspeedup = estimated_sequential_time / time_batch\n\nprint(f\"Speedup: {speedup:.0f}x faster with vmap + JIT \u2713\")\nprint()\nprint(\"Key insight: vmap parallelizes across datasets, JIT compiles once\")\n\n\n# ======================================================================\n# ## Part 4: Memory Optimization\n#\n# Avoiding out-of-memory (OOM) errors with large datasets.\n# ======================================================================\n\n\n# Memory optimization strategies\n\nprint(\"Memory Optimization Strategies:\")\nprint(\"=\" * 60)\nprint()\n\nprint(\"1. Use float32 instead of float64:\")\nx_f64 = jnp.array([1.0, 2.0, 3.0], dtype=jnp.float64)\nx_f32 = jnp.array([1.0, 2.0, 3.0], dtype=jnp.float32)\nprint(f\"   float64 memory: {x_f64.nbytes} bytes per element\")\nprint(f\"   float32 memory: {x_f32.nbytes} bytes per element\")\nprint(f\"   Savings: {(1 - x_f32.nbytes / x_f64.nbytes) * 100:.0f}%\")\nprint(\"   \u2192 Use float32 unless high precision is critical\\n\")\n\nprint(\"2. Process data in chunks (streaming):\")\nprint(\"   # For very large datasets (millions of points)\")\nprint(\"   chunk_size = 100000\")\nprint(\"   for i in range(0, len(data), chunk_size):\")\nprint(\"       chunk = data[i:i+chunk_size]\")\nprint(\"       result = fit(chunk)\")\nprint(\"       results.append(result)\\n\")\n\nprint(\"3. Clear JAX cache if needed:\")\nprint(\"   from jax import clear_caches\")\nprint(\"   clear_caches()  # Frees compilation cache\\n\")\n\nprint(\"4. Monitor memory usage:\")\n\n\ndef get_array_memory_mb(arr):\n    return arr.nbytes / (1024**2)\n\n\nlarge_array = jnp.ones((cap_samples(10000), cap_samples(1000)), dtype=jnp.float32)\nprint(\n    f\"   Example: {large_array.shape} array uses {get_array_memory_mb(large_array):.1f} MB\"\n)\nprint()\n\nprint(\"5. Typical memory requirements:\")\nprint(\"   10K points:     ~0.1 MB (negligible)\")\nprint(\"   1M points:      ~10 MB (easy)\")\nprint(\"   100M points:    ~1 GB (manageable)\")\nprint(\"   1B points:      ~10 GB (need chunking or distributed)\")\nprint()\nprint(\"\u2192 For datasets >100M points, use chunked processing or streaming\")\n\n\n# ======================================================================\n# ## Part 5: Performance Benchmarking\n#\n# Systematic performance measurement and optimization.\n# ======================================================================\n\n\n# Comprehensive performance benchmark\n\n\ndef benchmark_nlsq(n_points_list, n_params=2, n_runs=5):\n    \"\"\"Benchmark NLSQ across different problem sizes.\n\n    Parameters\n    ----------\n    n_points_list : list\n        List of dataset sizes to test\n    n_params : int\n        Number of parameters to fit\n    n_runs : int\n        Number of runs to average\n\n    Returns\n    -------\n    results : dict\n        Benchmark results\n    \"\"\"\n    results = {\"n_points\": [], \"mean_time_ms\": [], \"std_time_ms\": []}\n\n    cf_bench = CurveFit()\n\n    for n_points in n_points_list:\n        x = jnp.linspace(0, 5, n_points)\n        y = 3.0 * jnp.exp(-0.5 * x) + np.random.normal(0, 0.1, n_points)\n\n        # Warm up\n        _ = cf_bench.curve_fit(exponential_model, x, y, p0=[2.0, 0.3], maxiter=20)\n\n        # Benchmark\n        times = []\n        for _ in range(n_runs):\n            start = time.time()\n            popt, _ = cf_bench.curve_fit(\n                exponential_model, x, y, p0=[2.0, 0.3], maxiter=20\n            )\n            # Note: popt is numpy array (already synchronous), no need for block_until_ready\n            times.append((time.time() - start) * 1000)\n\n        results[\"n_points\"].append(n_points)\n        results[\"mean_time_ms\"].append(np.mean(times))\n        results[\"std_time_ms\"].append(np.std(times))\n\n    return results\n\n\nprint(\"Running comprehensive benchmark...\")\nprint(\"(This may take 30-60 seconds in full mode)\")\nprint()\n\n# Test different problem sizes\nsize_candidates = [50, 100, 200] if QUICK else [100, 500, 1000, 5000, 10000]\nsizes = sorted({cap_samples(s) for s in size_candidates})\nbench_results = benchmark_nlsq(sizes, n_runs=2 if QUICK else 5)\n\n# Display results\nprint(\"Benchmark Results:\")\nprint(\"=\" * 60)\nprint(f\"{'N Points':<12} {'Mean Time (ms)':<20} {'Throughput (fits/s)'}\")\nprint(\"-\" * 60)\n\nfor i, n in enumerate(bench_results[\"n_points\"]):\n    mean_t = bench_results[\"mean_time_ms\"][i]\n    std_t = bench_results[\"std_time_ms\"][i]\n    throughput = 1000 / mean_t\n    print(f\"{n:<12} {mean_t:>8.2f} \u00b1 {std_t:<8.2f} {throughput:>12.1f}\")\n\nprint()\n\n# Plot scaling\n_, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n# Time vs problem size\nax1.errorbar(\n    bench_results[\"n_points\"],\n    bench_results[\"mean_time_ms\"],\n    yerr=bench_results[\"std_time_ms\"],\n    marker=\"o\",\n    capsize=5,\n    label=\"NLSQ\",\n)\nax1.set_xlabel(\"Number of Data Points\")\nax1.set_ylabel(\"Time (ms)\")\nax1.set_title(\"Performance Scaling\")\nax1.legend()\nax1.grid(alpha=0.3)\n\n# Log-log plot to see scaling behavior\nax2.loglog(bench_results[\"n_points\"], bench_results[\"mean_time_ms\"], \"o-\", label=\"NLSQ\")\nax2.set_xlabel(\"Number of Data Points\")\nax2.set_ylabel(\"Time (ms)\")\nax2.set_title(\"Scaling Behavior (log-log)\")\nax2.legend()\nax2.grid(alpha=0.3, which=\"both\")\n\nplt.tight_layout()\n# Save figure to file\nfig_dir = Path.cwd() / \"figures\" / \"gpu_optimization_deep_dive\"\nfig_dir.mkdir(parents=True, exist_ok=True)\nplt.savefig(fig_dir / \"fig_01.png\", dpi=300, bbox_inches=\"tight\")\nplt.close()\n\nprint(\"Interpretation:\")\nprint(\"  - Nearly flat scaling: Well-optimized (GPU benefits)\")\nprint(\"  - Linear scaling: Expected for iterative optimization\")\nprint(\"  - Superlinear scaling: May indicate memory issues or poor caching\")\n\n\n# ======================================================================\n# ## Summary and Best Practices\n#\n# ### Performance Optimization Checklist\n#\n# **For Maximum Speed:**\n#\n# 1. \u2705 **Use GPU** if available (5-50x speedup for large problems)\n# 2. \u2705 **Keep array shapes consistent** to avoid recompilation\n# 3. \u2705 **Use float32** unless high precision is needed (2x memory savings)\n# 4. \u2705 **Batch process** with `vmap` for multiple datasets (10-100x faster)\n# 5. \u2705 **Warm up JIT** with small dataset before benchmarking\n# 6. \u2705 **Use `block_until_ready()`** when timing (JAX is async)\n#\n# **For Large Datasets:**\n#\n# 1. \u2705 **Chunk data** if >100M points\n# 2. \u2705 **Monitor memory** usage\n# 3. \u2705 **Consider downsampling** for smooth, oversampled data\n# 4. \u2705 **Use streaming** for datasets that don't fit in memory\n#\n# ### Performance Expectations\n#\n# | **Scenario** | **Typical Time** | **Optimization** |\n# |--------------|------------------|------------------|\n# | First call (cold start) | 0.5-2 seconds | Expected (JIT compilation) |\n# | Subsequent calls (warm) | 1-50 ms | Cached compilation |\n# | Large dataset (10K points) | 5-100 ms | Use GPU if available |\n# | Batch (1000 fits) | 100-5000 ms | Use vmap for parallelization |\n# | Huge dataset (1M points) | 50-500 ms | GPU + chunking |\n#\n# ### Troubleshooting Performance Issues\n#\n# **Problem**: First call is slow (>5 seconds)\n# - **Solution**: Normal for JIT. Subsequent calls will be fast.\n#\n# **Problem**: All calls are slow (>1 second for small data)\n# - **Solution**: Check if recompiling each time (varying shapes/dtypes)\n#\n# **Problem**: Out of memory errors\n# - **Solution**: Use float32, chunk data, or downsample\n#\n# **Problem**: GPU not being used\n# - **Solution**: Check `jax.devices()`, install jax[cuda] or jax[rocm]\n#\n# **Problem**: Batch processing not faster than sequential\n# - **Solution**: Problem may be too small, try larger batches or datasets\n#\n# ### Advanced Profiling\n#\n# For detailed profiling:\n#\n# ```python\n# # JAX profiling (requires jax[profiling])\n# import jax.profiler\n#\n# # Profile a code block\n# with jax.profiler.trace(\"/tmp/jax-trace\", create_perfetto_link=True):\n#     # Your NLSQ code here\n#     popt, pcov = cf.curve_fit(model, x, y, p0=...)\n#\n# # Opens profiling UI in browser\n# ```\n#\n# ### Production Recommendations\n#\n# ```python\n# # Example: Optimized production setup\n# import jax\n# import jax.numpy as jnp\n# from nlsq import CurveFit\n#\n# # Configure JAX for production\n# jax.config.update('jax_enable_x64', False)  # Use float32\n#\n# # Pre-warm JIT cache at startup\n# cf = CurveFit()\n# x_dummy = jnp.linspace(0, 1, 100)\n# y_dummy = jnp.ones(100)\n# _ = cf.curve_fit(model, x_dummy, y_dummy, p0=initial_guess)\n#\n# # Now ready for fast production fitting\n# ```\n#\n# ### Next Steps\n#\n# - **Scale up**: Try batch processing 10,000+ datasets with vmap\n# - **Optimize models**: Simplify model functions for faster evaluation\n# - **Profile**: Use JAX profiler to identify bottlenecks\n# - **Distribute**: For massive scale, consider JAX's `pmap` for multi-GPU\n#\n# ### References\n#\n# 1. **JAX Performance**: https://jax.readthedocs.io/en/latest/notebooks/thinking_in_jax.html\n# 2. **JAX Profiling**: https://jax.readthedocs.io/en/latest/profiling.html\n# 3. **GPU Acceleration**: https://jax.readthedocs.io/en/latest/gpu_performance_tips.html\n# 4. **Related examples**:\n#    - `custom_algorithms_advanced.ipynb` - vmap for batch fitting\n#    - `troubleshooting_guide.ipynb` - Performance debugging\n#\n# ---\n#\n# **Remember**: Premature optimization is the root of all evil. Profile first, optimize what matters!\n# ======================================================================\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}