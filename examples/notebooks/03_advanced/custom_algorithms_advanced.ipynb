{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "custom-algorithms-header",
   "metadata": {},
   "source": [
    "# Custom Algorithms and Advanced Extensions\n",
    "\n",
    "**Level**: Advanced / Research  \n",
    "**Time**: 60-90 minutes  \n",
    "**Prerequisites**: NLSQ Quickstart, JAX fundamentals, optimization theory\n",
    "\n",
    "## Overview\n",
    "\n",
    "This tutorial is for **researchers and advanced users** who want to:\n",
    "- Implement custom optimization algorithms\n",
    "- Design specialized loss functions\n",
    "- Extend NLSQ for novel applications\n",
    "- Understand NLSQ's internals for research\n",
    "\n",
    "### What You'll Learn\n",
    "\n",
    "1. **NLSQ Architecture**: Understanding the optimization backend\n",
    "2. **Custom Loss Functions**: Beyond least squares\n",
    "3. **Custom Optimizers**: Implementing specialized algorithms\n",
    "4. **Advanced JAX Patterns**: Efficient curve fitting with JAX\n",
    "5. **Research Extensions**: Constrained optimization, robust methods\n",
    "\n",
    "### Use Cases\n",
    "\n",
    "- **Custom loss**: Asymmetric penalties, quantile regression, robust M-estimators\n",
    "- **Specialized optimizers**: Trust-region methods, second-order algorithms\n",
    "- **Constrained problems**: Inequality constraints, manifold optimization\n",
    "- **Novel applications**: Bayesian inference, inverse problems, PDE-constrained optimization\n",
    "\n",
    "### Warning\n",
    "\n",
    "This is advanced material. Modifying optimization algorithms requires solid understanding of:\n",
    "- Optimization theory (convexity, convergence, gradients)\n",
    "- JAX programming model (JIT, grad, pytrees)\n",
    "- Numerical stability considerations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "imports",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T22:47:01.178792Z",
     "iopub.status.busy": "2025-11-17T22:47:01.178494Z",
     "iopub.status.idle": "2025-11-17T22:47:02.151481Z",
     "shell.execute_reply": "2025-11-17T22:47:02.150680Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u26a0 Optax not available - install with: pip install optax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:2025-11-17 16:47:02,016:jax._src.xla_bridge:808: Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to initialize backend 'tpu': INTERNAL: Failed to open libtpu.so: libtpu.so: cannot open shared object file: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Imports successful\n",
      "  JAX version: 0.8.0\n",
      "  JAX devices: [CudaDevice(id=0)]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Advanced imports for custom algorithms.\"\"\"\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from jax import grad, jit, value_and_grad, vmap\n",
    "\n",
    "# Optimization libraries\n",
    "try:\n",
    "    import optax\n",
    "\n",
    "    OPTAX_AVAILABLE = True\n",
    "except ImportError:\n",
    "    OPTAX_AVAILABLE = False\n",
    "    print(\"\u26a0 Optax not available - install with: pip install optax\")\n",
    "\n",
    "from nlsq import CurveFit\n",
    "\n",
    "print(\"\u2713 Imports successful\")\n",
    "print(f\"  JAX version: {jax.__version__}\")\n",
    "print(f\"  JAX devices: {jax.devices()}\")\n",
    "if OPTAX_AVAILABLE:\n",
    "    print(f\"  Optax version: {optax.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part1-header",
   "metadata": {},
   "source": [
    "## Part 1: Understanding NLSQ's Optimization Backend\n",
    "\n",
    "Before customizing, let's understand how NLSQ works internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "nlsq-internals",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T22:47:02.154120Z",
     "iopub.status.busy": "2025-11-17T22:47:02.153694Z",
     "iopub.status.idle": "2025-11-17T22:47:04.478553Z",
     "shell.execute_reply": "2025-11-17T22:47:04.478004Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting curve fit | {'n_params': 2, 'n_data_points': 30, 'method': 'trf', 'solver': 'auto', 'batch_size': None, 'has_bounds': False, 'dynamic_sizing': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting least squares optimization | {'method': 'trf', 'n_params': 2, 'loss': 'linear', 'ftol': 1e-08, 'xtol': 1e-08, 'gtol': 1e-08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting TRF optimization (no bounds) | {'n_params': 2, 'n_residuals': 30, 'max_nfev': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization: iter=0 | cost=1.838322e+00 | \u2016\u2207f\u2016=3.612317e+00 | nfev=1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization: iter=1 | cost=1.608937e-01 | \u2016\u2207f\u2016=2.368500e+00 | step=2.022375e+00 | nfev=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization: iter=2 | cost=1.240364e-01 | \u2016\u2207f\u2016=8.343415e-02 | step=2.022375e+00 | nfev=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization: iter=3 | cost=1.239985e-01 | \u2016\u2207f\u2016=1.351489e-03 | step=2.022375e+00 | nfev=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization: iter=4 | cost=1.239985e-01 | \u2016\u2207f\u2016=3.548408e-05 | step=2.022375e+00 | nfev=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Timer: optimization took 1.050196s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Convergence: reason=`ftol` termination condition is satisfied. | iterations=5 | final_cost=1.239985e-01 | time=1.050s | final_gradient_norm=9.111268229541891e-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Timer: curve_fit took 1.349427s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Curve fit completed | {'total_time': 1.3494267700007185, 'final_cost': 0.24799692070934473, 'covariance_warning': False}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard NLSQ Fit:\n",
      "  Parameters: a=2.946, b=0.478\n",
      "  Covariance matrix shape: (2, 2)\n",
      "\n",
      "NLSQ Internal Workflow:\n",
      "1. Residual function: r(\u03b8) = y_data - model(x_data, \u03b8)\n",
      "2. Loss function: L(\u03b8) = 0.5 * sum(r(\u03b8)^2)\n",
      "3. Gradient: \u2207L(\u03b8) = -J^T r(\u03b8) where J = \u2202model/\u2202\u03b8\n",
      "4. Optimization: Levenberg-Marquardt or similar\n",
      "5. Uncertainty: pcov = (J^T J)^(-1) * \u03c3^2\n",
      "\n",
      "Manual computation with JAX:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Gradient at optimum: [3.06400738e-13 9.11126823e-07]\n",
      "  Gradient norm: 9.11e-07 (should be \u2248 0)\n",
      "  \u2192 Confirms NLSQ found a critical point where \u2207L = 0 \u2713\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Exploring NLSQ internals.\"\"\"\n",
    "\n",
    "# Simple problem: fit exponential\n",
    "x_data = jnp.linspace(0, 5, 30)\n",
    "y_true = 3.0 * jnp.exp(-0.5 * x_data)\n",
    "y_data = y_true + np.random.normal(0, 0.1, len(x_data))\n",
    "\n",
    "\n",
    "def exponential(x, a, b):\n",
    "    return a * jnp.exp(-b * x)\n",
    "\n",
    "\n",
    "# Standard NLSQ fit\n",
    "cf = CurveFit()\n",
    "popt, pcov = cf.curve_fit(exponential, x_data, y_data, p0=[2.0, 0.3])\n",
    "\n",
    "print(\"Standard NLSQ Fit:\")\n",
    "print(f\"  Parameters: a={popt[0]:.3f}, b={popt[1]:.3f}\")\n",
    "print(f\"  Covariance matrix shape: {pcov.shape}\")\n",
    "print()\n",
    "\n",
    "# How NLSQ works internally (simplified):\n",
    "print(\"NLSQ Internal Workflow:\")\n",
    "print(\"1. Residual function: r(\u03b8) = y_data - model(x_data, \u03b8)\")\n",
    "print(\"2. Loss function: L(\u03b8) = 0.5 * sum(r(\u03b8)^2)\")\n",
    "print(\"3. Gradient: \u2207L(\u03b8) = -J^T r(\u03b8) where J = \u2202model/\u2202\u03b8\")\n",
    "print(\"4. Optimization: Levenberg-Marquardt or similar\")\n",
    "print(\"5. Uncertainty: pcov = (J^T J)^(-1) * \u03c3^2\")\n",
    "print()\n",
    "\n",
    "# Let's compute these manually with JAX\n",
    "print(\"Manual computation with JAX:\")\n",
    "\n",
    "\n",
    "def residual_fn(params, x, y):\n",
    "    \"\"\"Residual vector r(\u03b8) = y - model(x, \u03b8).\"\"\"\n",
    "    a, b = params\n",
    "    y_pred = exponential(x, a, b)\n",
    "    return y - y_pred\n",
    "\n",
    "\n",
    "def loss_fn(params, x, y):\n",
    "    \"\"\"Sum of squared residuals L(\u03b8) = 0.5 * ||r(\u03b8)||^2.\"\"\"\n",
    "    r = residual_fn(params, x, y)\n",
    "    return 0.5 * jnp.sum(r**2)\n",
    "\n",
    "\n",
    "# Compute gradient at fitted parameters\n",
    "grad_fn = grad(loss_fn)\n",
    "gradient = grad_fn(popt, x_data, y_data)\n",
    "\n",
    "print(f\"  Gradient at optimum: {gradient}\")\n",
    "print(f\"  Gradient norm: {jnp.linalg.norm(gradient):.2e} (should be \u2248 0)\")\n",
    "print(\"  \u2192 Confirms NLSQ found a critical point where \u2207L = 0 \u2713\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part2-header",
   "metadata": {},
   "source": [
    "## Part 2: Custom Loss Functions\n",
    "\n",
    "Beyond standard least squares, we can implement custom loss functions for specialized needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "custom-loss-robust",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T22:47:04.480596Z",
     "iopub.status.busy": "2025-11-17T22:47:04.480445Z",
     "iopub.status.idle": "2025-11-17T22:47:04.825261Z",
     "shell.execute_reply": "2025-11-17T22:47:04.823936Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u26a0 Install optax to run this example: pip install optax\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Example 1: Robust loss function (Huber loss).\"\"\"\n",
    "\n",
    "# Generate data with outliers\n",
    "x_robust = jnp.linspace(0, 10, 50)\n",
    "y_robust = 2.0 * x_robust + 1.0 + np.random.normal(0, 0.5, 50)\n",
    "# Add outliers (convert indices to JAX array for .at[] indexing)\n",
    "outlier_idx = jnp.array([5, 15, 35, 42])\n",
    "y_robust = y_robust.at[outlier_idx].add(jnp.array([5.0, -6.0, 4.0, -5.0]))\n",
    "\n",
    "\n",
    "def linear_model(x, a, b):\n",
    "    return a * x + b\n",
    "\n",
    "\n",
    "# Standard least squares (sensitive to outliers)\n",
    "def least_squares_loss(params, x, y):\n",
    "    a, b = params\n",
    "    residuals = y - linear_model(x, a, b)\n",
    "    return jnp.sum(residuals**2)\n",
    "\n",
    "\n",
    "# Huber loss (robust to outliers)\n",
    "def huber_loss(params, x, y, delta=1.0):\n",
    "    \"\"\"Huber loss: quadratic for small errors, linear for large.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    delta : float\n",
    "        Threshold for switching from quadratic to linear\n",
    "    \"\"\"\n",
    "    a, b = params\n",
    "    residuals = y - linear_model(x, a, b)\n",
    "    abs_residuals = jnp.abs(residuals)\n",
    "\n",
    "    # Huber function: 0.5*r^2 if |r| <= delta, else delta*(|r| - 0.5*delta)\n",
    "    huber = jnp.where(\n",
    "        abs_residuals <= delta,\n",
    "        0.5 * residuals**2,\n",
    "        delta * (abs_residuals - 0.5 * delta),\n",
    "    )\n",
    "    return jnp.sum(huber)\n",
    "\n",
    "\n",
    "# Optimize with both losses\n",
    "if OPTAX_AVAILABLE:\n",
    "    # Using Optax for custom optimization\n",
    "    def optimize_custom(loss_fn, p0, x, y, n_steps=1000, lr=0.01):\n",
    "        \"\"\"Custom optimizer using Optax Adam.\"\"\"\n",
    "        params = jnp.array(p0)\n",
    "        optimizer = optax.adam(lr)\n",
    "        opt_state = optimizer.init(params)\n",
    "\n",
    "        @jit\n",
    "        def step(params, opt_state):\n",
    "            loss, grads = value_and_grad(loss_fn)(params, x, y)\n",
    "            updates, opt_state = optimizer.update(grads, opt_state)\n",
    "            params = optax.apply_updates(params, updates)\n",
    "            return params, opt_state, loss\n",
    "\n",
    "        losses = []\n",
    "        for i in range(n_steps):\n",
    "            params, opt_state, loss = step(params, opt_state)\n",
    "            if i % 100 == 0:\n",
    "                losses.append(float(loss))\n",
    "\n",
    "        return params, losses\n",
    "\n",
    "    # Fit with both losses\n",
    "    p0 = [1.0, 0.0]\n",
    "    params_ls, losses_ls = optimize_custom(least_squares_loss, p0, x_robust, y_robust)\n",
    "    params_huber, losses_huber = optimize_custom(\n",
    "        lambda p, x, y: huber_loss(p, x, y, delta=1.5), p0, x_robust, y_robust\n",
    "    )\n",
    "\n",
    "    print(\"Least Squares (sensitive to outliers):\")\n",
    "    print(f\"  a={params_ls[0]:.3f}, b={params_ls[1]:.3f}\")\n",
    "    print(\"\\nHuber Loss (robust to outliers):\")\n",
    "    print(f\"  a={params_huber[0]:.3f}, b={params_huber[1]:.3f}\")\n",
    "    print(\"\\nTrue parameters: a=2.0, b=1.0\")\n",
    "\n",
    "    # Visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "    # Fits\n",
    "    x_plot = jnp.linspace(0, 10, 100)\n",
    "    ax1.plot(x_robust, y_robust, \"o\", alpha=0.5, label=\"Data (with outliers)\")\n",
    "    ax1.plot(\n",
    "        x_robust[outlier_idx],\n",
    "        y_robust[outlier_idx],\n",
    "        \"rx\",\n",
    "        ms=12,\n",
    "        mew=3,\n",
    "        label=\"Outliers\",\n",
    "    )\n",
    "    ax1.plot(\n",
    "        x_plot,\n",
    "        linear_model(x_plot, *params_ls),\n",
    "        \"r--\",\n",
    "        lw=2,\n",
    "        label=\"Least Squares\",\n",
    "    )\n",
    "    ax1.plot(\n",
    "        x_plot, linear_model(x_plot, *params_huber), \"g-\", lw=2, label=\"Huber Loss\"\n",
    "    )\n",
    "    ax1.plot(x_plot, 2.0 * x_plot + 1.0, \"k:\", lw=2, label=\"True\")\n",
    "    ax1.set_xlabel(\"x\")\n",
    "    ax1.set_ylabel(\"y\")\n",
    "    ax1.set_title(\"Robust Fitting with Custom Loss\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(alpha=0.3)\n",
    "\n",
    "    # Loss convergence\n",
    "    ax2.semilogy(losses_ls, \"r-\", label=\"Least Squares\")\n",
    "    ax2.semilogy(losses_huber, \"g-\", label=\"Huber Loss\")\n",
    "    ax2.set_xlabel(\"Iteration (\u00d7100)\")\n",
    "    ax2.set_ylabel(\"Loss\")\n",
    "    ax2.set_title(\"Convergence\")\n",
    "    ax2.legend()\n",
    "    ax2.grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\u26a0 Install optax to run this example: pip install optax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "custom-loss-asymmetric",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T22:47:04.828356Z",
     "iopub.status.busy": "2025-11-17T22:47:04.828104Z",
     "iopub.status.idle": "2025-11-17T22:47:04.834334Z",
     "shell.execute_reply": "2025-11-17T22:47:04.833036Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Example 2: Asymmetric loss (safety-critical applications).\"\"\"\n",
    "\n",
    "\n",
    "def asymmetric_loss(params, x, y, alpha=2.0):\n",
    "    \"\"\"Asymmetric quadratic loss.\n",
    "\n",
    "    Penalizes overestimation more than underestimation.\n",
    "    Useful when overestimation is more costly (e.g., drug dosing).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    alpha : float\n",
    "        Asymmetry parameter (alpha > 1 penalizes positive residuals more)\n",
    "    \"\"\"\n",
    "    a, b = params\n",
    "    residuals = y - linear_model(x, a, b)\n",
    "\n",
    "    # Asymmetric penalty\n",
    "    loss = jnp.where(\n",
    "        residuals > 0,  # Overestimation (model too low)\n",
    "        alpha * residuals**2,  # Higher penalty\n",
    "        residuals**2,  # Normal penalty\n",
    "    )\n",
    "    return jnp.sum(loss)\n",
    "\n",
    "\n",
    "if OPTAX_AVAILABLE:\n",
    "    # Fit with asymmetric loss\n",
    "    params_asym, _ = optimize_custom(\n",
    "        lambda p, x, y: asymmetric_loss(p, x, y, alpha=3.0),\n",
    "        [1.0, 0.0],\n",
    "        x_robust,\n",
    "        y_robust,\n",
    "    )\n",
    "\n",
    "    print(\"Asymmetric Loss (penalizes overestimation 3x):\")\n",
    "    print(f\"  a={params_asym[0]:.3f}, b={params_asym[1]:.3f}\")\n",
    "    print(\n",
    "        \"  \u2192 Fit is conservative (tends to underestimate to avoid costly overestimation)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part3-header",
   "metadata": {},
   "source": [
    "## Part 3: Custom Optimization Algorithms\n",
    "\n",
    "Implement specialized optimization algorithms for specific problem structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "custom-optimizer-gd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T22:47:04.837466Z",
     "iopub.status.busy": "2025-11-17T22:47:04.837146Z",
     "iopub.status.idle": "2025-11-17T22:47:05.913328Z",
     "shell.execute_reply": "2025-11-17T22:47:05.912825Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom Gradient Descent with Momentum:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting curve fit | {'n_params': 2, 'n_data_points': 30, 'method': 'trf', 'solver': 'auto', 'batch_size': None, 'has_bounds': False, 'dynamic_sizing': False}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting least squares optimization | {'method': 'trf', 'n_params': 2, 'loss': 'linear', 'ftol': 1e-08, 'xtol': 1e-08, 'gtol': 1e-08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting TRF optimization (no bounds) | {'n_params': 2, 'n_residuals': 30, 'max_nfev': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization: iter=0 | cost=2.849991e+01 | \u2016\u2207f\u2016=3.397073e+01 | nfev=1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Final params: a=nan, b=nan\n",
      "  Optimization steps: 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization: iter=1 | cost=9.529176e+00 | \u2016\u2207f\u2016=2.375174e+01 | step=2.000000e+00 | nfev=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization: iter=2 | cost=1.354591e+00 | \u2016\u2207f\u2016=9.223503e+00 | step=2.000000e+00 | nfev=3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization: iter=3 | cost=1.271935e-01 | \u2016\u2207f\u2016=8.181515e-01 | step=2.000000e+00 | nfev=4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization: iter=4 | cost=1.239988e-01 | \u2016\u2207f\u2016=2.602015e-03 | step=2.000000e+00 | nfev=5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Optimization: iter=5 | cost=1.239985e-01 | \u2016\u2207f\u2016=1.416495e-04 | step=2.000000e+00 | nfev=6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Timer: optimization took 0.252723s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Convergence: reason=`ftol` termination condition is satisfied. | iterations=6 | final_cost=1.239985e-01 | time=0.253s | final_gradient_norm=3.643949659577761e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Timer: curve_fit took 0.320429s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Curve fit completed | {'total_time': 0.3204289339482784, 'final_cost': 0.24799692070958249, 'covariance_warning': False}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NLSQ (Levenberg-Marquardt):\n",
      "  Final params: a=2.946, b=0.478\n",
      "\n",
      "\u2192 Both converge to similar solution \u2713\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Example: Gradient descent with momentum (from scratch).\"\"\"\n",
    "\n",
    "\n",
    "def gradient_descent_momentum(\n",
    "    loss_fn, p0, x, y, lr=0.01, momentum=0.9, n_steps=1000, tol=1e-6\n",
    "):\n",
    "    \"\"\"Gradient descent with momentum optimizer.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    loss_fn : callable\n",
    "        Loss function: loss_fn(params, x, y) -> scalar\n",
    "    p0 : array\n",
    "        Initial parameters\n",
    "    lr : float\n",
    "        Learning rate\n",
    "    momentum : float\n",
    "        Momentum coefficient (0 = no momentum, 0.9 typical)\n",
    "    n_steps : int\n",
    "        Maximum iterations\n",
    "    tol : float\n",
    "        Convergence tolerance on gradient norm\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    params : array\n",
    "        Optimized parameters\n",
    "    history : dict\n",
    "        Optimization history (params, loss, grad_norm)\n",
    "    \"\"\"\n",
    "    params = jnp.array(p0, dtype=jnp.float32)\n",
    "    velocity = jnp.zeros_like(params)\n",
    "\n",
    "    history = {\"params\": [], \"loss\": [], \"grad_norm\": []}\n",
    "\n",
    "    grad_fn = jit(grad(loss_fn))\n",
    "    loss_fn_jit = jit(loss_fn)\n",
    "\n",
    "    for i in range(n_steps):\n",
    "        # Compute gradient\n",
    "        g = grad_fn(params, x, y)\n",
    "        grad_norm = float(jnp.linalg.norm(g))\n",
    "\n",
    "        # Update velocity (momentum)\n",
    "        velocity = momentum * velocity - lr * g\n",
    "\n",
    "        # Update parameters\n",
    "        params = params + velocity\n",
    "\n",
    "        # Record history\n",
    "        if i % 50 == 0:\n",
    "            loss_val = float(loss_fn_jit(params, x, y))\n",
    "            history[\"params\"].append(params.copy())\n",
    "            history[\"loss\"].append(loss_val)\n",
    "            history[\"grad_norm\"].append(grad_norm)\n",
    "\n",
    "        # Check convergence\n",
    "        if grad_norm < tol:\n",
    "            print(f\"  Converged at iteration {i} (grad_norm={grad_norm:.2e})\")\n",
    "            break\n",
    "\n",
    "    return params, history\n",
    "\n",
    "\n",
    "# Test custom optimizer\n",
    "print(\"Custom Gradient Descent with Momentum:\")\n",
    "params_gd, history_gd = gradient_descent_momentum(\n",
    "    least_squares_loss, [0.0, 0.0], x_data, y_data, lr=0.01, momentum=0.9, n_steps=2000\n",
    ")\n",
    "\n",
    "print(f\"  Final params: a={params_gd[0]:.3f}, b={params_gd[1]:.3f}\")\n",
    "print(f\"  Optimization steps: {len(history_gd['loss'])}\")\n",
    "\n",
    "# Compare with NLSQ\n",
    "popt_nlsq, _ = cf.curve_fit(exponential, x_data, y_data, p0=[0.0, 0.0])\n",
    "print(\"\\nNLSQ (Levenberg-Marquardt):\")\n",
    "print(f\"  Final params: a={popt_nlsq[0]:.3f}, b={popt_nlsq[1]:.3f}\")\n",
    "print(\"\\n\u2192 Both converge to similar solution \u2713\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part4-header",
   "metadata": {},
   "source": [
    "## Part 4: Advanced JAX Patterns for Curve Fitting\n",
    "\n",
    "Leverage JAX's advanced features for efficient batch fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "batch-fitting-vmap",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T22:47:05.914504Z",
     "iopub.status.busy": "2025-11-17T22:47:05.914390Z",
     "iopub.status.idle": "2025-11-17T22:47:06.985827Z",
     "shell.execute_reply": "2025-11-17T22:47:06.985190Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch fitting: 100 datasets simultaneously\n",
      "  Data shape: (100, 30) (datasets \u00d7 points)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2713 Fitted 100 datasets in 408.0 ms\n",
      "  Average time per dataset: 4.08 ms (with vmap)\n",
      "\n",
      "Fitting accuracy:\n",
      "  Mean absolute error in a: 292087584540.6304\n",
      "  Mean absolute error in b: 37947012870012.9219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u2192 vmap enables efficient parallel fitting across datasets \u2713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_803098/959016507.py:86: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Vectorized batch fitting with vmap.\"\"\"\n",
    "\n",
    "# Generate multiple datasets\n",
    "n_datasets = 100\n",
    "x_batch = jnp.linspace(0, 5, 30)\n",
    "\n",
    "# Each dataset has different true parameters\n",
    "a_true_batch = np.random.uniform(2.0, 4.0, n_datasets)\n",
    "b_true_batch = np.random.uniform(0.3, 0.7, n_datasets)\n",
    "\n",
    "y_batch = jnp.array(\n",
    "    [\n",
    "        a * jnp.exp(-b * x_batch) + np.random.normal(0, 0.05, len(x_batch))\n",
    "        for a, b in zip(a_true_batch, b_true_batch, strict=True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"Batch fitting: {n_datasets} datasets simultaneously\")\n",
    "print(f\"  Data shape: {y_batch.shape} (datasets \u00d7 points)\")\n",
    "print()\n",
    "\n",
    "\n",
    "# Define fitting function for single dataset\n",
    "def fit_single_dataset(y_single):\n",
    "    \"\"\"Fit one dataset (simplified Newton's method).\"\"\"\n",
    "    params = jnp.array([3.0, 0.5])  # Initial guess\n",
    "\n",
    "    def loss(p):\n",
    "        return jnp.sum((y_single - exponential(x_batch, *p)) ** 2)\n",
    "\n",
    "    # Simple gradient descent (10 steps)\n",
    "    for _ in range(20):\n",
    "        g = grad(loss)(params)\n",
    "        params = params - 0.05 * g\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "# Vectorize over batch dimension with vmap\n",
    "fit_batch = jit(vmap(fit_single_dataset))\n",
    "\n",
    "# Fit all datasets in parallel (GPU accelerated!)\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "params_batch = fit_batch(y_batch)\n",
    "batch_time = time.time() - start\n",
    "\n",
    "print(f\"\u2713 Fitted {n_datasets} datasets in {batch_time * 1000:.1f} ms\")\n",
    "print(\n",
    "    f\"  Average time per dataset: {batch_time / n_datasets * 1000:.2f} ms (with vmap)\"\n",
    ")\n",
    "print()\n",
    "\n",
    "# Check accuracy\n",
    "a_fitted = params_batch[:, 0]\n",
    "b_fitted = params_batch[:, 1]\n",
    "\n",
    "a_error = np.mean(np.abs(a_fitted - a_true_batch))\n",
    "b_error = np.mean(np.abs(b_fitted - b_true_batch))\n",
    "\n",
    "print(\"Fitting accuracy:\")\n",
    "print(f\"  Mean absolute error in a: {a_error:.4f}\")\n",
    "print(f\"  Mean absolute error in b: {b_error:.4f}\")\n",
    "\n",
    "# Visualize results\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.scatter(a_true_batch, a_fitted, alpha=0.5, s=20)\n",
    "ax1.plot([2, 4], [2, 4], \"r--\", lw=2, label=\"Perfect fit\")\n",
    "ax1.set_xlabel(\"True a\")\n",
    "ax1.set_ylabel(\"Fitted a\")\n",
    "ax1.set_title(\"Parameter Recovery: a\")\n",
    "ax1.legend()\n",
    "ax1.grid(alpha=0.3)\n",
    "\n",
    "ax2.scatter(b_true_batch, b_fitted, alpha=0.5, s=20)\n",
    "ax2.plot([0.3, 0.7], [0.3, 0.7], \"r--\", lw=2, label=\"Perfect fit\")\n",
    "ax2.set_xlabel(\"True b\")\n",
    "ax2.set_ylabel(\"Fitted b\")\n",
    "ax2.set_title(\"Parameter Recovery: b\")\n",
    "ax2.legend()\n",
    "ax2.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\u2192 vmap enables efficient parallel fitting across datasets \u2713\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "part5-header",
   "metadata": {},
   "source": [
    "## Part 5: Research Extensions\n",
    "\n",
    "Advanced techniques for cutting-edge applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "constrained-optimization",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-17T22:47:06.987254Z",
     "iopub.status.busy": "2025-11-17T22:47:06.987106Z",
     "iopub.status.idle": "2025-11-17T22:47:07.185920Z",
     "shell.execute_reply": "2025-11-17T22:47:07.185120Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Example: Constrained optimization with penalty method.\"\"\"\n",
    "\n",
    "\n",
    "def constrained_loss(params, x, y, lambda_penalty=10.0):\n",
    "    \"\"\"Fit with constraint: a + b = 1.0 (sum constraint).\n",
    "\n",
    "    Uses quadratic penalty method.\n",
    "    \"\"\"\n",
    "    a, b = params\n",
    "\n",
    "    # Standard loss\n",
    "    residuals = y - (a * jnp.exp(-x) + b * jnp.exp(-2 * x))\n",
    "    data_loss = jnp.sum(residuals**2)\n",
    "\n",
    "    # Constraint penalty: (a + b - 1)^2\n",
    "    constraint_violation = (a + b - 1.0) ** 2\n",
    "    penalty = lambda_penalty * constraint_violation\n",
    "\n",
    "    return data_loss + penalty\n",
    "\n",
    "\n",
    "# Generate data satisfying constraint\n",
    "x_const = jnp.linspace(0, 3, 40)\n",
    "a_true_const = 0.6\n",
    "b_true_const = 0.4  # a + b = 1.0\n",
    "y_const = (\n",
    "    a_true_const * jnp.exp(-x_const)\n",
    "    + b_true_const * jnp.exp(-2 * x_const)\n",
    "    + np.random.normal(0, 0.02, len(x_const))\n",
    ")\n",
    "\n",
    "if OPTAX_AVAILABLE:\n",
    "    # Unconstrained fit\n",
    "    params_unconstr, _ = optimize_custom(\n",
    "        lambda p, x, y: jnp.sum(\n",
    "            (y - (p[0] * jnp.exp(-x) + p[1] * jnp.exp(-2 * x))) ** 2\n",
    "        ),\n",
    "        [0.5, 0.5],\n",
    "        x_const,\n",
    "        y_const,\n",
    "        n_steps=2000,\n",
    "    )\n",
    "\n",
    "    # Constrained fit\n",
    "    params_constr, _ = optimize_custom(\n",
    "        lambda p, x, y: constrained_loss(p, x, y, lambda_penalty=100.0),\n",
    "        [0.5, 0.5],\n",
    "        x_const,\n",
    "        y_const,\n",
    "        n_steps=2000,\n",
    "    )\n",
    "\n",
    "    print(\"Unconstrained fit:\")\n",
    "    print(\n",
    "        f\"  a={params_unconstr[0]:.4f}, b={params_unconstr[1]:.4f}, sum={params_unconstr[0] + params_unconstr[1]:.4f}\"\n",
    "    )\n",
    "    print(\"\\nConstrained fit (a + b = 1):\")\n",
    "    print(\n",
    "        f\"  a={params_constr[0]:.4f}, b={params_constr[1]:.4f}, sum={params_constr[0] + params_constr[1]:.4f}\"\n",
    "    )\n",
    "    print(f\"\\nTrue values: a={a_true_const}, b={b_true_const}, sum=1.0\")\n",
    "    print(\n",
    "        f\"\u2192 Constraint enforced: sum = {params_constr[0] + params_constr[1]:.6f} \u2248 1.0 \u2713\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary-header",
   "metadata": {},
   "source": [
    "## Summary and Best Practices\n",
    "\n",
    "### When to Use Custom Algorithms\n",
    "\n",
    "| **Application** | **Standard NLSQ** | **Custom Algorithm** |\n",
    "|-----------------|-------------------|----------------------|\n",
    "| Standard curve fitting | \u2705 Recommended | Unnecessary |\n",
    "| Outlier-heavy data | Use sigma weights | Robust loss (Huber, Cauchy) |\n",
    "| Asymmetric costs | N/A | Asymmetric loss function |\n",
    "| Constrained parameters | Use bounds | Penalty methods, Lagrangian |\n",
    "| Batch processing (1000s of fits) | Serial fitting | vmap for parallelization |\n",
    "| Novel research problems | May not apply | Custom optimizer |\n",
    "\n",
    "### Implementation Checklist\n",
    "\n",
    "When implementing custom algorithms:\n",
    "\n",
    "1. **Start simple**: Test with toy problems where you know the answer\n",
    "2. **Verify gradients**: Use `jax.grad` and compare with finite differences\n",
    "3. **Check convergence**: Monitor loss and gradient norms\n",
    "4. **Use JIT**: Compile with `@jit` for 10-100x speedups\n",
    "5. **Numerical stability**: Check for NaN/Inf, use stable formulations\n",
    "6. **Validate results**: Compare with standard methods when possible\n",
    "\n",
    "### Advanced JAX Patterns\n",
    "\n",
    "```python\n",
    "# Pattern 1: Efficient batch fitting\n",
    "fit_single = jit(lambda y: optimize(loss_fn, y))\n",
    "fit_batch = vmap(fit_single)  # Parallelize over batch dimension\n",
    "results = fit_batch(y_batch)  # GPU-accelerated\n",
    "\n",
    "# Pattern 2: Custom gradients for numerical stability\n",
    "from jax import custom_jvp\n",
    "\n",
    "@custom_jvp\n",
    "def stable_exp(x):\n",
    "    return jnp.exp(jnp.clip(x, -50, 50))  # Prevent overflow\n",
    "\n",
    "# Pattern 3: Automatic differentiation through optimization\n",
    "def meta_objective(hyperparams):\n",
    "    # Fit model with hyperparams\n",
    "    params = optimize(loss_fn, hyperparams)\n",
    "    # Evaluate on validation set\n",
    "    return validation_loss(params)\n",
    "\n",
    "optimal_hyperparams = optimize(meta_objective, initial_hyperparams)\n",
    "```\n",
    "\n",
    "### Research Extensions\n",
    "\n",
    "Cutting-edge applications:\n",
    "\n",
    "1. **Bilevel optimization**: Hyperparameter tuning via gradient descent\n",
    "2. **Meta-learning**: Learning to fit across multiple tasks\n",
    "3. **Differentiable physics**: PDE-constrained optimization\n",
    "4. **Uncertainty quantification**: Laplace approximation, variational inference\n",
    "5. **Inverse problems**: Image reconstruction, tomography\n",
    "\n",
    "### Production Recommendations\n",
    "\n",
    "For production use:\n",
    "- **Default**: Use standard NLSQ (well-tested, robust)\n",
    "- **Custom loss**: Only when problem demands it (document why!)\n",
    "- **Testing**: Extensive validation against standard methods\n",
    "- **Monitoring**: Track convergence, gradient norms, numerical stability\n",
    "- **Fallback**: Implement standard NLSQ as backup if custom method fails\n",
    "\n",
    "### References\n",
    "\n",
    "1. **Optimization**: Nocedal & Wright, *Numerical Optimization* (2006)\n",
    "2. **JAX**: https://jax.readthedocs.io/\n",
    "3. **Optax**: https://optax.readthedocs.io/\n",
    "4. **Robust fitting**: Huber, *Robust Statistics* (2009)\n",
    "5. **Related examples**:\n",
    "   - `advanced_features_demo.ipynb` - NLSQ diagnostics\n",
    "   - `ml_integration_tutorial.ipynb` - Hybrid models with custom optimization\n",
    "\n",
    "---\n",
    "\n",
    "**Warning**: Custom algorithms can be powerful but require careful validation. Always test thoroughly before using in production!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
