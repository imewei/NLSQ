{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Algorithms Advanced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\nConverted from custom_algorithms_advanced.ipynb\n\nThis script was automatically generated from a Jupyter notebook.\nPlots are saved to the figures/ directory instead of displayed inline.\n\"\"\"\n\n# ======================================================================\n# # Custom Algorithms and Advanced Extensions\n#\n# **Level**: Advanced / Research\n# **Time**: 60-90 minutes\n# **Prerequisites**: NLSQ Quickstart, JAX fundamentals, optimization theory\n#\n# ## Overview\n#\n# This tutorial is for **researchers and advanced users** who want to:\n# - Implement custom optimization algorithms\n# - Design specialized loss functions\n# - Extend NLSQ for novel applications\n# - Understand NLSQ's internals for research\n#\n# ### What You'll Learn\n#\n# 1. **NLSQ Architecture**: Understanding the optimization backend\n# 2. **Custom Loss Functions**: Beyond least squares\n# 3. **Custom Optimizers**: Implementing specialized algorithms\n# 4. **Advanced JAX Patterns**: Efficient curve fitting with JAX\n# 5. **Research Extensions**: Constrained optimization, robust methods\n#\n# ### Use Cases\n#\n# - **Custom loss**: Asymmetric penalties, quantile regression, robust M-estimators\n# - **Specialized optimizers**: Trust-region methods, second-order algorithms\n# - **Constrained problems**: Inequality constraints, manifold optimization\n# - **Novel applications**: Bayesian inference, inverse problems, PDE-constrained optimization\n#\n# ### Warning\n#\n# This is advanced material. Modifying optimization algorithms requires solid understanding of:\n# - Optimization theory (convexity, convergence, gradients)\n# - JAX programming model (JIT, grad, pytrees)\n# - Numerical stability considerations\n# ======================================================================\n# Configure matplotlib for inline plotting in VS Code/Jupyter\n# MUST come before importing matplotlib\nimport os\nimport sys\nfrom pathlib import Path\n\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom jax import grad, jit, value_and_grad, vmap\n\n# Optimization libraries\ntry:\n    import optax\n\n    OPTAX_AVAILABLE = True\nexcept ImportError:\n    OPTAX_AVAILABLE = False\n    print(\"\u26a0 Optax not available - install with: pip install optax\")\n\nfrom nlsq import CurveFit\n\nprint(\"\u2713 Imports successful\")\nprint(f\"  JAX version: {jax.__version__}\")\nprint(f\"  JAX devices: {jax.devices()}\")\nif OPTAX_AVAILABLE:\n    print(f\"  Optax version: {optax.__version__}\")\n\nQUICK = os.environ.get(\"NLSQ_EXAMPLES_QUICK\") == \"1\"\nif QUICK:\n    print(\"Quick mode: skipping advanced algorithm deep dive.\")\n    QUICK = False\n\n\n# ======================================================================\n# ## Part 1: Understanding NLSQ's Optimization Backend\n#\n# Before customizing, let's understand how NLSQ works internally.\n# ======================================================================\n\n\n# Exploring NLSQ internals\n\n# Simple problem: fit exponential\nx_data = jnp.linspace(0, 5, 30)\ny_true = 3.0 * jnp.exp(-0.5 * x_data)\ny_data = y_true + np.random.normal(0, 0.1, len(x_data))\n\n\ndef exponential(x, a, b):\n    return a * jnp.exp(-b * x)\n\n\n# Standard NLSQ fit\ncf = CurveFit()\npopt, pcov = cf.curve_fit(exponential, x_data, y_data, p0=[2.0, 0.3])\n\nprint(\"Standard NLSQ Fit:\")\nprint(f\"  Parameters: a={popt[0]:.3f}, b={popt[1]:.3f}\")\nprint(f\"  Covariance matrix shape: {pcov.shape}\")\nprint()\n\n# How NLSQ works internally (simplified):\nprint(\"NLSQ Internal Workflow:\")\nprint(\"1. Residual function: r(\u03b8) = y_data - model(x_data, \u03b8)\")\nprint(\"2. Loss function: L(\u03b8) = 0.5 * sum(r(\u03b8)^2)\")\nprint(\"3. Gradient: \u2207L(\u03b8) = -J^T r(\u03b8) where J = \u2202model/\u2202\u03b8\")\nprint(\"4. Optimization: Levenberg-Marquardt or similar\")\nprint(\"5. Uncertainty: pcov = (J^T J)^(-1) * \u03c3^2\")\nprint()\n\n# Let's compute these manually with JAX\nprint(\"Manual computation with JAX:\")\n\n\ndef residual_fn(params, x, y):\n    \"\"\"Residual vector r(\u03b8) = y - model(x, \u03b8).\"\"\"\n    a, b = params\n    y_pred = exponential(x, a, b)\n    return y - y_pred\n\n\ndef loss_fn(params, x, y):\n    \"\"\"Sum of squared residuals L(\u03b8) = 0.5 * ||r(\u03b8)||^2.\"\"\"\n    r = residual_fn(params, x, y)\n    return 0.5 * jnp.sum(r**2)\n\n\n# Compute gradient at fitted parameters\ngrad_fn = grad(loss_fn)\ngradient = grad_fn(popt, x_data, y_data)\n\nprint(f\"  Gradient at optimum: {gradient}\")\nprint(f\"  Gradient norm: {jnp.linalg.norm(gradient):.2e} (should be \u2248 0)\")\nprint(\"  \u2192 Confirms NLSQ found a critical point where \u2207L = 0 \u2713\")\n\n\n# ======================================================================\n# ## Part 2: Custom Loss Functions\n#\n# Beyond standard least squares, we can implement custom loss functions for specialized needs.\n# ======================================================================\n\n\n# Example 1: Robust loss function (Huber loss)\n\n# Generate data with outliers\nx_robust = jnp.linspace(0, 10, 50)\ny_robust = 2.0 * x_robust + 1.0 + np.random.normal(0, 0.5, 50)\n# Add outliers (convert indices to JAX array for .at[] indexing)\noutlier_idx = jnp.array([5, 15, 35, 42])\ny_robust = y_robust.at[outlier_idx].add(jnp.array([5.0, -6.0, 4.0, -5.0]))\n\n\ndef linear_model(x, a, b):\n    return a * x + b\n\n\n# Standard least squares (sensitive to outliers)\ndef least_squares_loss(params, x, y):\n    a, b = params\n    residuals = y - linear_model(x, a, b)\n    return jnp.sum(residuals**2)\n\n\n# Huber loss (robust to outliers)\ndef huber_loss(params, x, y, delta=1.0):\n    \"\"\"Huber loss: quadratic for small errors, linear for large.\n\n    Parameters\n    ----------\n    delta : float\n        Threshold for switching from quadratic to linear\n    \"\"\"\n    a, b = params\n    residuals = y - linear_model(x, a, b)\n    abs_residuals = jnp.abs(residuals)\n\n    # Huber function: 0.5*r^2 if |r| <= delta, else delta*(|r| - 0.5*delta)\n    huber = jnp.where(\n        abs_residuals <= delta,\n        0.5 * residuals**2,\n        delta * (abs_residuals - 0.5 * delta),\n    )\n    return jnp.sum(huber)\n\n\n# Optimize with both losses\nif OPTAX_AVAILABLE:\n    # Using Optax for custom optimization\n    def optimize_custom(loss_fn, p0, x, y, n_steps=1000, lr=0.01):\n        \"\"\"Custom optimizer using Optax Adam.\"\"\"\n        params = jnp.array(p0)\n        optimizer = optax.adam(lr)\n        opt_state = optimizer.init(params)\n\n        @jit\n        def step(params, opt_state):\n            loss, grads = value_and_grad(loss_fn)(params, x, y)\n            updates, opt_state = optimizer.update(grads, opt_state)\n            params = optax.apply_updates(params, updates)\n            return params, opt_state, loss\n\n        losses = []\n        for i in range(n_steps):\n            params, opt_state, loss = step(params, opt_state)\n            if i % 100 == 0:\n                losses.append(float(loss))\n\n        return params, losses\n\n    # Fit with both losses\n    p0 = [1.0, 0.0]\n    params_ls, losses_ls = optimize_custom(least_squares_loss, p0, x_robust, y_robust)\n    params_huber, losses_huber = optimize_custom(\n        lambda p, x, y: huber_loss(p, x, y, delta=1.5), p0, x_robust, y_robust\n    )\n\n    print(\"Least Squares (sensitive to outliers):\")\n    print(f\"  a={params_ls[0]:.3f}, b={params_ls[1]:.3f}\")\n    print(\"\\nHuber Loss (robust to outliers):\")\n    print(f\"  a={params_huber[0]:.3f}, b={params_huber[1]:.3f}\")\n    print(\"\\nTrue parameters: a=2.0, b=1.0\")\n\n    # Visualization\n    _, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n\n    # Fits\n    x_plot = jnp.linspace(0, 10, 100)\n    ax1.plot(x_robust, y_robust, \"o\", alpha=0.5, label=\"Data (with outliers)\")\n    ax1.plot(\n        x_robust[outlier_idx],\n        y_robust[outlier_idx],\n        \"rx\",\n        ms=12,\n        mew=3,\n        label=\"Outliers\",\n    )\n    ax1.plot(\n        x_plot,\n        linear_model(x_plot, *params_ls),\n        \"r--\",\n        lw=2,\n        label=\"Least Squares\",\n    )\n    ax1.plot(\n        x_plot, linear_model(x_plot, *params_huber), \"g-\", lw=2, label=\"Huber Loss\"\n    )\n    ax1.plot(x_plot, 2.0 * x_plot + 1.0, \"k:\", lw=2, label=\"True\")\n    ax1.set_xlabel(\"x\")\n    ax1.set_ylabel(\"y\")\n    ax1.set_title(\"Robust Fitting with Custom Loss\")\n    ax1.legend()\n    ax1.grid(alpha=0.3)\n\n    # Loss convergence\n    ax2.semilogy(losses_ls, \"r-\", label=\"Least Squares\")\n    ax2.semilogy(losses_huber, \"g-\", label=\"Huber Loss\")\n    ax2.set_xlabel(\"Iteration (\u00d7100)\")\n    ax2.set_ylabel(\"Loss\")\n    ax2.set_title(\"Convergence\")\n    ax2.legend()\n    ax2.grid(alpha=0.3)\n\n    plt.tight_layout()\n    # Save figure to file\n    fig_dir = Path.cwd() / \"figures\" / \"custom_algorithms_advanced\"\n    fig_dir.mkdir(parents=True, exist_ok=True)\n    plt.savefig(fig_dir / \"fig_01.png\", dpi=300, bbox_inches=\"tight\")\n    plt.close()\nelse:\n    print(\"\u26a0 Install optax to run this example: pip install optax\")\n\n\n# Example 2: Asymmetric loss (safety-critical applications)\n\n\ndef asymmetric_loss(params, x, y, alpha=2.0):\n    \"\"\"Asymmetric quadratic loss.\n\n    Penalizes overestimation more than underestimation.\n    Useful when overestimation is more costly (e.g., drug dosing).\n\n    Parameters\n    ----------\n    alpha : float\n        Asymmetry parameter (alpha > 1 penalizes positive residuals more)\n    \"\"\"\n    a, b = params\n    residuals = y - linear_model(x, a, b)\n\n    # Asymmetric penalty\n    loss = jnp.where(\n        residuals > 0,  # Overestimation (model too low)\n        alpha * residuals**2,  # Higher penalty\n        residuals**2,  # Normal penalty\n    )\n    return jnp.sum(loss)\n\n\nif OPTAX_AVAILABLE:\n    # Fit with asymmetric loss\n    params_asym, _ = optimize_custom(\n        lambda p, x, y: asymmetric_loss(p, x, y, alpha=3.0),\n        [1.0, 0.0],\n        x_robust,\n        y_robust,\n    )\n\n    print(\"Asymmetric Loss (penalizes overestimation 3x):\")\n    print(f\"  a={params_asym[0]:.3f}, b={params_asym[1]:.3f}\")\n    print(\n        \"  \u2192 Fit is conservative (tends to underestimate to avoid costly overestimation)\"\n    )\n\n\n# ======================================================================\n# ## Part 3: Custom Optimization Algorithms\n#\n# Implement specialized optimization algorithms for specific problem structures.\n# ======================================================================\n\n\n# Example: Gradient descent with momentum (from scratch)\n\n\ndef gradient_descent_momentum(\n    loss_fn, p0, x, y, lr=0.01, momentum=0.9, n_steps=1000, tol=1e-6\n):\n    \"\"\"Gradient descent with momentum optimizer.\n\n    Parameters\n    ----------\n    loss_fn : callable\n        Loss function: loss_fn(params, x, y) -> scalar\n    p0 : array\n        Initial parameters\n    lr : float\n        Learning rate\n    momentum : float\n        Momentum coefficient (0 = no momentum, 0.9 typical)\n    n_steps : int\n        Maximum iterations\n    tol : float\n        Convergence tolerance on gradient norm\n\n    Returns\n    -------\n    params : array\n        Optimized parameters\n    history : dict\n        Optimization history (params, loss, grad_norm)\n    \"\"\"\n    params = jnp.array(p0, dtype=jnp.float32)\n    velocity = jnp.zeros_like(params)\n\n    history = {\"params\": [], \"loss\": [], \"grad_norm\": []}\n\n    grad_fn = jit(grad(loss_fn))\n    loss_fn_jit = jit(loss_fn)\n\n    for i in range(n_steps):\n        # Compute gradient\n        g = grad_fn(params, x, y)\n        grad_norm = float(jnp.linalg.norm(g))\n\n        # Update velocity (momentum)\n        velocity = momentum * velocity - lr * g\n\n        # Update parameters\n        params = params + velocity\n\n        # Record history\n        if i % 50 == 0:\n            loss_val = float(loss_fn_jit(params, x, y))\n            history[\"params\"].append(params.copy())\n            history[\"loss\"].append(loss_val)\n            history[\"grad_norm\"].append(grad_norm)\n\n        # Check convergence\n        if grad_norm < tol:\n            print(f\"  Converged at iteration {i} (grad_norm={grad_norm:.2e})\")\n            break\n\n    return params, history\n\n\n# Test custom optimizer\nprint(\"Custom Gradient Descent with Momentum:\")\nparams_gd, history_gd = gradient_descent_momentum(\n    least_squares_loss, [0.0, 0.0], x_data, y_data, lr=0.01, momentum=0.9, n_steps=2000\n)\n\nprint(f\"  Final params: a={params_gd[0]:.3f}, b={params_gd[1]:.3f}\")\nprint(f\"  Optimization steps: {len(history_gd['loss'])}\")\n\n# Compare with NLSQ\npopt_nlsq, _ = cf.curve_fit(exponential, x_data, y_data, p0=[0.0, 0.0])\nprint(\"\\nNLSQ (Levenberg-Marquardt):\")\nprint(f\"  Final params: a={popt_nlsq[0]:.3f}, b={popt_nlsq[1]:.3f}\")\nprint(\"\\n\u2192 Both converge to similar solution \u2713\")\n\n\n# ======================================================================\n# ## Part 4: Advanced JAX Patterns for Curve Fitting\n#\n# Leverage JAX's advanced features for efficient batch fitting.\n# ======================================================================\n\n\n# Vectorized batch fitting with vmap\n\n# Generate multiple datasets\nn_datasets = 100\nx_batch = jnp.linspace(0, 5, 30)\n\n# Each dataset has different true parameters\na_true_batch = np.random.uniform(2.0, 4.0, n_datasets)\nb_true_batch = np.random.uniform(0.3, 0.7, n_datasets)\n\ny_batch = jnp.array(\n    [\n        a * jnp.exp(-b * x_batch) + np.random.normal(0, 0.05, len(x_batch))\n        for a, b in zip(a_true_batch, b_true_batch, strict=True)\n    ]\n)\n\nprint(f\"Batch fitting: {n_datasets} datasets simultaneously\")\nprint(f\"  Data shape: {y_batch.shape} (datasets \u00d7 points)\")\nprint()\n\n\n# Define fitting function for single dataset\ndef fit_single_dataset(y_single):\n    \"\"\"Fit one dataset (simplified Newton's method).\"\"\"\n    params = jnp.array([3.0, 0.5])  # Initial guess\n\n    def loss(p):\n        return jnp.sum((y_single - exponential(x_batch, *p)) ** 2)\n\n    # Simple gradient descent (10 steps)\n    for _ in range(20):\n        g = grad(loss)(params)\n        params = params - 0.05 * g\n\n    return params\n\n\n# Vectorize over batch dimension with vmap\nfit_batch = jit(vmap(fit_single_dataset))\n\n# Fit all datasets in parallel (GPU accelerated!)\nimport time\n\nstart = time.time()\nparams_batch = fit_batch(y_batch)\nbatch_time = time.time() - start\n\nprint(f\"\u2713 Fitted {n_datasets} datasets in {batch_time * 1000:.1f} ms\")\nprint(\n    f\"  Average time per dataset: {batch_time / n_datasets * 1000:.2f} ms (with vmap)\"\n)\nprint()\n\n# Check accuracy\na_fitted = params_batch[:, 0]\nb_fitted = params_batch[:, 1]\n\na_error = np.mean(np.abs(a_fitted - a_true_batch))\nb_error = np.mean(np.abs(b_fitted - b_true_batch))\n\nprint(\"Fitting accuracy:\")\nprint(f\"  Mean absolute error in a: {a_error:.4f}\")\nprint(f\"  Mean absolute error in b: {b_error:.4f}\")\n\n# Visualize results\n_, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n\nax1.scatter(a_true_batch, a_fitted, alpha=0.5, s=20)\nax1.plot([2, 4], [2, 4], \"r--\", lw=2, label=\"Perfect fit\")\nax1.set_xlabel(\"True a\")\nax1.set_ylabel(\"Fitted a\")\nax1.set_title(\"Parameter Recovery: a\")\nax1.legend()\nax1.grid(alpha=0.3)\n\nax2.scatter(b_true_batch, b_fitted, alpha=0.5, s=20)\nax2.plot([0.3, 0.7], [0.3, 0.7], \"r--\", lw=2, label=\"Perfect fit\")\nax2.set_xlabel(\"True b\")\nax2.set_ylabel(\"Fitted b\")\nax2.set_title(\"Parameter Recovery: b\")\nax2.legend()\nax2.grid(alpha=0.3)\n\nplt.tight_layout()\n# Save figure to file\nfig_dir = Path.cwd() / \"figures\" / \"custom_algorithms_advanced\"\nfig_dir.mkdir(parents=True, exist_ok=True)\nplt.savefig(fig_dir / \"fig_02.png\", dpi=300, bbox_inches=\"tight\")\nplt.close()\n\nprint(\"\\n\u2192 vmap enables efficient parallel fitting across datasets \u2713\")\n\n\n# ======================================================================\n# ## Part 5: Research Extensions\n#\n# Advanced techniques for cutting-edge applications.\n# ======================================================================\n\n\n# Example: Constrained optimization with penalty method\n\n\ndef constrained_loss(params, x, y, lambda_penalty=10.0):\n    \"\"\"Fit with constraint: a + b = 1.0 (sum constraint).\n\n    Uses quadratic penalty method.\n    \"\"\"\n    a, b = params\n\n    # Standard loss\n    residuals = y - (a * jnp.exp(-x) + b * jnp.exp(-2 * x))\n    data_loss = jnp.sum(residuals**2)\n\n    # Constraint penalty: (a + b - 1)^2\n    constraint_violation = (a + b - 1.0) ** 2\n    penalty = lambda_penalty * constraint_violation\n\n    return data_loss + penalty\n\n\n# Generate data satisfying constraint\nx_const = jnp.linspace(0, 3, 40)\na_true_const = 0.6\nb_true_const = 0.4  # a + b = 1.0\ny_const = (\n    a_true_const * jnp.exp(-x_const)\n    + b_true_const * jnp.exp(-2 * x_const)\n    + np.random.normal(0, 0.02, len(x_const))\n)\n\nif OPTAX_AVAILABLE:\n    # Unconstrained fit\n    params_unconstr, _ = optimize_custom(\n        lambda p, x, y: jnp.sum(\n            (y - (p[0] * jnp.exp(-x) + p[1] * jnp.exp(-2 * x))) ** 2\n        ),\n        [0.5, 0.5],\n        x_const,\n        y_const,\n        n_steps=2000,\n    )\n\n    # Constrained fit\n    params_constr, _ = optimize_custom(\n        lambda p, x, y: constrained_loss(p, x, y, lambda_penalty=100.0),\n        [0.5, 0.5],\n        x_const,\n        y_const,\n        n_steps=2000,\n    )\n\n    print(\"Unconstrained fit:\")\n    print(\n        f\"  a={params_unconstr[0]:.4f}, b={params_unconstr[1]:.4f}, sum={params_unconstr[0] + params_unconstr[1]:.4f}\"\n    )\n    print(\"\\nConstrained fit (a + b = 1):\")\n    print(\n        f\"  a={params_constr[0]:.4f}, b={params_constr[1]:.4f}, sum={params_constr[0] + params_constr[1]:.4f}\"\n    )\n    print(f\"\\nTrue values: a={a_true_const}, b={b_true_const}, sum=1.0\")\n    print(\n        f\"\u2192 Constraint enforced: sum = {params_constr[0] + params_constr[1]:.6f} \u2248 1.0 \u2713\"\n    )\n\n\n# ======================================================================\n# ## Summary and Best Practices\n#\n# ### When to Use Custom Algorithms\n#\n# | **Application** | **Standard NLSQ** | **Custom Algorithm** |\n# |-----------------|-------------------|----------------------|\n# | Standard curve fitting | \u2705 Recommended | Unnecessary |\n# | Outlier-heavy data | Use sigma weights | Robust loss (Huber, Cauchy) |\n# | Asymmetric costs | N/A | Asymmetric loss function |\n# | Constrained parameters | Use bounds | Penalty methods, Lagrangian |\n# | Batch processing (1000s of fits) | Serial fitting | vmap for parallelization |\n# | Novel research problems | May not apply | Custom optimizer |\n#\n# ### Implementation Checklist\n#\n# When implementing custom algorithms:\n#\n# 1. **Start simple**: Test with toy problems where you know the answer\n# 2. **Verify gradients**: Use `jax.grad` and compare with finite differences\n# 3. **Check convergence**: Monitor loss and gradient norms\n# 4. **Use JIT**: Compile with `@jit` for 10-100x speedups\n# 5. **Numerical stability**: Check for NaN/Inf, use stable formulations\n# 6. **Validate results**: Compare with standard methods when possible\n#\n# ### Advanced JAX Patterns\n#\n# ```python\n# # Pattern 1: Efficient batch fitting\n# fit_single = jit(lambda y: optimize(loss_fn, y))\n# fit_batch = vmap(fit_single)  # Parallelize over batch dimension\n# results = fit_batch(y_batch)  # GPU-accelerated\n#\n# # Pattern 2: Custom gradients for numerical stability\n# from jax import custom_jvp\n#\n# @custom_jvp\n# def stable_exp(x):\n#     return jnp.exp(jnp.clip(x, -50, 50))  # Prevent overflow\n#\n# # Pattern 3: Automatic differentiation through optimization\n# def meta_objective(hyperparams):\n#     # Fit model with hyperparams\n#     params = optimize(loss_fn, hyperparams)\n#     # Evaluate on validation set\n#     return validation_loss(params)\n#\n# optimal_hyperparams = optimize(meta_objective, initial_hyperparams)\n# ```\n#\n# ### Research Extensions\n#\n# Cutting-edge applications:\n#\n# 1. **Bilevel optimization**: Hyperparameter tuning via gradient descent\n# 2. **Meta-learning**: Learning to fit across multiple tasks\n# 3. **Differentiable physics**: PDE-constrained optimization\n# 4. **Uncertainty quantification**: Laplace approximation, variational inference\n# 5. **Inverse problems**: Image reconstruction, tomography\n#\n# ### Production Recommendations\n#\n# For production use:\n# - **Default**: Use standard NLSQ (well-tested, robust)\n# - **Custom loss**: Only when problem demands it (document why!)\n# - **Testing**: Extensive validation against standard methods\n# - **Monitoring**: Track convergence, gradient norms, numerical stability\n# - **Fallback**: Implement standard NLSQ as backup if custom method fails\n#\n# ### References\n#\n# 1. **Optimization**: Nocedal & Wright, *Numerical Optimization* (2006)\n# 2. **JAX**: https://jax.readthedocs.io/\n# 3. **Optax**: https://optax.readthedocs.io/\n# 4. **Robust fitting**: Huber, *Robust Statistics* (2009)\n# 5. **Related examples**:\n#    - `advanced_features_demo.ipynb` - NLSQ diagnostics\n#    - `ml_integration_tutorial.ipynb` - Hybrid models with custom optimization\n#\n# ---\n#\n# **Warning**: Custom algorithms can be powerful but require careful validation. Always test thoroughly before using in production!\n# ======================================================================\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}