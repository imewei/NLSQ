{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "    Parameters\n    ----------\n    base_dir : str, optional\n        Base directory for checkpoints.\n    Returns\n    -------\n    str\n        Path to the created checkpoint directory.\n    \"\"\"\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    checkpoint_dir = Path(base_dir) / f\"checkpoint_{timestamp}\"\n    checkpoint_dir.mkdir(parents=True, exist_ok=True)\n    return str(checkpoint_dir)\ndef save_checkpoint(\n    checkpoint_dir: str, iteration: int, params, loss: float, metadata=None\n):\n    \"\"\"Save optimization checkpoint using JSON.\n    Parameters\n    ----------\n    checkpoint_dir : str\n        Directory to save checkpoint\n    iteration : int\n        Current iteration number\n    params : np.ndarray\n        Current parameter values\n    loss : float\n        Current loss value\n    metadata : dict, optional\n        Additional metadata to save\n    \"\"\"\n    checkpoint_path = Path(checkpoint_dir) / f\"checkpoint_{iteration:06d}.json\"\n    checkpoint_data = {\n        \"iteration\": iteration,\n        \"params\": np.array(params).tolist(),\n        \"loss\": float(loss),\n        \"metadata\": metadata or {},\n        \"timestamp\": datetime.now().isoformat(),\n    }\n    with open(checkpoint_path, \"w\") as f:\n        json.dump(checkpoint_data, f, indent=2)\n    print(f\"  Saved checkpoint: {checkpoint_path.name}\")\n    return checkpoint_path\ndef load_latest_checkpoint(checkpoint_dir: str) -> dict | None:\n    \"\"\"Load the most recent checkpoint.\n    Parameters\n    ----------\n    checkpoint_dir : str\n        Directory containing checkpoints\n    Returns\n    -------\n    dict or None\n        Checkpoint data if found, None otherwise\n    \"\"\"\n    checkpoint_dir = Path(checkpoint_dir)\n    if not checkpoint_dir.exists():\n        return None\n    checkpoints = list(checkpoint_dir.glob(\"checkpoint_*.json\"))\n    if not checkpoints:\n        return None\n    latest = sorted(checkpoints)[-1]\n    with open(latest) as f:\n        checkpoint_data = json.load(f)\n    checkpoint_data[\"params\"] = np.array(checkpoint_data[\"params\"])\n    print(f\"  Loaded checkpoint: {latest.name}\")\n    return checkpoint_data\ndef optimization_with_checkpoints(\n    checkpoint_dir: str, max_iterations: int = 100, checkpoint_interval: int = 10\n):\n    \"\"\"Example optimization loop with checkpoint support.\"\"\"\n    checkpoint = load_latest_checkpoint(checkpoint_dir)\n    if checkpoint:\n        start_iteration = checkpoint[\"iteration\"] + 1\n        params = checkpoint[\"params\"]\n        print(f\"  Resuming from iteration {start_iteration}\")\n    else:\n        start_iteration = 0\n        params = np.array([1.0, 1.0, 0.0])  # Initial guess\n        print(\"  Starting fresh optimization\")\n    for iteration in range(start_iteration, max_iterations):\n        params = params + 0.001 * np.random.randn(3)\n        loss = np.sum(params**2)  # Dummy loss\n        if iteration > 0 and iteration % checkpoint_interval == 0:\n            save_checkpoint(checkpoint_dir, iteration, params, loss)\n    save_checkpoint(checkpoint_dir, max_iterations - 1, params, loss)\n    return params\ndef main():\n    print(\"=\" * 70)\n    print(\"HPC Integration and Checkpointing\")\n    print(\"=\" * 70)\n    print()\n    np.random.seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. ClusterDetector and ClusterInfo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"1. ClusterDetector and ClusterInfo:\")\nprint(\"-\" * 50)\ndetector = ClusterDetector()\nprint(f\"  PBS environment detected: {detector.is_pbs_environment()}\")\ncluster_info = detector.detect()\nif cluster_info:\n    print(\"\\n  Cluster detected:\")\n    print(f\"    Scheduler: {cluster_info.scheduler}\")\n    print(f\"    Node count: {cluster_info.node_count}\")\n    print(f\"    GPUs per node: {cluster_info.gpus_per_node}\")\n    print(f\"    Total GPUs: {cluster_info.total_gpus}\")\nelse:\n    print(\"\\n  No cluster environment detected (running locally)\")\nsimulated_cluster = ClusterInfo(\n    node_count=4,\n    gpus_per_node=8,\n    total_gpus=32,\n    node_list=[\"node01\", \"node02\", \"node03\", \"node04\"],\n    scheduler=\"pbs\",\n    job_id=\"12345.pbs_server\",\n    interconnect=\"infiniband\",\n)\nprint(\"\\n  Simulated PBS cluster:\")\nprint(f\"    Nodes: {simulated_cluster.node_count}\")\nprint(f\"    GPUs: {simulated_cluster.total_gpus}\")\nprint(f\"    Job ID: {simulated_cluster.job_id}\")\nprint(f\"    Interconnect: {simulated_cluster.interconnect}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Streaming Strategy for Large Datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print()\nprint(\"2. Streaming Strategy for Large Datasets:\")\nprint(\"-\" * 50)\nprint(\"  Available strategies (memory-based selection):\")\nprint(\"    - standard:  Full in-memory computation\")\nprint(\"    - chunked:   Memory-managed chunk processing\")\nprint(\"    - streaming: Mini-batch gradient descent\")\nprint(\"\\n  Streaming configurations:\")\nprint(\"    HybridStreamingConfig - Streaming optimizer configuration\")\nprint(\"    HybridStreamingConfig.defense_strict() - Checkpoint resume preset\")\nprint(\"    HybridStreamingConfig.scientific_default() - Production preset\")\nconfig = HybridStreamingConfig.defense_strict()\nprint(\"\\n  defense_strict() configuration:\")\nprint(f\"    warmup_iterations: {config.warmup_iterations}\")\nprint(f\"    normalize: {config.normalize}\")\nif QUICK:\n    print()\n    print(\"=\" * 70)\n    print(\"Summary (Quick Mode)\")\n    print(\"=\" * 70)\n    print()\n    print(\"HPC Integration:\")\n    print(\"  - ClusterDetector.detect() for PBS Pro detection\")\n    print(\"  - ClusterInfo for cluster metadata (nodes, GPUs, job ID)\")\n    print()\n    print(\"Checkpointing:\")\n    print(\"  - Use streaming strategy for fault tolerance\")\n    print(\"  - enable_checkpoints=True, checkpoint_dir='./checkpoints'\")\n    print()\n    print(\"Defense Layers for Checkpoint Resume (v0.3.6+):\")\n    print(\"  - Use HybridStreamingConfig.defense_strict() for resume protection\")\n    print(\"  - 4-layer defense prevents L-BFGS warmup from diverging\")\n    return\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. Checkpointing Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print()\nprint(\"3. Checkpointing Configuration:\")\nprint(\"-\" * 50)\ncheckpoint_dir = create_checkpoint_directory()\nprint(f\"  Created checkpoint directory: {checkpoint_dir}\")\ncustom_checkpoint_dir = create_checkpoint_directory(\n    base_dir=\"./my_project_checkpoints\"\n)\nprint(f\"  Custom checkpoint directory: {custom_checkpoint_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. Checkpoint Resume Workflow\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print()\nprint(\"4. Checkpoint Resume Workflow:\")\nprint(\"-\" * 50)\ndemo_dir = create_checkpoint_directory(base_dir=\"./demo_checkpoints\")\nprint(\"\\n  Simulating optimization with checkpoints...\")\nfor i in range(0, 30, 10):\n    params = np.array([2.0 + 0.01 * i, 1.0 - 0.005 * i, 0.5])\n    loss = 0.1 / (1 + i * 0.1)\n    save_checkpoint(demo_dir, i, params, loss, metadata={\"epoch\": i // 10})\nprint(\"\\n  Loading latest checkpoint for resume...\")\nlatest = load_latest_checkpoint(demo_dir)\nif latest:\n    print(f\"    Iteration: {latest['iteration']}\")\n    print(f\"    Parameters: {latest['params']}\")\n    print(f\"    Loss: {latest['loss']:.6f}\")\nprint(\"\\n  Running optimization loop with checkpoints...\")\nfinal_params = optimization_with_checkpoints(demo_dir, max_iterations=50)\nprint(f\"  Final parameters: {final_params}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "5. HPC Distributed Preset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print()\nprint(\"5. HPC Workflow Presets:\")\nprint(\"-\" * 50)\nif \"hpc_distributed\" in WORKFLOW_PRESETS:\n    hpc_preset = WORKFLOW_PRESETS[\"hpc_distributed\"]\n    print(\"  hpc_distributed preset:\")\n    for key, value in list(hpc_preset.items())[:8]:\n        print(f\"    {key}: {value}\")\nelse:\n    print(\"  Available presets for HPC:\")\n    for name in WORKFLOW_PRESETS:\n        desc = WORKFLOW_PRESETS[name].get(\"description\", \"\")\n        print(f\"    - {name}: {desc}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "6. PBS Pro Job Script\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "    print()\n    print(\"6. PBS Pro Job Script:\")\n    print(\"-\" * 50)\n    pbs_script = \"\"\"#!/bin/bash\ncd $PBS_O_WORKDIR\nmodule load python/3.12\nmodule load cuda/12.0\nmodule load cudnn/8.9\nsource ./venv/bin/activate\nexport NLSQ_WORKFLOW_GOAL=robust\nexport NLSQ_MEMORY_LIMIT_GB=200\nexport NLSQ_CHECKPOINT_DIR=$PBS_O_WORKDIR/checkpoints\nmkdir -p $NLSQ_CHECKPOINT_DIR\necho \"========================================\"\necho \"NLSQ Fitting Job Started\"\necho \"========================================\"\necho \"Job ID: $PBS_JOBID\"\necho \"Node list:\"\ncat $PBS_NODEFILE\necho \"========================================\"\npython fit_large_dataset.py \\\\\n    --data-file ./data/large_dataset.h5 \\\\\n    --output-dir ./results \\\\\n    --checkpoint-dir $NLSQ_CHECKPOINT_DIR \\\\\n    --enable-checkpoints \\\\\n    --checkpoint-interval 50\necho \"========================================\"\necho \"Job Completed: $(date)\"\necho \"========================================\"\n\"\"\"\n    pbs_script_path = Path(\"nlsq_fit.pbs\")\n    pbs_script_path.write_text(pbs_script)\n    print(\"  Created PBS job script: nlsq_fit.pbs\")\n    print(\"  Key directives:\")\n    print(\"    #PBS -l select=4:ncpus=32:ngpus=8:mem=256gb\")\n    print(\"    #PBS -l walltime=24:00:00\")\n    print(\"    #PBS -q gpu\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "7. Defense Layers for Checkpoint Resume (v0.3.6+)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print()\nprint(\"7. Defense Layers for Checkpoint Resume (v0.3.6+):\")\nprint(\"-\" * 70)\nprint()\nprint(\"When resuming from checkpoints, your initial parameters are near-optimal.\")\nprint(\"This is a classic warm-start scenario where defense layers are critical.\")\nprint()\nprint(\"Without defense layers, L-BFGS warmup can DIVERGE from your checkpoint:\")\nprint(\"  - Momentum builds up from large initial gradients\")\nprint(\"  - Parameters overshoot and loss increases\")\nprint(\"  - All progress from previous run is lost\")\nprint()\nprint(\"With 4-layer defense, checkpoint resume is protected:\")\nprint(\"  Layer 1: Detects you're starting near-optimal -> may skip warmup\")\nprint(\"  Layer 2: Scales learning rate based on initial fit quality\")\nprint(\"  Layer 3: Aborts warmup if loss increases > 5%\")\nprint(\"  Layer 4: Clips step magnitudes to prevent overshooting\")\nprint()\nprint(\"Recommended configuration for checkpoint resume:\")\nprint()\nprint(\"  from nlsq import HybridStreamingConfig\")\nprint()\nprint(\"  # Use defense_strict for checkpoint resume scenarios\")\nprint(\"  config = HybridStreamingConfig.defense_strict()\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cleanup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print()\nprint(\"Cleaning up...\")\nfor path_str in [\n    \"nlsq_checkpoints\",\n    \"demo_checkpoints\",\n    \"my_project_checkpoints\",\n]:\n    path = Path(path_str)\n    if path.exists():\n        shutil.rmtree(path)\n        print(f\"  Removed: {path_str}\")\nif pbs_script_path.exists():\n    pbs_script_path.unlink()\n    print(\"  Removed: nlsq_fit.pbs\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "    print()\n    print(\"=\" * 70)\n    print(\"Summary\")\n    print(\"=\" * 70)\n    print()\n    print(\"HPC Integration:\")\n    print(\"  - ClusterDetector.detect() for PBS Pro detection\")\n    print(\"  - ClusterInfo for cluster metadata (nodes, GPUs, job ID)\")\n    print()\n    print(\"Checkpointing:\")\n    print(\"  - Use streaming strategy for very large datasets\")\n    print(\"  - create_checkpoint_directory() for timestamped directories\")\n    print(\"  - JSON-based checkpoints for portability\")\n    print()\n    print(\"PBS Pro Job Scripts:\")\n    print(\"  - #PBS -l select=N:ncpus=C:ngpus=G:mem=Mgb\")\n    print(\"  - Environment variables: NLSQ_WORKFLOW_GOAL, NLSQ_MEMORY_LIMIT_GB\")\n    print(\"  - Checkpoint directory: NLSQ_CHECKPOINT_DIR\")\n    print()\n    print(\"Defense Layers for Checkpoint Resume (v0.3.6+):\")\n    print(\"  - Checkpoint resume = warm-start scenario (parameters near optimal)\")\n    print(\"  - Use HybridStreamingConfig.defense_strict() for resume protection\")\n    print(\"  - 4-layer defense prevents L-BFGS warmup from diverging\")\nif __name__ == \"__main__\":\n    main()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
