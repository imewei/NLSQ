{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# HPC Integration and Checkpointing\n",
    "\n",
    "> Configure NLSQ for HPC clusters with fault-tolerant checkpointing\n",
    "\n",
    "**30 minutes** | **Level: Advanced**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "- Use `ClusterDetector` and `ClusterInfo` to detect PBS Pro environments\n",
    "- Configure `WorkflowTier.STREAMING_CHECKPOINT` for fault tolerance\n",
    "- Set up checkpointing with `enable_checkpoints=True` and `checkpoint_dir`\n",
    "- Use `create_checkpoint_directory()` for timestamp-based directories\n",
    "- Implement checkpoint resume workflows\n",
    "- Create PBS Pro job scripts for NLSQ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Learning Path\n",
    "\n",
    "**You are here:** Workflow System > **HPC and Checkpointing**\n",
    "\n",
    "```\n",
    "YAML Configuration --> [You are here: HPC & Checkpointing]\n",
    "```\n",
    "\n",
    "**Prerequisites:**\n",
    "- [05_yaml_configuration.ipynb](05_yaml_configuration.ipynb) - YAML configuration basics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Before You Begin\n",
    "\n",
    "**Required knowledge:**\n",
    "- Familiarity with HPC batch schedulers (PBS Pro)\n",
    "- Understanding of NLSQ workflow tiers\n",
    "\n",
    "**Required software:**\n",
    "- NLSQ >= 0.3.4\n",
    "- Python >= 3.12\n",
    "\n",
    "**Note:** This tutorial demonstrates HPC features. Some examples require\n",
    "a PBS Pro cluster environment to run fully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "HPC clusters enable fitting on massive datasets (100M+ points) but introduce challenges:\n",
    "\n",
    "1. **Job time limits:** Cluster jobs have wall time limits (hours to days)\n",
    "2. **Node failures:** Hardware failures can terminate jobs unexpectedly\n",
    "3. **Preemption:** Higher-priority jobs may preempt your job\n",
    "4. **Multi-GPU scaling:** Need to efficiently use available GPUs\n",
    "\n",
    "**Checkpointing solves these by:**\n",
    "- Saving optimization state periodically\n",
    "- Enabling resume from last checkpoint\n",
    "- Preventing loss of computation on failure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Quick Start (30 seconds)\n",
    "\n",
    "Enable checkpointing in a single line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlsq.workflow import WorkflowConfig, WorkflowTier, create_checkpoint_directory\n",
    "\n",
    "# Create checkpoint-enabled workflow\n",
    "config = WorkflowConfig(\n",
    "    tier=WorkflowTier.STREAMING_CHECKPOINT,\n",
    "    enable_checkpoints=True,\n",
    "    checkpoint_dir=create_checkpoint_directory(),\n",
    ")\n",
    "\n",
    "print(f\"Tier: {config.tier}\")\n",
    "print(f\"Checkpoints enabled: {config.enable_checkpoints}\")\n",
    "print(f\"Checkpoint directory: {config.checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from nlsq.workflow import (\n",
    "    WorkflowConfig,\n",
    "    WorkflowTier,\n",
    "    OptimizationGoal,\n",
    "    ClusterDetector,\n",
    "    ClusterInfo,\n",
    "    create_checkpoint_directory,\n",
    "    get_multi_gpu_config,\n",
    "    create_distributed_config,\n",
    "    WORKFLOW_PRESETS,\n",
    ")\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Tutorial Content\n",
    "\n",
    "### Section 1: ClusterDetector and ClusterInfo\n",
    "\n",
    "The `ClusterDetector` automatically detects PBS Pro cluster environments\n",
    "via the `$PBS_NODEFILE` environment variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cluster detector\n",
    "detector = ClusterDetector()\n",
    "\n",
    "print(\"ClusterDetector methods:\")\n",
    "print(\"  - detect(): Auto-detect cluster environment\")\n",
    "print(\"  - is_pbs_environment(): Check for PBS Pro\")\n",
    "print(\"  - detect_pbs(): Get PBS-specific info\")\n",
    "print(\"  - detect_local_gpus(): Get local GPU count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current environment\n",
    "print(f\"PBS environment detected: {detector.is_pbs_environment()}\")\n",
    "\n",
    "# Try to detect cluster\n",
    "cluster_info = detector.detect()\n",
    "\n",
    "if cluster_info:\n",
    "    print(f\"\\nCluster detected:\")\n",
    "    print(f\"  Scheduler: {cluster_info.scheduler}\")\n",
    "    print(f\"  Node count: {cluster_info.node_count}\")\n",
    "    print(f\"  GPUs per node: {cluster_info.gpus_per_node}\")\n",
    "    print(f\"  Total GPUs: {cluster_info.total_gpus}\")\n",
    "    if cluster_info.job_id:\n",
    "        print(f\"  Job ID: {cluster_info.job_id}\")\n",
    "else:\n",
    "    print(\"\\nNo cluster environment detected (running locally)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ClusterInfo structure (for reference)\n",
    "print(\"ClusterInfo fields:\")\n",
    "print(\"  - node_count: int\")\n",
    "print(\"  - gpus_per_node: int\")\n",
    "print(\"  - total_gpus: int\")\n",
    "print(\"  - node_list: list[str]\")\n",
    "print(\"  - scheduler: str ('pbs', 'local', 'unknown')\")\n",
    "print(\"  - job_id: str | None\")\n",
    "print(\"  - interconnect: str | None ('infiniband', 'ethernet')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated PBS environment for demonstration\n",
    "simulated_cluster = ClusterInfo(\n",
    "    node_count=4,\n",
    "    gpus_per_node=8,\n",
    "    total_gpus=32,\n",
    "    node_list=[\"node01\", \"node02\", \"node03\", \"node04\"],\n",
    "    scheduler=\"pbs\",\n",
    "    job_id=\"12345.pbs_server\",\n",
    "    interconnect=\"infiniband\",\n",
    ")\n",
    "\n",
    "print(\"Simulated PBS cluster:\")\n",
    "print(f\"  Nodes: {simulated_cluster.node_count}\")\n",
    "print(f\"  GPUs: {simulated_cluster.total_gpus}\")\n",
    "print(f\"  Job ID: {simulated_cluster.job_id}\")\n",
    "print(f\"  Interconnect: {simulated_cluster.interconnect}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "### Section 2: WorkflowTier.STREAMING_CHECKPOINT\n",
    "\n",
    "The `STREAMING_CHECKPOINT` tier combines streaming optimization with\n",
    "automatic checkpointing for fault tolerance on massive datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View available workflow tiers\n",
    "print(\"Available WorkflowTiers:\")\n",
    "for tier in WorkflowTier:\n",
    "    print(f\"  - {tier.name}\")\n",
    "\n",
    "print(\"\\nTier descriptions:\")\n",
    "print(\"  STANDARD: curve_fit() for small datasets\")\n",
    "print(\"  CHUNKED: LargeDatasetFitter with automatic chunking\")\n",
    "print(\"  STREAMING: AdaptiveHybridStreamingOptimizer for huge datasets\")\n",
    "print(\"  STREAMING_CHECKPOINT: Streaming + automatic checkpointing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create STREAMING_CHECKPOINT configuration\n",
    "hpc_config = WorkflowConfig(\n",
    "    tier=WorkflowTier.STREAMING_CHECKPOINT,\n",
    "    goal=OptimizationGoal.ROBUST,\n",
    "    gtol=1e-7,\n",
    "    ftol=1e-7,\n",
    "    xtol=1e-7,\n",
    "    enable_checkpoints=True,\n",
    "    checkpoint_dir=\"./nlsq_checkpoints\",\n",
    "    enable_multistart=True,\n",
    "    n_starts=10,\n",
    ")\n",
    "\n",
    "print(\"HPC Configuration:\")\n",
    "print(f\"  tier: {hpc_config.tier}\")\n",
    "print(f\"  goal: {hpc_config.goal}\")\n",
    "print(f\"  tolerances: gtol={hpc_config.gtol}, ftol={hpc_config.ftol}\")\n",
    "print(f\"  enable_checkpoints: {hpc_config.enable_checkpoints}\")\n",
    "print(f\"  checkpoint_dir: {hpc_config.checkpoint_dir}\")\n",
    "print(f\"  enable_multistart: {hpc_config.enable_multistart}\")\n",
    "print(f\"  n_starts: {hpc_config.n_starts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "### Section 3: Checkpointing Configuration\n",
    "\n",
    "Configure checkpointing with these key parameters:\n",
    "\n",
    "| Parameter | Description |\n",
    "|-----------|-------------|\n",
    "| `enable_checkpoints` | Enable automatic checkpointing |\n",
    "| `checkpoint_dir` | Directory to save checkpoints |\n",
    "| `checkpoint_interval` | Iterations between checkpoints (in HybridStreamingConfig) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_checkpoint_directory() creates timestamped directories\n",
    "checkpoint_dir = create_checkpoint_directory()\n",
    "\n",
    "print(f\"Created checkpoint directory: {checkpoint_dir}\")\n",
    "print(f\"Directory exists: {Path(checkpoint_dir).exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom base directory\n",
    "custom_checkpoint_dir = create_checkpoint_directory(base_dir=\"./my_project_checkpoints\")\n",
    "\n",
    "print(f\"Custom checkpoint directory: {custom_checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full checkpoint configuration\n",
    "checkpoint_config = WorkflowConfig(\n",
    "    tier=WorkflowTier.STREAMING_CHECKPOINT,\n",
    "    goal=OptimizationGoal.ROBUST,\n",
    "    enable_checkpoints=True,\n",
    "    checkpoint_dir=checkpoint_dir,\n",
    ")\n",
    "\n",
    "# Serialize config for saving\n",
    "config_dict = checkpoint_config.to_dict()\n",
    "print(\"Serialized config:\")\n",
    "for key, value in config_dict.items():\n",
    "    if not key.startswith(\"_\"):\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "### Section 4: Checkpoint Resume Workflow\n",
    "\n",
    "Implement checkpoint-resume logic for fault-tolerant optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(checkpoint_dir, iteration, params, loss, metadata=None):\n",
    "    \"\"\"Save optimization checkpoint.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    checkpoint_dir : str\n",
    "        Directory to save checkpoint\n",
    "    iteration : int\n",
    "        Current iteration number\n",
    "    params : np.ndarray\n",
    "        Current parameter values\n",
    "    loss : float\n",
    "        Current loss value\n",
    "    metadata : dict, optional\n",
    "        Additional metadata to save\n",
    "    \"\"\"\n",
    "    checkpoint_path = Path(checkpoint_dir) / f\"checkpoint_{iteration:06d}.pkl\"\n",
    "    \n",
    "    checkpoint_data = {\n",
    "        \"iteration\": iteration,\n",
    "        \"params\": np.array(params),\n",
    "        \"loss\": float(loss),\n",
    "        \"metadata\": metadata or {},\n",
    "    }\n",
    "    \n",
    "    with open(checkpoint_path, \"wb\") as f:\n",
    "        pickle.dump(checkpoint_data, f)\n",
    "    \n",
    "    print(f\"Saved checkpoint: {checkpoint_path.name}\")\n",
    "    return checkpoint_path\n",
    "\n",
    "\n",
    "def load_latest_checkpoint(checkpoint_dir):\n",
    "    \"\"\"Load the most recent checkpoint.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    checkpoint_dir : str\n",
    "        Directory containing checkpoints\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict or None\n",
    "        Checkpoint data if found, None otherwise\n",
    "    \"\"\"\n",
    "    checkpoint_dir = Path(checkpoint_dir)\n",
    "    if not checkpoint_dir.exists():\n",
    "        return None\n",
    "    \n",
    "    # Find all checkpoint files\n",
    "    checkpoints = list(checkpoint_dir.glob(\"checkpoint_*.pkl\"))\n",
    "    if not checkpoints:\n",
    "        return None\n",
    "    \n",
    "    # Sort by name (which includes iteration number)\n",
    "    latest = sorted(checkpoints)[-1]\n",
    "    \n",
    "    with open(latest, \"rb\") as f:\n",
    "        checkpoint_data = pickle.load(f)\n",
    "    \n",
    "    print(f\"Loaded checkpoint: {latest.name}\")\n",
    "    return checkpoint_data\n",
    "\n",
    "\n",
    "print(\"Checkpoint functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate checkpoint save/load\n",
    "demo_dir = create_checkpoint_directory(base_dir=\"./demo_checkpoints\")\n",
    "\n",
    "# Simulate saving checkpoints during optimization\n",
    "for i in range(0, 30, 10):\n",
    "    params = np.array([2.0 + 0.01 * i, 1.0 - 0.005 * i, 0.5])\n",
    "    loss = 0.1 / (1 + i * 0.1)\n",
    "    save_checkpoint(demo_dir, i, params, loss, metadata={\"epoch\": i // 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load latest checkpoint for resume\n",
    "latest = load_latest_checkpoint(demo_dir)\n",
    "\n",
    "if latest:\n",
    "    print(f\"\\nResuming from iteration {latest['iteration']}\")\n",
    "    print(f\"  Parameters: {latest['params']}\")\n",
    "    print(f\"  Loss: {latest['loss']:.6f}\")\n",
    "    print(f\"  Metadata: {latest['metadata']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resume-aware optimization loop pattern\n",
    "def optimization_with_checkpoints(checkpoint_dir, max_iterations=100, checkpoint_interval=10):\n",
    "    \"\"\"Example optimization loop with checkpoint support.\"\"\"\n",
    "    \n",
    "    # Try to resume from checkpoint\n",
    "    checkpoint = load_latest_checkpoint(checkpoint_dir)\n",
    "    \n",
    "    if checkpoint:\n",
    "        start_iteration = checkpoint[\"iteration\"] + 1\n",
    "        params = checkpoint[\"params\"]\n",
    "        print(f\"Resuming from iteration {start_iteration}\")\n",
    "    else:\n",
    "        start_iteration = 0\n",
    "        params = np.array([1.0, 1.0, 0.0])  # Initial guess\n",
    "        print(\"Starting fresh optimization\")\n",
    "    \n",
    "    # Optimization loop\n",
    "    for iteration in range(start_iteration, max_iterations):\n",
    "        # Simulate optimization step\n",
    "        params = params + 0.001 * np.random.randn(3)\n",
    "        loss = np.sum(params ** 2)  # Dummy loss\n",
    "        \n",
    "        # Checkpoint at intervals\n",
    "        if iteration > 0 and iteration % checkpoint_interval == 0:\n",
    "            save_checkpoint(checkpoint_dir, iteration, params, loss)\n",
    "    \n",
    "    # Final checkpoint\n",
    "    save_checkpoint(checkpoint_dir, max_iterations - 1, params, loss)\n",
    "    \n",
    "    return params\n",
    "\n",
    "# Run optimization (will checkpoint every 10 iterations)\n",
    "final_params = optimization_with_checkpoints(demo_dir, max_iterations=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "### Section 5: HPC Distributed Configuration\n",
    "\n",
    "Use `create_distributed_config()` to generate HPC-optimized settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create distributed config from cluster info\n",
    "dist_config = create_distributed_config(simulated_cluster)\n",
    "\n",
    "print(\"Distributed configuration for PBS cluster:\")\n",
    "for key, value in dist_config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get multi-GPU configuration\n",
    "gpu_config = get_multi_gpu_config(simulated_cluster)\n",
    "\n",
    "if gpu_config:\n",
    "    print(\"Multi-GPU configuration:\")\n",
    "    print(f\"  n_devices: {gpu_config.n_devices}\")\n",
    "    print(f\"  per_device_batch_size: {gpu_config.per_device_batch_size}\")\n",
    "    print(f\"  total_batch_size: {gpu_config.total_batch_size}\")\n",
    "    print(f\"  use_pmap: {gpu_config.use_pmap}\")\n",
    "    print(f\"  use_pjit: {gpu_config.use_pjit}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the hpc_distributed preset\n",
    "hpc_preset = WORKFLOW_PRESETS[\"hpc_distributed\"]\n",
    "\n",
    "print(\"HPC Distributed Preset:\")\n",
    "for key, value in hpc_preset.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-30",
   "metadata": {},
   "source": [
    "### Section 6: PBS Pro Job Script\n",
    "\n",
    "Here's an example PBS Pro job script for running NLSQ on an HPC cluster.\n",
    "\n",
    "**Note:** This is for PBS Pro specifically (not SLURM) per the requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate example PBS job script\n",
    "pbs_script = '''#!/bin/bash\n",
    "#PBS -N nlsq_fit\n",
    "#PBS -l select=4:ncpus=32:ngpus=8:mem=256gb\n",
    "#PBS -l walltime=24:00:00\n",
    "#PBS -q gpu\n",
    "#PBS -j oe\n",
    "#PBS -o nlsq_fit.log\n",
    "\n",
    "# NLSQ Curve Fitting Job Script for PBS Pro\n",
    "# ===========================================\n",
    "#\n",
    "# This script demonstrates running NLSQ on a multi-node GPU cluster.\n",
    "# Adjust the resource requests based on your cluster configuration.\n",
    "#\n",
    "# Resources requested:\n",
    "#   - 4 nodes with 32 CPUs each\n",
    "#   - 8 GPUs per node (e.g., NVIDIA A100)\n",
    "#   - 256GB RAM per node\n",
    "#   - 24 hour walltime\n",
    "\n",
    "# Change to submission directory\n",
    "cd $PBS_O_WORKDIR\n",
    "\n",
    "# Load required modules (adjust for your cluster)\n",
    "module load python/3.12\n",
    "module load cuda/12.0\n",
    "module load cudnn/8.9\n",
    "\n",
    "# Activate virtual environment\n",
    "source ./venv/bin/activate\n",
    "\n",
    "# Set NLSQ environment variables\n",
    "export NLSQ_WORKFLOW_GOAL=robust\n",
    "export NLSQ_MEMORY_LIMIT_GB=200\n",
    "export NLSQ_CHECKPOINT_DIR=$PBS_O_WORKDIR/checkpoints\n",
    "\n",
    "# Create checkpoint directory\n",
    "mkdir -p $NLSQ_CHECKPOINT_DIR\n",
    "\n",
    "# Display job information\n",
    "echo \"========================================\"\n",
    "echo \"NLSQ Fitting Job Started\"\n",
    "echo \"========================================\"\n",
    "echo \"Job ID: $PBS_JOBID\"\n",
    "echo \"Node list:\"\n",
    "cat $PBS_NODEFILE\n",
    "echo \"========================================\"\n",
    "\n",
    "# Run NLSQ fitting script\n",
    "python fit_large_dataset.py \\\\\n",
    "    --data-file ./data/large_dataset.h5 \\\\\n",
    "    --output-dir ./results \\\\\n",
    "    --checkpoint-dir $NLSQ_CHECKPOINT_DIR \\\\\n",
    "    --enable-checkpoints \\\\\n",
    "    --checkpoint-interval 50\n",
    "\n",
    "echo \"========================================\"\n",
    "echo \"Job Completed: $(date)\"\n",
    "echo \"========================================\"\n",
    "'''\n",
    "\n",
    "# Save the PBS script\n",
    "pbs_script_path = Path(\"nlsq_fit.pbs\")\n",
    "pbs_script_path.write_text(pbs_script)\n",
    "\n",
    "print(\"Created PBS job script: nlsq_fit.pbs\")\n",
    "print()\n",
    "print(\"Contents:\")\n",
    "print(\"=\" * 50)\n",
    "print(pbs_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-32",
   "metadata": {},
   "source": [
    "### Section 7: Example Fitting Script for HPC\n",
    "\n",
    "Here's an example Python script to use with the PBS job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example HPC fitting script content\n",
    "hpc_script = '''#!/usr/bin/env python\n",
    "\"\"\"NLSQ HPC Fitting Script with Checkpointing.\n",
    "\n",
    "This script demonstrates running NLSQ on HPC clusters with:\n",
    "- Automatic cluster detection\n",
    "- Checkpoint-based fault tolerance\n",
    "- Environment variable configuration\n",
    "\n",
    "Usage:\n",
    "    python fit_large_dataset.py --data-file data.h5 --output-dir results\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "\n",
    "from nlsq import curve_fit_large\n",
    "from nlsq.workflow import (\n",
    "    ClusterDetector,\n",
    "    create_checkpoint_directory,\n",
    "    load_config_with_overrides,\n",
    ")\n",
    "\n",
    "\n",
    "def model(x, a, b, c):\n",
    "    \"\"\"Model function.\"\"\"\n",
    "    return a * jnp.exp(-b * x) + c\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description=\"NLSQ HPC Fitting\")\n",
    "    parser.add_argument(\"--data-file\", required=True)\n",
    "    parser.add_argument(\"--output-dir\", default=\"./results\")\n",
    "    parser.add_argument(\"--checkpoint-dir\", default=None)\n",
    "    parser.add_argument(\"--enable-checkpoints\", action=\"store_true\")\n",
    "    parser.add_argument(\"--checkpoint-interval\", type=int, default=50)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Detect cluster environment\n",
    "    detector = ClusterDetector()\n",
    "    cluster_info = detector.detect()\n",
    "    \n",
    "    if cluster_info:\n",
    "        print(f\"Running on {cluster_info.scheduler} cluster\")\n",
    "        print(f\"  Nodes: {cluster_info.node_count}\")\n",
    "        print(f\"  Total GPUs: {cluster_info.total_gpus}\")\n",
    "    else:\n",
    "        print(\"Running locally\")\n",
    "    \n",
    "    # Load configuration with environment overrides\n",
    "    config = load_config_with_overrides()\n",
    "    memory_limit = config.get(\"memory_limit_gb\", 16.0)\n",
    "    \n",
    "    # Setup checkpointing\n",
    "    checkpoint_dir = args.checkpoint_dir\n",
    "    if checkpoint_dir is None and args.enable_checkpoints:\n",
    "        checkpoint_dir = create_checkpoint_directory()\n",
    "    \n",
    "    # Load data (example - replace with actual data loading)\n",
    "    # In production, use h5py or similar for large datasets\n",
    "    print(f\"Loading data from: {args.data_file}\")\n",
    "    # x_data, y_data = load_hdf5_data(args.data_file)\n",
    "    \n",
    "    # For demonstration, generate synthetic data\n",
    "    n_points = 10_000_000\n",
    "    x_data = np.linspace(0, 10, n_points)\n",
    "    y_data = 2.5 * np.exp(-1.3 * x_data) + 0.5 + 0.1 * np.random.randn(n_points)\n",
    "    \n",
    "    print(f\"Dataset size: {n_points:,} points\")\n",
    "    \n",
    "    # Run fitting\n",
    "    popt, pcov = curve_fit_large(\n",
    "        model,\n",
    "        x_data,\n",
    "        y_data,\n",
    "        p0=[1.0, 1.0, 0.0],\n",
    "        memory_limit_gb=memory_limit,\n",
    "        multistart=True,\n",
    "        n_starts=10,\n",
    "    )\n",
    "    \n",
    "    # Save results\n",
    "    output_dir = Path(args.output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    np.save(output_dir / \"popt.npy\", popt)\n",
    "    np.save(output_dir / \"pcov.npy\", pcov)\n",
    "    \n",
    "    print(f\"\\\\nResults saved to: {output_dir}\")\n",
    "    print(f\"Fitted parameters: a={popt[0]:.4f}, b={popt[1]:.4f}, c={popt[2]:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Save the example script\n",
    "script_path = Path(\"fit_large_dataset.py\")\n",
    "script_path.write_text(hpc_script)\n",
    "\n",
    "print(\"Created example fitting script: fit_large_dataset.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-34",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "After completing this notebook, remember:\n",
    "\n",
    "1. **Cluster detection:**\n",
    "   - `ClusterDetector().detect()` auto-detects PBS environments\n",
    "   - `ClusterInfo` provides node count, GPU count, job ID\n",
    "\n",
    "2. **Checkpointing configuration:**\n",
    "   - Use `WorkflowTier.STREAMING_CHECKPOINT` for fault tolerance\n",
    "   - Set `enable_checkpoints=True` and `checkpoint_dir`\n",
    "   - Use `create_checkpoint_directory()` for timestamped directories\n",
    "\n",
    "3. **PBS Pro integration:**\n",
    "   - Set resources with `#PBS -l select=...:ngpus=...`\n",
    "   - Use environment variables for configuration\n",
    "   - Create checkpoint directory with `mkdir -p`\n",
    "\n",
    "4. **Resume workflow:**\n",
    "   - Check for existing checkpoints at job start\n",
    "   - Resume from latest checkpoint iteration\n",
    "   - Save checkpoints at regular intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Questions\n",
    "\n",
    "**Q: How often should I checkpoint?**\n",
    "\n",
    "A: Balance checkpoint frequency against I/O overhead. For hour-long jobs, checkpoint every 10-15 minutes. For multi-day jobs, every 30-60 minutes.\n",
    "\n",
    "**Q: Can I use SLURM instead of PBS?**\n",
    "\n",
    "A: NLSQ currently auto-detects PBS via `$PBS_NODEFILE`. For SLURM, you can manually create `ClusterInfo` from `$SLURM_*` variables.\n",
    "\n",
    "**Q: How much disk space do checkpoints use?**\n",
    "\n",
    "A: Each checkpoint stores parameters (small) plus optimizer state. Typically a few MB per checkpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-36",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Related Resources\n",
    "\n",
    "**Previous tutorials:**\n",
    "- [05_yaml_configuration.ipynb](05_yaml_configuration.ipynb) - YAML configuration\n",
    "- [06_auto_selection.ipynb](06_auto_selection.ipynb) - Automatic workflow selection\n",
    "\n",
    "**Further reading:**\n",
    "- [PBS Pro User Guide](https://www.pbsworks.com/documentation)\n",
    "- [JAX Multi-GPU Documentation](https://jax.readthedocs.io/en/latest/multi_process.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-37",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Glossary\n",
    "\n",
    "**PBS Pro:** Portable Batch System Professional - an HPC job scheduler.\n",
    "\n",
    "**Checkpoint:** A saved snapshot of optimization state that enables resume.\n",
    "\n",
    "**pmap/pjit:** JAX primitives for multi-device parallelism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "import shutil\n",
    "\n",
    "# Clean up demo directories and files\n",
    "for path in [\"nlsq_checkpoints\", \"demo_checkpoints\", \"my_project_checkpoints\"]:\n",
    "    if Path(path).exists():\n",
    "        shutil.rmtree(path)\n",
    "        print(f\"Cleaned up: {path}\")\n",
    "\n",
    "for path in [\"nlsq_fit.pbs\", \"fit_large_dataset.py\"]:\n",
    "    if Path(path).exists():\n",
    "        Path(path).unlink()\n",
    "        print(f\"Cleaned up: {path}\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Summary\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nHPC Integration:\")\n",
    "print(\"  - ClusterDetector for PBS Pro detection\")\n",
    "print(\"  - ClusterInfo for cluster metadata\")\n",
    "print(\"\\nCheckpointing:\")\n",
    "print(\"  - WorkflowTier.STREAMING_CHECKPOINT\")\n",
    "print(\"  - enable_checkpoints=True\")\n",
    "print(\"  - create_checkpoint_directory()\")\n",
    "print(\"\\nPBS Pro:\")\n",
    "print(\"  - #PBS -l select=N:ngpus=M\")\n",
    "print(\"  - Environment variable overrides\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
