{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow Tiers: STANDARD, CHUNKED, STREAMING\n",
    "\n",
    "> Understand the different processing tiers and when each is used\n",
    "\n",
    "**20 minutes** | **Level: Intermediate**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "- Understand the four workflow tiers: STANDARD, CHUNKED, STREAMING, STREAMING_CHECKPOINT\n",
    "- Know the automatic tier selection thresholds based on dataset size\n",
    "- Override automatic selection with `WorkflowConfig(tier=...)`\n",
    "- Choose the appropriate tier for your dataset and memory constraints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Learning Path\n",
    "\n",
    "**You are here:** Workflow System > **Workflow Tiers**\n",
    "\n",
    "```\n",
    "fit() Quickstart --> [You are here: Workflow Tiers] --> Optimization Goals --> Presets\n",
    "```\n",
    "\n",
    "**Recommended flow:**\n",
    "- **Previous:** [01_fit_quickstart.ipynb](01_fit_quickstart.ipynb) - Basic fit() usage\n",
    "- **Next:** [03_optimization_goals.ipynb](03_optimization_goals.ipynb) - FAST, ROBUST, QUALITY goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Before You Begin\n",
    "\n",
    "**Required knowledge:**\n",
    "- Basic familiarity with `fit()` or `curve_fit()`\n",
    "- Understanding of dataset size and memory constraints\n",
    "\n",
    "**Required software:**\n",
    "- NLSQ >= 0.3.4\n",
    "- Python >= 3.12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "Different dataset sizes require different processing strategies:\n",
    "\n",
    "- **Small datasets** (< 10K points): Fit entirely in memory, no special handling needed\n",
    "- **Medium datasets** (10K - 10M points): May need chunking to manage memory\n",
    "- **Large datasets** (10M - 100M points): Require streaming to avoid memory overflow\n",
    "- **Massive datasets** (> 100M points): Need checkpointing for fault tolerance\n",
    "\n",
    "NLSQ's workflow system automatically selects the best tier, but understanding the tiers\n",
    "helps you make informed decisions when memory is constrained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Quick Start (30 seconds)\n",
    "\n",
    "See workflow tiers in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure matplotlib for inline plotting (MUST come before imports)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlsq import WorkflowConfig, WorkflowTier\n",
    "\n",
    "# View all available tiers\n",
    "for tier in WorkflowTier:\n",
    "    print(f\"  {tier.name}: {tier}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from nlsq import WorkflowConfig, WorkflowTier, OptimizationGoal, fit\n",
    "from nlsq.workflow import (\n",
    "    DatasetSizeTier,\n",
    "    MemoryTier,\n",
    "    auto_select_workflow,\n",
    ")\n",
    "from nlsq.large_dataset import MemoryEstimator, get_memory_tier\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Tutorial Content\n",
    "\n",
    "### Section 1: The Four Workflow Tiers\n",
    "\n",
    "NLSQ provides four workflow tiers, each optimized for different dataset sizes and memory constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display tier information\n",
    "tier_info = {\n",
    "    WorkflowTier.STANDARD: {\n",
    "        \"description\": \"Standard curve_fit() for small datasets\",\n",
    "        \"dataset_size\": \"< 10K points\",\n",
    "        \"memory\": \"O(N) - loads all data into memory\",\n",
    "        \"use_case\": \"Most common use case, full precision\",\n",
    "    },\n",
    "    WorkflowTier.CHUNKED: {\n",
    "        \"description\": \"LargeDatasetFitter with automatic chunking\",\n",
    "        \"dataset_size\": \"10K - 10M points\",\n",
    "        \"memory\": \"O(chunk_size) - processes data in chunks\",\n",
    "        \"use_case\": \"Medium-to-large datasets, memory-constrained\",\n",
    "    },\n",
    "    WorkflowTier.STREAMING: {\n",
    "        \"description\": \"AdaptiveHybridStreamingOptimizer for huge datasets\",\n",
    "        \"dataset_size\": \"10M - 100M points\",\n",
    "        \"memory\": \"O(batch_size) - mini-batch gradient descent\",\n",
    "        \"use_case\": \"Large datasets with limited memory\",\n",
    "    },\n",
    "    WorkflowTier.STREAMING_CHECKPOINT: {\n",
    "        \"description\": \"Streaming with automatic checkpointing\",\n",
    "        \"dataset_size\": \"> 100M points\",\n",
    "        \"memory\": \"O(batch_size) + checkpoint storage\",\n",
    "        \"use_case\": \"Massive datasets, fault tolerance required\",\n",
    "    },\n",
    "}\n",
    "\n",
    "print(\"Workflow Tiers Overview\")\n",
    "print(\"=\" * 70)\n",
    "for tier, info in tier_info.items():\n",
    "    print(f\"\\n{tier.name}:\")\n",
    "    print(f\"  Description: {info['description']}\")\n",
    "    print(f\"  Dataset Size: {info['dataset_size']}\")\n",
    "    print(f\"  Memory: {info['memory']}\")\n",
    "    print(f\"  Use Case: {info['use_case']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2: Automatic Tier Selection\n",
    "\n",
    "The `WorkflowSelector` automatically chooses the appropriate tier based on:\n",
    "1. Dataset size (number of points)\n",
    "2. Available memory (CPU + GPU)\n",
    "3. Optimization goal (FAST, ROBUST, QUALITY, MEMORY_EFFICIENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset size thresholds for tier selection\n",
    "print(\"Dataset Size Tiers and Thresholds\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for size_tier in DatasetSizeTier:\n",
    "    max_pts = size_tier.max_points\n",
    "    tol = size_tier.tolerance\n",
    "    if max_pts == float(\"inf\"):\n",
    "        print(f\"{size_tier.name:12s}: > 100M points, tolerance = {tol:.0e}\")\n",
    "    else:\n",
    "        print(f\"{size_tier.name:12s}: < {max_pts/1e6:.0f}M points, tolerance = {tol:.0e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory tier thresholds\n",
    "print(\"\\nMemory Tiers\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for mem_tier in MemoryTier:\n",
    "    print(f\"{mem_tier.name:10s}: {mem_tier.description}\")\n",
    "\n",
    "# Check current system memory\n",
    "available_memory = MemoryEstimator.get_available_memory_gb()\n",
    "current_tier = get_memory_tier(available_memory)\n",
    "print(f\"\\nCurrent system: {available_memory:.1f} GB available -> {current_tier.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate automatic tier selection for different dataset sizes\n",
    "test_sizes = [1_000, 50_000, 500_000, 5_000_000, 50_000_000, 500_000_000]\n",
    "n_params = 5\n",
    "\n",
    "print(\"Automatic Tier Selection (based on current memory)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Available memory: {available_memory:.1f} GB\")\n",
    "print()\n",
    "\n",
    "for n_points in test_sizes:\n",
    "    config = auto_select_workflow(n_points, n_params)\n",
    "    config_type = type(config).__name__\n",
    "    \n",
    "    # Determine tier from config type\n",
    "    if \"GlobalOptimization\" in config_type:\n",
    "        tier = \"STANDARD (with multi-start)\"\n",
    "    elif \"LDMemory\" in config_type:\n",
    "        tier = \"STANDARD or CHUNKED\"\n",
    "    elif \"HybridStreaming\" in config_type:\n",
    "        tier = \"STREAMING or STREAMING_CHECKPOINT\"\n",
    "    else:\n",
    "        tier = config_type\n",
    "    \n",
    "    if n_points >= 1_000_000:\n",
    "        size_str = f\"{n_points/1_000_000:.0f}M\"\n",
    "    elif n_points >= 1_000:\n",
    "        size_str = f\"{n_points/1_000:.0f}K\"\n",
    "    else:\n",
    "        size_str = str(n_points)\n",
    "    \n",
    "    print(f\"{size_str:>8s} points -> {tier}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 3: Tier Selection Decision Tree\n",
    "\n",
    "The following diagram shows how tiers are selected based on dataset size and memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tier selection decision tree visualization\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 10)\n",
    "ax.axis('off')\n",
    "\n",
    "# Title\n",
    "ax.text(5, 9.5, \"Workflow Tier Selection Decision Tree\", ha='center', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Root node\n",
    "ax.add_patch(plt.Rectangle((3.5, 8.2), 3, 0.8, fill=True, facecolor='lightblue', edgecolor='black'))\n",
    "ax.text(5, 8.6, \"Dataset Size?\", ha='center', va='center', fontsize=11)\n",
    "\n",
    "# Level 1 branches\n",
    "# Small\n",
    "ax.plot([4.2, 2, 2], [8.2, 7.5, 7.0], 'k-', linewidth=1)\n",
    "ax.text(2.5, 7.7, \"< 10K\", fontsize=9)\n",
    "ax.add_patch(plt.Rectangle((0.5, 6.2), 3, 0.8, fill=True, facecolor='lightgreen', edgecolor='black'))\n",
    "ax.text(2, 6.6, \"STANDARD\", ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Medium\n",
    "ax.plot([5, 5], [8.2, 7.0], 'k-', linewidth=1)\n",
    "ax.text(5.3, 7.5, \"10K - 10M\", fontsize=9)\n",
    "ax.add_patch(plt.Rectangle((3.5, 6.2), 3, 0.8, fill=True, facecolor='lightyellow', edgecolor='black'))\n",
    "ax.text(5, 6.6, \"Memory Check\", ha='center', va='center', fontsize=10)\n",
    "\n",
    "# Large\n",
    "ax.plot([5.8, 8, 8], [8.2, 7.5, 7.0], 'k-', linewidth=1)\n",
    "ax.text(7.2, 7.7, \"> 10M\", fontsize=9)\n",
    "ax.add_patch(plt.Rectangle((6.5, 6.2), 3, 0.8, fill=True, facecolor='lightyellow', edgecolor='black'))\n",
    "ax.text(8, 6.6, \"Memory Check\", ha='center', va='center', fontsize=10)\n",
    "\n",
    "# Level 2 - Medium dataset branches\n",
    "ax.plot([4.2, 3, 3], [6.2, 5.5, 5.0], 'k-', linewidth=1)\n",
    "ax.text(3.3, 5.6, \"> 16GB\", fontsize=9)\n",
    "ax.add_patch(plt.Rectangle((1.5, 4.2), 3, 0.8, fill=True, facecolor='lightgreen', edgecolor='black'))\n",
    "ax.text(3, 4.6, \"STANDARD\", ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax.plot([5.8, 7, 7], [6.2, 5.5, 5.0], 'k-', linewidth=1)\n",
    "ax.text(6.5, 5.6, \"< 16GB\", fontsize=9)\n",
    "ax.add_patch(plt.Rectangle((5.5, 4.2), 3, 0.8, fill=True, facecolor='orange', edgecolor='black'))\n",
    "ax.text(7, 4.6, \"CHUNKED\", ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "# Level 2 - Large dataset branches\n",
    "ax.plot([7.2, 6, 6], [6.2, 5.5, 3.0], 'k-', linewidth=1)\n",
    "ax.text(6.3, 5.6, \"> 64GB\", fontsize=9)\n",
    "ax.add_patch(plt.Rectangle((4.5, 2.2), 3, 0.8, fill=True, facecolor='orange', edgecolor='black'))\n",
    "ax.text(6, 2.6, \"CHUNKED\", ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "ax.plot([8.8, 9.5, 9.5], [6.2, 5.5, 3.0], 'k-', linewidth=1)\n",
    "ax.text(9.2, 5.6, \"< 64GB\", fontsize=9)\n",
    "ax.add_patch(plt.Rectangle((8, 2.2), 1.8, 0.8, fill=True, facecolor='salmon', edgecolor='black'))\n",
    "ax.text(8.9, 2.6, \"STREAMING\", ha='center', va='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Additional note for massive datasets\n",
    "ax.add_patch(plt.Rectangle((0.5, 0.5), 9, 1.2, fill=True, facecolor='lightgray', edgecolor='black', alpha=0.3))\n",
    "ax.text(5, 1.1, \"For > 100M points: STREAMING_CHECKPOINT (adds fault tolerance)\", \n",
    "        ha='center', va='center', fontsize=10, style='italic')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/02_tier_decision_tree.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 4: Manual Tier Override\n",
    "\n",
    "You can override the automatic tier selection using `WorkflowConfig`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create configs with explicit tiers\n",
    "config_standard = WorkflowConfig(tier=WorkflowTier.STANDARD)\n",
    "config_chunked = WorkflowConfig(tier=WorkflowTier.CHUNKED)\n",
    "config_streaming = WorkflowConfig(tier=WorkflowTier.STREAMING)\n",
    "config_checkpoint = WorkflowConfig(tier=WorkflowTier.STREAMING_CHECKPOINT)\n",
    "\n",
    "print(\"Manual Tier Override Examples\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"config_standard.tier = {config_standard.tier}\")\n",
    "print(f\"config_chunked.tier = {config_chunked.tier}\")\n",
    "print(f\"config_streaming.tier = {config_streaming.tier}\")\n",
    "print(f\"config_checkpoint.tier = {config_checkpoint.tier}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a test model\n",
    "def exponential_decay(x, a, b, c):\n",
    "    \"\"\"Exponential decay: y = a * exp(-b * x) + c\"\"\"\n",
    "    return a * jnp.exp(-b * x) + c\n",
    "\n",
    "# Generate test data\n",
    "n_samples = 1000\n",
    "x_data = np.linspace(0, 5, n_samples)\n",
    "true_a, true_b, true_c = 3.0, 1.2, 0.5\n",
    "y_true = true_a * np.exp(-true_b * x_data) + true_c\n",
    "y_data = y_true + 0.1 * np.random.randn(n_samples)\n",
    "\n",
    "print(f\"Test dataset: {n_samples} points\")\n",
    "print(f\"True parameters: a={true_a}, b={true_b}, c={true_c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Force CHUNKED tier even for small dataset (demonstration)\n",
    "# In practice, auto-selection would use STANDARD for 1000 points\n",
    "\n",
    "print(\"\\nUsing STANDARD tier (auto-selected for small data):\")\n",
    "popt_standard, _ = fit(\n",
    "    exponential_decay,\n",
    "    x_data,\n",
    "    y_data,\n",
    "    p0=[1.0, 1.0, 0.0],\n",
    ")\n",
    "print(f\"  Result: a={popt_standard[0]:.4f}, b={popt_standard[1]:.4f}, c={popt_standard[2]:.4f}\")\n",
    "\n",
    "# Note: For small datasets, manually forcing CHUNKED or STREAMING \n",
    "# would require using curve_fit_large directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 5: Memory Usage Comparison\n",
    "\n",
    "Each tier has different memory characteristics. Let's visualize the theoretical memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory usage estimation for different tiers\n",
    "def estimate_memory_usage(n_points, n_params, tier):\n",
    "    \"\"\"Estimate memory usage in GB for a given tier.\"\"\"\n",
    "    bytes_per_point = 8 * (3 + n_params)  # x, y, residual + jacobian\n",
    "    \n",
    "    if tier == WorkflowTier.STANDARD:\n",
    "        # All data in memory\n",
    "        return n_points * bytes_per_point / 1e9\n",
    "    elif tier == WorkflowTier.CHUNKED:\n",
    "        # Chunk size typically 100K-1M\n",
    "        chunk_size = min(1_000_000, n_points)\n",
    "        return chunk_size * bytes_per_point / 1e9\n",
    "    elif tier in (WorkflowTier.STREAMING, WorkflowTier.STREAMING_CHECKPOINT):\n",
    "        # Batch size typically 50K\n",
    "        batch_size = 50_000\n",
    "        return batch_size * bytes_per_point / 1e9\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare memory usage across dataset sizes\n",
    "dataset_sizes = np.logspace(3, 9, 50)  # 1K to 1B points\n",
    "n_params = 5\n",
    "\n",
    "memory_standard = [estimate_memory_usage(int(n), n_params, WorkflowTier.STANDARD) for n in dataset_sizes]\n",
    "memory_chunked = [estimate_memory_usage(int(n), n_params, WorkflowTier.CHUNKED) for n in dataset_sizes]\n",
    "memory_streaming = [estimate_memory_usage(int(n), n_params, WorkflowTier.STREAMING) for n in dataset_sizes]\n",
    "\n",
    "# Plot memory comparison\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "ax.loglog(dataset_sizes, memory_standard, 'b-', linewidth=2, label='STANDARD')\n",
    "ax.loglog(dataset_sizes, memory_chunked, 'orange', linewidth=2, label='CHUNKED')\n",
    "ax.loglog(dataset_sizes, memory_streaming, 'r-', linewidth=2, label='STREAMING')\n",
    "\n",
    "# Add memory threshold lines\n",
    "ax.axhline(y=16, color='gray', linestyle='--', alpha=0.5, label='16 GB limit')\n",
    "ax.axhline(y=64, color='gray', linestyle=':', alpha=0.5, label='64 GB limit')\n",
    "\n",
    "# Add tier transition zones\n",
    "ax.axvline(x=10_000, color='green', linestyle='--', alpha=0.3)\n",
    "ax.axvline(x=10_000_000, color='orange', linestyle='--', alpha=0.3)\n",
    "ax.axvline(x=100_000_000, color='red', linestyle='--', alpha=0.3)\n",
    "\n",
    "ax.text(3000, 100, \"STANDARD\\nzone\", fontsize=9, ha='center')\n",
    "ax.text(300_000, 100, \"CHUNKED\\nzone\", fontsize=9, ha='center')\n",
    "ax.text(30_000_000, 100, \"STREAMING\\nzone\", fontsize=9, ha='center')\n",
    "\n",
    "ax.set_xlabel(\"Dataset Size (points)\")\n",
    "ax.set_ylabel(\"Peak Memory Usage (GB)\")\n",
    "ax.set_title(\"Memory Usage by Workflow Tier\")\n",
    "ax.legend(loc='upper left')\n",
    "ax.grid(True, alpha=0.3, which='both')\n",
    "ax.set_xlim(1e3, 1e9)\n",
    "ax.set_ylim(1e-3, 1e3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/02_memory_comparison.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Interpretation:\")\n",
    "print(\"  - STANDARD: Memory grows linearly with dataset size\")\n",
    "print(\"  - CHUNKED: Memory capped at chunk size (~1M points)\")\n",
    "print(\"  - STREAMING: Memory capped at batch size (~50K points)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "After completing this notebook, remember:\n",
    "\n",
    "1. **Four tiers available:** STANDARD, CHUNKED, STREAMING, STREAMING_CHECKPOINT\n",
    "\n",
    "2. **Automatic selection based on:**\n",
    "   - Dataset size (primary factor)\n",
    "   - Available memory (CPU + GPU)\n",
    "   - Optimization goal\n",
    "\n",
    "3. **Memory trade-offs:**\n",
    "   - STANDARD: O(N) memory, best precision\n",
    "   - CHUNKED: O(chunk_size) memory, good precision\n",
    "   - STREAMING: O(batch_size) memory, streaming convergence\n",
    "\n",
    "4. **Override when needed:**\n",
    "   ```python\n",
    "   config = WorkflowConfig(tier=WorkflowTier.STREAMING)\n",
    "   ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Common Questions\n",
    "\n",
    "**Q: When should I manually override the tier?**\n",
    "\n",
    "A: Override when you know your memory constraints better than the auto-detector. For example, if you're running alongside other processes that consume memory, force a lower-memory tier.\n",
    "\n",
    "**Q: Does CHUNKED give the same results as STANDARD?**\n",
    "\n",
    "A: Nearly identical. CHUNKED processes data in chunks and refines parameters progressively. For well-conditioned problems, results are typically within 0.1% of STANDARD.\n",
    "\n",
    "**Q: When is STREAMING_CHECKPOINT needed?**\n",
    "\n",
    "A: For multi-hour fits on massive datasets where fault tolerance is important. Checkpointing allows resuming from the last saved state if the job is interrupted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Related Resources\n",
    "\n",
    "**Next steps:**\n",
    "- [03_optimization_goals.ipynb](03_optimization_goals.ipynb) - FAST, ROBUST, QUALITY goals\n",
    "- [06_auto_selection.ipynb](06_auto_selection.ipynb) - Deep dive into WorkflowSelector\n",
    "\n",
    "**Further reading:**\n",
    "- [Large Dataset Guide](https://nlsq.readthedocs.io/large-datasets/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Glossary\n",
    "\n",
    "**Chunking:** Processing data in fixed-size portions to manage memory usage.\n",
    "\n",
    "**Streaming:** Processing data in mini-batches using gradient-based optimization.\n",
    "\n",
    "**Checkpointing:** Saving optimization state periodically to enable recovery from failures.\n",
    "\n",
    "**Memory Tier:** Classification of available system memory (LOW, MEDIUM, HIGH, VERY_HIGH)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"Summary\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(\"Workflow Tiers:\")\n",
    "print(\"  STANDARD:            < 10K points, full precision\")\n",
    "print(\"  CHUNKED:             10K - 10M points, memory-managed\")\n",
    "print(\"  STREAMING:           10M - 100M points, mini-batch\")\n",
    "print(\"  STREAMING_CHECKPOINT: > 100M points, fault-tolerant\")\n",
    "print()\n",
    "print(\"Override syntax:\")\n",
    "print(\"  config = WorkflowConfig(tier=WorkflowTier.CHUNKED)\")\n",
    "print()\n",
    "print(f\"Current system memory: {available_memory:.1f} GB ({current_tier.name})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
