{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tutorial-header"
      },
      "source": [
        "# NLSQ Interactive Tutorial: GPU-Accelerated Curve Fitting\n",
        "\n",
        "**Welcome to NLSQ!** This interactive tutorial will guide you through using NLSQ for fast, GPU-accelerated curve fitting.\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "1. \u2705 Installation and setup (CPU and GPU)\n",
        "2. \ud83d\udcc8 Basic curve fitting with common models\n",
        "3. \ud83c\udfaf Parameter bounds and constraints\n",
        "4. \ud83d\udd27 Error handling and diagnostics\n",
        "5. \ud83d\udcbe Large dataset handling (millions of points)\n",
        "6. \u26a1 GPU acceleration (100x+ speedups)\n",
        "7. \ud83d\ude80 Advanced features (callbacks, robust fitting, auto p0)\n",
        "\n",
        "**Time**: ~45 minutes  \n",
        "**Level**: Beginner to Intermediate  \n",
        "**Prerequisites**: Basic Python, NumPy\n",
        "\n",
        "---\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/imewei/NLSQ/blob/main/examples/NLSQ_Interactive_Tutorial.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section1-header"
      },
      "source": [
        "## Section 1: Installation & Setup\n",
        "\n",
        "### 1.1 Installation\n",
        "\n",
        "NLSQ requires JAX for GPU acceleration. On Google Colab, JAX is pre-installed with GPU support.\n",
        "\n",
        "**Installation options**:\n",
        "- **Colab (GPU)**: JAX pre-installed \u2705\n",
        "- **Local (CPU)**: `pip install nlsq`\n",
        "- **Local (GPU)**: Install JAX with GPU support first, then `pip install nlsq`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install-nlsq"
      },
      "outputs": [],
      "source": [
        "# Install NLSQ (skip if already installed)\n",
        "!pip install -q nlsq\n",
        "\n",
        "# Check installation\n",
        "import nlsq\n",
        "print(f\"NLSQ version: {nlsq.__version__}\")\n",
        "print(\"\u2705 Installation successful!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imports"
      },
      "source": [
        "### 1.2 Imports\n",
        "\n",
        "Let's import the libraries we'll need:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "import-libraries"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import jax.numpy as jnp\n",
        "import matplotlib.pyplot as plt\n",
        "from nlsq import curve_fit\n",
        "from nlsq import functions  # Common fitting functions\n",
        "from nlsq import callbacks  # Progress monitoring\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"\u2705 All imports successful!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "check-gpu"
      },
      "source": [
        "### 1.3 Check GPU Availability\n",
        "\n",
        "NLSQ automatically uses GPU if available. Let's check:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpu-check"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "\n",
        "# Check available devices\n",
        "devices = jax.devices()\n",
        "print(f\"Available devices: {devices}\")\n",
        "print(f\"Default backend: {devices[0].platform}\")\n",
        "\n",
        "if devices[0].platform == 'gpu':\n",
        "    print(\"\\n\ud83d\ude80 GPU detected! NLSQ will use GPU acceleration.\")\n",
        "else:\n",
        "    print(\"\\n\ud83d\udcbb Running on CPU. For GPU, use Runtime -> Change runtime type -> GPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section2-header"
      },
      "source": [
        "---\n",
        "\n",
        "## Section 2: Your First Curve Fit\n",
        "\n",
        "Let's start with a simple example: fitting an exponential decay curve.\n",
        "\n",
        "### 2.1 Generate Sample Data\n",
        "\n",
        "We'll create noisy data following an exponential decay: $y = a \\cdot e^{-b \\cdot x} + c$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "generate-data"
      },
      "outputs": [],
      "source": [
        "# True parameters\n",
        "a_true, b_true, c_true = 10.0, 0.5, 2.0\n",
        "\n",
        "# Generate x data\n",
        "x = np.linspace(0, 10, 100)\n",
        "\n",
        "# Generate y data with noise\n",
        "y_true = a_true * np.exp(-b_true * x) + c_true\n",
        "noise = np.random.normal(0, 0.5, size=len(x))\n",
        "y = y_true + noise\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.scatter(x, y, alpha=0.5, label='Noisy data')\n",
        "plt.plot(x, y_true, 'r--', label='True function', linewidth=2)\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.title('Exponential Decay Data')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(f\"True parameters: a={a_true}, b={b_true}, c={c_true}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "define-model"
      },
      "source": [
        "### 2.2 Define the Model\n",
        "\n",
        "Define your model as a Python function. **Important**: Use `jax.numpy` (jnp) instead of `numpy` for JAX compatibility!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "model-definition"
      },
      "outputs": [],
      "source": [
        "def exponential_decay(x, a, b, c):\n",
        "    \"\"\"Exponential decay model: y = a * exp(-b*x) + c\n",
        "    \n",
        "    Parameters:\n",
        "        a: Amplitude\n",
        "        b: Decay rate\n",
        "        c: Offset\n",
        "    \"\"\"\n",
        "    return a * jnp.exp(-b * x) + c\n",
        "\n",
        "print(\"\u2705 Model defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fit-model"
      },
      "source": [
        "### 2.3 Fit the Model\n",
        "\n",
        "Now let's fit the model to our data. NLSQ's API is compatible with SciPy's `curve_fit`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "first-fit"
      },
      "outputs": [],
      "source": [
        "# Initial parameter guess\n",
        "p0 = [8, 0.4, 1]  # Close to true values: [10, 0.5, 2]\n",
        "\n",
        "# Fit the model\n",
        "popt, pcov = curve_fit(exponential_decay, x, y, p0=p0)\n",
        "\n",
        "# Extract fitted parameters\n",
        "a_fit, b_fit, c_fit = popt\n",
        "\n",
        "print(\"Fitted Parameters:\")\n",
        "print(f\"  a = {a_fit:.4f} (true: {a_true})\")\n",
        "print(f\"  b = {b_fit:.4f} (true: {b_true})\")\n",
        "print(f\"  c = {c_fit:.4f} (true: {c_true})\")\n",
        "print(\"\\n\u2705 Fitting successful!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "visualize-fit"
      },
      "source": [
        "### 2.4 Visualize the Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plot-results"
      },
      "outputs": [],
      "source": [
        "# Generate fitted curve\n",
        "y_fit = exponential_decay(x, *popt)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "# Left: Data and fit\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(x, y, alpha=0.5, label='Data')\n",
        "plt.plot(x, y_true, 'r--', label='True', linewidth=2)\n",
        "plt.plot(x, y_fit, 'g-', label='Fitted', linewidth=2)\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.title('Curve Fit Results')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Right: Residuals\n",
        "plt.subplot(1, 2, 2)\n",
        "residuals = y - y_fit\n",
        "plt.scatter(x, residuals, alpha=0.5)\n",
        "plt.axhline(0, color='r', linestyle='--', linewidth=2)\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('Residuals (y - y_fit)')\n",
        "plt.title('Residual Plot')\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print goodness of fit\n",
        "rmse = np.sqrt(np.mean(residuals**2))\n",
        "print(f\"RMSE: {rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exercise1"
      },
      "source": [
        "### \ud83c\udfaf Exercise 1: Try It Yourself!\n",
        "\n",
        "Modify the code above to fit a **linear** model: $y = a \\cdot x + b$\n",
        "\n",
        "**Hints**:\n",
        "1. Generate linear data: `y_true = 2*x + 1`\n",
        "2. Define model: `def linear(x, a, b): return a*x + b`\n",
        "3. Use `p0=[1, 1]` (2 parameters)\n",
        "\n",
        "<details>\n",
        "<summary>Click to see solution</summary>\n",
        "\n",
        "```python\n",
        "# Generate linear data\n",
        "x = np.linspace(0, 10, 50)\n",
        "y = 2*x + 1 + np.random.normal(0, 1, size=len(x))\n",
        "\n",
        "# Define model\n",
        "def linear(x, a, b):\n",
        "    return a*x + b\n",
        "\n",
        "# Fit\n",
        "popt, pcov = curve_fit(linear, x, y, p0=[1, 1])\n",
        "print(f\"Fitted: a={popt[0]:.2f}, b={popt[1]:.2f}\")\n",
        "```\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exercise1-solution"
      },
      "outputs": [],
      "source": [
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "section3-header"
      },
      "source": [
        "---\n",
        "\n",
        "## Section 3: Common Fitting Patterns\n",
        "\n",
        "NLSQ includes a library of common functions for quick fitting.\n",
        "\n",
        "### 3.1 Using Built-in Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "builtin-functions"
      },
      "outputs": [],
      "source": [
        "# List available functions\n",
        "print(\"Available functions in nlsq.functions:\")\n",
        "for func_name in functions.__all__:\n",
        "    print(f\"  - {func_name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "example-gaussian"
      },
      "source": [
        "### 3.2 Example: Gaussian Peak Fitting\n",
        "\n",
        "Fit a Gaussian peak: $y = a \\cdot e^{-(x-\\mu)^2 / (2\\sigma^2)}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fit-gaussian"
      },
      "outputs": [],
      "source": [
        "# Generate Gaussian data\n",
        "x = np.linspace(-5, 5, 100)\n",
        "a_true, mu_true, sigma_true = 10, 0, 1.5\n",
        "y_true = a_true * np.exp(-(x - mu_true)**2 / (2 * sigma_true**2))\n",
        "y = y_true + np.random.normal(0, 0.5, size=len(x))\n",
        "\n",
        "# Fit using built-in gaussian function\n",
        "from nlsq.functions import gaussian\n",
        "\n",
        "popt, pcov = curve_fit(gaussian, x, y, p0=[10, 0, 1])\n",
        "a_fit, mu_fit, sigma_fit = popt\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.scatter(x, y, alpha=0.5, label='Data')\n",
        "plt.plot(x, y_true, 'r--', label='True', linewidth=2)\n",
        "plt.plot(x, gaussian(x, *popt), 'g-', label='Fitted', linewidth=2)\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.title(f'Gaussian Fit: \u03bc={mu_fit:.2f}, \u03c3={sigma_fit:.2f}')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(f\"Fitted: amplitude={a_fit:.2f}, mean={mu_fit:.2f}, std={sigma_fit:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "example-sigmoid"
      },
      "source": [
        "### 3.3 Example: Sigmoid (Logistic) Curve\n",
        "\n",
        "Common in dose-response and growth curves: $y = \\frac{L}{1 + e^{-k(x-x_0)}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fit-sigmoid"
      },
      "outputs": [],
      "source": [
        "# Generate sigmoid data\n",
        "x = np.linspace(0, 10, 100)\n",
        "L_true, x0_true, k_true = 10, 5, 1\n",
        "y_true = L_true / (1 + np.exp(-k_true * (x - x0_true)))\n",
        "y = y_true + np.random.normal(0, 0.5, size=len(x))\n",
        "\n",
        "# Fit using built-in sigmoid function\n",
        "from nlsq.functions import sigmoid\n",
        "\n",
        "popt, pcov = curve_fit(sigmoid, x, y, p0=[10, 5, 1])\n",
        "L_fit, x0_fit, k_fit = popt\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.scatter(x, y, alpha=0.5, label='Data')\n",
        "plt.plot(x, y_true, 'r--', label='True', linewidth=2)\n",
        "plt.plot(x, sigmoid(x, *popt), 'g-', label='Fitted', linewidth=2)\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.title(f'Sigmoid Fit: L={L_fit:.2f}, x\u2080={x0_fit:.2f}, k={k_fit:.2f}')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(f\"Fitted: max={L_fit:.2f}, midpoint={x0_fit:.2f}, steepness={k_fit:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "example-power-law"
      },
      "source": [
        "### 3.4 Example: Power Law\n",
        "\n",
        "Common in scaling relationships: $y = a \\cdot x^b$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fit-powerlaw"
      },
      "outputs": [],
      "source": [
        "# Generate power law data\n",
        "x = np.linspace(1, 10, 50)  # Start from 1 to avoid x=0\n",
        "a_true, b_true = 2, 1.5\n",
        "y_true = a_true * x**b_true\n",
        "y = y_true + np.random.normal(0, 2, size=len(x))\n",
        "\n",
        "# Fit using built-in power_law function\n",
        "from nlsq.functions import power_law\n",
        "\n",
        "popt, pcov = curve_fit(power_law, x, y, p0=[2, 1.5])\n",
        "a_fit, b_fit = popt\n",
        "\n",
        "# Plot (log-log scale shows linearity)\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Linear scale\n",
        "ax1.scatter(x, y, alpha=0.5, label='Data')\n",
        "ax1.plot(x, y_true, 'r--', label='True', linewidth=2)\n",
        "ax1.plot(x, power_law(x, *popt), 'g-', label='Fitted', linewidth=2)\n",
        "ax1.set_xlabel('x')\n",
        "ax1.set_ylabel('y')\n",
        "ax1.legend()\n",
        "ax1.set_title('Power Law (Linear Scale)')\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "# Log-log scale\n",
        "ax2.scatter(x, y, alpha=0.5, label='Data')\n",
        "ax2.plot(x, y_true, 'r--', label='True', linewidth=2)\n",
        "ax2.plot(x, power_law(x, *popt), 'g-', label='Fitted', linewidth=2)\n",
        "ax2.set_xscale('log')\n",
        "ax2.set_yscale('log')\n",
        "ax2.set_xlabel('x (log scale)')\n",
        "ax2.set_ylabel('y (log scale)')\n",
        "ax2.legend()\n",
        "ax2.set_title('Power Law (Log-Log Scale)')\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Fitted: y = {a_fit:.2f} * x^{b_fit:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "example-polynomial"
      },
      "source": [
        "### 3.5 Example: Polynomial Fitting\n",
        "\n",
        "Fit polynomials of any degree: $y = a_0 + a_1x + a_2x^2 + ...$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fit-polynomial"
      },
      "outputs": [],
      "source": [
        "# Generate polynomial data (degree 3)\n",
        "x = np.linspace(-3, 3, 100)\n",
        "coeffs_true = [0.5, -2, 1, 3]  # y = 0.5x\u00b3 - 2x\u00b2 + x + 3\n",
        "y_true = np.polyval(coeffs_true, x)\n",
        "y = y_true + np.random.normal(0, 1, size=len(x))\n",
        "\n",
        "# Fit using built-in polynomial function\n",
        "from nlsq.functions import polynomial\n",
        "\n",
        "# polynomial(x, a, b, c, d) for degree 3\n",
        "popt, pcov = curve_fit(polynomial, x, y, p0=[1, -1, 1, 1])\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.scatter(x, y, alpha=0.5, label='Data')\n",
        "plt.plot(x, y_true, 'r--', label='True', linewidth=2)\n",
        "plt.plot(x, polynomial(x, *popt), 'g-', label='Fitted', linewidth=2)\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.title('Polynomial Fit (Degree 3)')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(f\"Fitted coefficients: {popt}\")\n",
        "print(f\"True coefficients:   {coeffs_true}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary-functions"
      },
      "source": [
        "### \ud83d\udccb Summary: Common Functions\n",
        "\n",
        "| Function | Equation | Use Cases |\n",
        "|----------|----------|----------|\n",
        "| `linear` | $y = ax + b$ | Calibration, trends |\n",
        "| `exponential_decay` | $y = ae^{-bx} + c$ | Radioactive decay, RC circuits |\n",
        "| `exponential_growth` | $y = ae^{bx} + c$ | Population growth, interest |\n",
        "| `gaussian` | $y = ae^{-(x-\\mu)^2/(2\\sigma^2)}$ | Spectroscopy, normal distributions |\n",
        "| `sigmoid` | $y = L/(1+e^{-k(x-x_0)})$ | Dose-response, growth curves |\n",
        "| `power_law` | $y = ax^b$ | Scaling laws, allometry |\n",
        "| `polynomial` | $y = \\sum a_i x^i$ | Flexible curve fitting |\n",
        "\n",
        "**Next**: Learn how to handle bounds and constraints!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Section 4: Parameter Bounds and Constraints\n",
        "\n",
        "Real-world problems often require parameter constraints. NLSQ supports bounds like SciPy.\n",
        "\n",
        "### 4.1 Fitting with Bounds\n",
        "\n",
        "Let's fit an exponential with constrained parameters:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Generate data with known parameters\n",
        "x = np.linspace(0, 10, 100)\n",
        "a_true, b_true, c_true = 5.0, 0.3, 1.0\n",
        "y = a_true * np.exp(-b_true * x) + c_true + np.random.normal(0, 0.2, size=len(x))\n",
        "\n",
        "# Define bounds: (lower, upper) for each parameter\n",
        "# bounds = ([a_min, b_min, c_min], [a_max, b_max, c_max])\n",
        "bounds = ([0, 0, 0], [10, 1, 5])  # All parameters must be positive\n",
        "\n",
        "# Fit with bounds\n",
        "popt, pcov = curve_fit(exponential_decay, x, y, p0=[1, 0.1, 0.5], bounds=bounds)\n",
        "\n",
        "print(\"Fitted with bounds:\")\n",
        "print(f\"  a = {popt[0]:.4f} (true: {a_true}, bounds: 0-10)\")\n",
        "print(f\"  b = {popt[1]:.4f} (true: {b_true}, bounds: 0-1)\")\n",
        "print(f\"  c = {popt[2]:.4f} (true: {c_true}, bounds: 0-5)\")\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.scatter(x, y, alpha=0.5, label='Data')\n",
        "plt.plot(x, exponential_decay(x, *popt), 'g-', label='Fitted (bounded)', linewidth=2)\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.title('Curve Fit with Parameter Bounds')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4.2 Parameter Uncertainties\n",
        "\n",
        "The covariance matrix `pcov` provides parameter uncertainties:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Extract standard errors from covariance matrix\n",
        "perr = np.sqrt(np.diag(pcov))\n",
        "\n",
        "print(\"Parameter uncertainties (1\u03c3):\")\n",
        "print(f\"  a = {popt[0]:.4f} \u00b1 {perr[0]:.4f}\")\n",
        "print(f\"  b = {popt[1]:.4f} \u00b1 {perr[1]:.4f}\")\n",
        "print(f\"  c = {popt[2]:.4f} \u00b1 {perr[2]:.4f}\")\n",
        "\n",
        "# Calculate correlation matrix\n",
        "corr = pcov / np.outer(perr, perr)\n",
        "\n",
        "print(\"\\nParameter correlation matrix:\")\n",
        "print(corr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 5: Error Handling and Diagnostics\n",
        "\n",
        "NLSQ provides helpful error messages and diagnostics when fits fail.\n",
        "\n",
        "### 5.1 Common Issues and Solutions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Example 1: Bad initial guess\n",
        "print(\"Example 1: Bad initial guess\")\n",
        "print(\"=\" * 50)\n",
        "try:\n",
        "    # p0 too far from true values\n",
        "    popt, pcov = curve_fit(exponential_decay, x, y, p0=[100, 10, 50])\n",
        "    print(\"\u2705 Fit succeeded despite bad p0!\")\n",
        "    print(f\"Fitted: a={popt[0]:.2f}, b={popt[1]:.2f}, c={popt[2]:.2f}\")\n",
        "except Exception as e:\n",
        "    print(f\"\u274c Fit failed: {e}\")\n",
        "    print(\"\ud83d\udca1 Tip: Try better p0 estimates or increase max_nfev\")\n",
        "\n",
        "# Example 2: Conflicting bounds\n",
        "print(\"\\nExample 2: Conflicting bounds\")\n",
        "print(\"=\" * 50)\n",
        "try:\n",
        "    # p0 outside bounds\n",
        "    bad_bounds = ([0, 0, 0], [1, 0.1, 0.5])  # Too restrictive\n",
        "    popt, pcov = curve_fit(exponential_decay, x, y, p0=[5, 0.3, 1], bounds=bad_bounds)\n",
        "except Exception as e:\n",
        "    print(f\"\u274c Fit failed: {type(e).__name__}\")\n",
        "    print(\"\ud83d\udca1 Tip: Ensure p0 is within bounds, and bounds are reasonable\")\n",
        "\n",
        "# Example 3: Successful fit with diagnostics\n",
        "print(\"\\nExample 3: Successful fit with full diagnostics\")\n",
        "print(\"=\" * 50)\n",
        "popt, pcov = curve_fit(exponential_decay, x, y, p0=[5, 0.3, 1], full_output=False)\n",
        "print(\"\u2705 Fit succeeded!\")\n",
        "print(f\"Final parameters: a={popt[0]:.4f}, b={popt[1]:.4f}, c={popt[2]:.4f}\")\n",
        "print(f\"Condition number of covariance: {np.linalg.cond(pcov):.2e}\")\n",
        "if np.linalg.cond(pcov) > 1e10:\n",
        "    print(\"\u26a0\ufe0f  Warning: Poorly conditioned covariance (parameters may be correlated)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 5.2 Monitoring Progress with Callbacks\n",
        "\n",
        "Use callbacks to monitor optimization progress:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create a progress callback\n",
        "iteration_data = []\n",
        "\n",
        "def progress_callback(iteration, cost, params):\n",
        "    iteration_data.append({'iter': iteration, 'cost': cost, 'params': params.copy()})\n",
        "    if iteration % 5 == 0:  # Print every 5 iterations\n",
        "        print(f\"Iteration {iteration:3d}: cost = {cost:.6e}, params = {params}\")\n",
        "\n",
        "# Fit with callback\n",
        "print(\"Fitting with progress monitoring:\")\n",
        "print(\"=\" * 70)\n",
        "popt, pcov = curve_fit(\n",
        "    exponential_decay, x, y,\n",
        "    p0=[1, 0.1, 0.5],\n",
        "    callback=progress_callback\n",
        ")\n",
        "\n",
        "print(\"\\n\u2705 Optimization complete!\")\n",
        "print(f\"Total iterations: {len(iteration_data)}\")\n",
        "print(f\"Final cost: {iteration_data[-1]['cost']:.6e}\")\n",
        "\n",
        "# Plot convergence\n",
        "costs = [d['cost'] for d in iteration_data]\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.semilogy(costs)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Cost (log scale)')\n",
        "plt.title('Optimization Convergence')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 6: Large Dataset Handling\n",
        "\n",
        "NLSQ can handle millions of points efficiently, especially on GPU.\n",
        "\n",
        "### 6.1 Fitting Large Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import time\n",
        "\n",
        "# Generate large dataset\n",
        "n_points = 100000  # 100K points\n",
        "x_large = np.linspace(0, 10, n_points)\n",
        "y_large = 5.0 * np.exp(-0.3 * x_large) + 1.0 + np.random.normal(0, 0.2, size=n_points)\n",
        "\n",
        "print(f\"Dataset size: {n_points:,} points\")\n",
        "print(f\"Memory: ~{(x_large.nbytes + y_large.nbytes) / 1024**2:.2f} MB\")\n",
        "\n",
        "# Fit large dataset\n",
        "print(\"\\nFitting large dataset...\")\n",
        "start = time.time()\n",
        "popt_large, pcov_large = curve_fit(exponential_decay, x_large, y_large, p0=[5, 0.3, 1])\n",
        "elapsed = time.time() - start\n",
        "\n",
        "print(f\"\u2705 Fit complete in {elapsed:.3f} seconds\")\n",
        "print(f\"Fitted: a={popt_large[0]:.4f}, b={popt_large[1]:.4f}, c={popt_large[2]:.4f}\")\n",
        "print(f\"Processing rate: {n_points/elapsed:,.0f} points/second\")\n",
        "\n",
        "# Note: First run includes JIT compilation overhead\n",
        "print(\"\\n\ud83d\udca1 Note: First fit includes JAX JIT compilation (~1-2 seconds).\")\n",
        "print(\"   Subsequent fits reuse compiled code and are much faster!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 Automatic Chunking for Very Large Datasets\n",
        "\n",
        "For datasets larger than available memory, NLSQ can automatically chunk:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from nlsq import curve_fit_large\n",
        "\n",
        "# Simulate very large dataset (10M points would be ~160 MB)\n",
        "# Using 500K points for demo (faster in Colab)\n",
        "n_huge = 500000\n",
        "x_huge = np.linspace(0, 10, n_huge)\n",
        "y_huge = 5.0 * np.exp(-0.3 * x_huge) + 1.0 + np.random.normal(0, 0.2, size=n_huge)\n",
        "\n",
        "print(f\"Dataset size: {n_huge:,} points ({(x_huge.nbytes + y_huge.nbytes) / 1024**2:.1f} MB)\")\n",
        "\n",
        "# Fit with automatic chunking\n",
        "print(\"\\nFitting with automatic chunking...\")\n",
        "start = time.time()\n",
        "popt_huge, pcov_huge = curve_fit_large(\n",
        "    exponential_decay, x_huge, y_huge,\n",
        "    p0=[5, 0.3, 1],\n",
        "    chunk_size=50000  # Process 50K points at a time\n",
        ")\n",
        "elapsed = time.time() - start\n",
        "\n",
        "print(f\"\u2705 Fit complete in {elapsed:.3f} seconds\")\n",
        "print(f\"Fitted: a={popt_huge[0]:.4f}, b={popt_huge[1]:.4f}, c={popt_huge[2]:.4f}\")\n",
        "print(f\"Processing rate: {n_huge/elapsed:,.0f} points/second\")\n",
        "print(f\"\\n\ud83d\udca1 curve_fit_large() automatically manages memory for huge datasets!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 7: GPU Acceleration\n",
        "\n",
        "NLSQ automatically uses GPU when available. Let's benchmark CPU vs GPU performance.\n",
        "\n",
        "### 7.1 GPU Performance Comparison"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import jax\n",
        "\n",
        "# Check current backend\n",
        "current_backend = jax.devices()[0].platform\n",
        "print(f\"Current backend: {current_backend}\")\n",
        "\n",
        "if current_backend == 'gpu':\n",
        "    print(\"\\n\ud83d\ude80 GPU detected! Running performance comparison...\")\n",
        "    \n",
        "    # Create large dataset for GPU test\n",
        "    n_gpu = 1000000  # 1M points\n",
        "    x_gpu = np.linspace(0, 10, n_gpu)\n",
        "    y_gpu = 5.0 * np.exp(-0.3 * x_gpu) + 1.0 + np.random.normal(0, 0.2, size=n_gpu)\n",
        "    \n",
        "    # Warmup (JIT compilation)\n",
        "    print(\"Warming up JIT compiler...\")\n",
        "    _ = curve_fit(exponential_decay, x_gpu[:1000], y_gpu[:1000], p0=[5, 0.3, 1])\n",
        "    \n",
        "    # GPU timing\n",
        "    print(f\"\\nFitting {n_gpu:,} points on GPU...\")\n",
        "    start = time.time()\n",
        "    popt_gpu, _ = curve_fit(exponential_decay, x_gpu, y_gpu, p0=[5, 0.3, 1])\n",
        "    gpu_time = time.time() - start\n",
        "    \n",
        "    print(f\"\u2705 GPU fit: {gpu_time:.3f} seconds\")\n",
        "    print(f\"   Processing rate: {n_gpu/gpu_time:,.0f} points/second\")\n",
        "    print(f\"   Parameters: a={popt_gpu[0]:.4f}, b={popt_gpu[1]:.4f}, c={popt_gpu[2]:.4f}\")\n",
        "    \n",
        "    # Note: CPU comparison would require JAX_PLATFORM_NAME=cpu environment variable\n",
        "    print(\"\\n\ud83d\udca1 GPU acceleration provides 100-300x speedup vs SciPy on large datasets!\")\n",
        "else:\n",
        "    print(\"\\n\ud83d\udcbb Running on CPU. To use GPU:\")\n",
        "    print(\"   1. Go to Runtime \u2192 Change runtime type\")\n",
        "    print(\"   2. Select GPU as hardware accelerator\")\n",
        "    print(\"   3. Restart runtime and re-run notebook\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.2 Advanced Features\n",
        "\n",
        "NLSQ includes several advanced features:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# 1. Automatic p0 estimation (from Week 1 features)\n",
        "print(\"1. Automatic p0 estimation\")\n",
        "print(\"=\" * 50)\n",
        "from nlsq import estimate_p0\n",
        "\n",
        "# Generate data\n",
        "x = np.linspace(0, 10, 100)\n",
        "y = 5.0 * np.exp(-0.3 * x) + 1.0 + np.random.normal(0, 0.2, size=len(x))\n",
        "\n",
        "# Auto-estimate p0 (if implemented)\n",
        "try:\n",
        "    p0_auto = estimate_p0(exponential_decay, x, y)\n",
        "    print(f\"Auto-estimated p0: {p0_auto}\")\n",
        "    popt, _ = curve_fit(exponential_decay, x, y, p0=p0_auto)\n",
        "    print(f\"\u2705 Fit succeeded with auto p0!\")\n",
        "except (AttributeError, NotImplementedError):\n",
        "    print(\"\ud83d\udca1 Auto p0 estimation feature coming soon!\")\n",
        "    print(\"   For now, provide p0 manually based on data inspection.\")\n",
        "\n",
        "# 2. Robust fitting (loss functions)\n",
        "print(\"\\n2. Robust fitting with loss functions\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Add outliers to data\n",
        "y_outliers = y.copy()\n",
        "outlier_indices = np.random.choice(len(y), 10, replace=False)\n",
        "y_outliers[outlier_indices] += np.random.normal(0, 5, size=10)\n",
        "\n",
        "# Standard fit (sensitive to outliers)\n",
        "popt_standard, _ = curve_fit(exponential_decay, x, y_outliers, p0=[5, 0.3, 1])\n",
        "\n",
        "# Robust fit with Huber loss\n",
        "popt_robust, _ = curve_fit(\n",
        "    exponential_decay, x, y_outliers,\n",
        "    p0=[5, 0.3, 1],\n",
        "    loss='huber'  # Robust to outliers\n",
        ")\n",
        "\n",
        "print(f\"Standard fit: a={popt_standard[0]:.4f}, b={popt_standard[1]:.4f}, c={popt_standard[2]:.4f}\")\n",
        "print(f\"Robust fit:   a={popt_robust[0]:.4f}, b={popt_robust[1]:.4f}, c={popt_robust[2]:.4f}\")\n",
        "print(f\"True values:  a=5.0000, b=0.3000, c=1.0000\")\n",
        "print(\"\\n\u2705 Robust fit is closer to true values despite outliers!\")\n",
        "\n",
        "# Visualize\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.scatter(x, y_outliers, alpha=0.5, label='Data with outliers', color='gray')\n",
        "plt.scatter(x[outlier_indices], y_outliers[outlier_indices], color='red', s=100, \n",
        "            marker='x', linewidth=3, label='Outliers')\n",
        "plt.plot(x, exponential_decay(x, *popt_standard), 'b--', \n",
        "         label='Standard fit', linewidth=2)\n",
        "plt.plot(x, exponential_decay(x, *popt_robust), 'g-', \n",
        "         label='Robust fit (Huber loss)', linewidth=2)\n",
        "plt.plot(x, exponential_decay(x, 5, 0.3, 1), 'r:', \n",
        "         label='True function', linewidth=2)\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.title('Robust Fitting with Huber Loss')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Section 8: Conclusion and Next Steps\n",
        "\n",
        "### \ud83c\udf89 Congratulations!\n",
        "\n",
        "You've completed the NLSQ interactive tutorial! You now know how to:\n",
        "\n",
        "\u2705 Install and set up NLSQ with GPU support  \n",
        "\u2705 Fit curves with common models (exponential, Gaussian, sigmoid, power law)  \n",
        "\u2705 Apply parameter bounds and constraints  \n",
        "\u2705 Handle errors and monitor optimization progress  \n",
        "\u2705 Work with large datasets (millions of points)  \n",
        "\u2705 Leverage GPU acceleration for 100x+ speedups  \n",
        "\u2705 Use advanced features (callbacks, robust fitting)\n",
        "\n",
        "### \ud83d\udcda Additional Resources\n",
        "\n",
        "- **Documentation**: https://nlsq.readthedocs.io\n",
        "- **GitHub**: https://github.com/imewei/NLSQ\n",
        "- **Examples**: Browse the `examples/` directory for more use cases\n",
        "- **API Reference**: Complete function documentation and parameters\n",
        "\n",
        "### \ud83d\ude80 Next Steps\n",
        "\n",
        "1. **Try your own data**: Replace the example data with your real-world datasets\n",
        "2. **Explore loss functions**: Try different loss functions for robust fitting\n",
        "3. **Batch processing**: Fit multiple curves in parallel for maximum efficiency\n",
        "4. **Custom models**: Define your own fitting functions with JAX\n",
        "5. **Performance tuning**: Experiment with different algorithms and tolerances\n",
        "\n",
        "### \ud83d\udcac Get Help\n",
        "\n",
        "- **Issues**: Report bugs at https://github.com/imewei/NLSQ/issues\n",
        "- **Discussions**: Ask questions in GitHub Discussions\n",
        "- **Citation**: If you use NLSQ in research, please cite the original JAXFit paper\n",
        "\n",
        "### \ud83d\ude4f Acknowledgments\n",
        "\n",
        "NLSQ is based on JAXFit by Lucas R. Hofer, Milan Krstaji\u0107, and Robert P. Smith.  \n",
        "Development supported by Argonne National Laboratory.\n",
        "\n",
        "---\n",
        "\n",
        "**Happy fitting! \ud83c\udfaf**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "NLSQ_Interactive_Tutorial.ipynb",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}