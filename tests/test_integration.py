"""
Comprehensive integration tests for NLSQ large dataset functionality.

Tests JAX tracing compatibility, chunking accuracy, and end-to-end validation.
"""

import unittest
import warnings
from unittest.mock import patch

import jax
import jax.numpy as jnp
import numpy as np
from scipy import optimize as scipy_optimize

from nlsq import (
    CurveFit,
    LargeDatasetFitter,
    curve_fit,
    curve_fit_large,
    estimate_memory_requirements,
)
from nlsq.large_dataset import LDMemoryConfig


class TestJAXTracingCompatibility(unittest.TestCase):
    """Test JAX tracing compatibility for various function signatures."""

    def test_1d_function_1_param(self):
        """Test 1D function with 1 parameter."""

        def linear(x, a):
            return a * x

        x = np.linspace(0, 10, 100)
        y = 2.5 * x + np.random.normal(0, 0.1, 100)

        # Should work without JAX tracing errors
        popt, pcov = curve_fit(linear, x, y, p0=[1.0])
        self.assertAlmostEqual(popt[0], 2.5, places=1)

    def test_1d_function_5_params(self):
        """Test 1D function with 5 parameters."""

        def poly4(x, a0, a1, a2, a3, a4):
            return a0 + a1 * x + a2 * x**2 + a3 * x**3 + a4 * x**4

        x = np.linspace(-1, 1, 100)
        true_params = [1.0, -0.5, 0.3, -0.1, 0.05]
        y = poly4(x, *true_params) + np.random.normal(0, 0.01, 100)

        popt, pcov = curve_fit(poly4, x, y, p0=[1, 1, 1, 1, 1])
        # Polynomial fitting can be numerically challenging, especially for higher order terms
        # Check lower order terms more strictly
        np.testing.assert_allclose(popt[:3], true_params[:3], rtol=0.1)
        # Allow more tolerance for higher order terms which are more sensitive to noise
        np.testing.assert_allclose(popt[3:], true_params[3:], rtol=0.3)

    def test_1d_function_10_params(self):
        """Test 1D function with 10 parameters."""

        def poly9(x, p0, p1, p2, p3, p4, p5, p6, p7, p8, p9):
            return sum(
                p * x**i for i, p in enumerate([p0, p1, p2, p3, p4, p5, p6, p7, p8, p9])
            )

        x = np.linspace(-1, 1, 200)
        true_params = [1.0, -0.5, 0.3, -0.1, 0.05, -0.02, 0.01, -0.005, 0.002, -0.001]
        y = poly9(x, *true_params) + np.random.normal(0, 0.01, 200)

        popt, pcov = curve_fit(poly9, x, y, p0=[1] * 10)
        # High order polynomials are harder to fit, so we allow more tolerance
        # Only check the first 3 coefficients as higher order ones are increasingly unstable
        np.testing.assert_allclose(popt[:3], true_params[:3], rtol=0.3)

    def test_1d_function_15_params(self):
        """Test 1D function with 15 parameters (edge case)."""

        def many_params(x, *params):
            # Simple sum of exponentials with different decay rates
            result = jnp.zeros_like(x)
            for i, p in enumerate(params):
                result = result + p * jnp.exp(-i * 0.1 * x)
            return result

        x = np.linspace(0, 5, 300)
        true_params = np.random.uniform(0.5, 2.0, 15)
        y = many_params(x, *true_params) + np.random.normal(0, 0.01, 300)

        # Should handle this with warning but no error
        with warnings.catch_warnings():
            warnings.simplefilter("ignore")
            popt, pcov = curve_fit(many_params, x, y, p0=np.ones(15))

        # Check that we got reasonable parameters (not exact due to complexity)
        self.assertEqual(len(popt), 15)
        self.assertTrue(np.all(np.isfinite(popt)))

    def test_jit_compilation(self):
        """Test that functions compile properly with JIT."""

        @jax.jit
        def exponential(x, a, b, c):
            return a * jnp.exp(-b * x) + c

        x = np.linspace(0, 5, 100)
        y = 2.5 * np.exp(-1.3 * x) + 0.1 + np.random.normal(0, 0.01, 100)

        # Should work with JIT-compiled function
        popt, pcov = curve_fit(exponential, x, y, p0=[1, 1, 0])
        np.testing.assert_allclose(popt, [2.5, 1.3, 0.1], rtol=0.1)


class TestChunkingAccuracy(unittest.TestCase):
    """Test accuracy of chunked fitting compared to single fit."""

    def setUp(self):
        """Set up test data and configuration."""
        np.random.seed(42)
        self.config = LDMemoryConfig(
            memory_limit_gb=0.001,  # Force chunking
            min_chunk_size=100,
            max_chunk_size=500,
        )

    def test_exponential_chunking_accuracy(self):
        """Test chunked fitting accuracy for exponential function."""

        def exponential(x, a, b, c):
            return a * jnp.exp(-b * x) + c

        # Generate data
        x = np.linspace(0, 5, 2000)
        true_params = [2.5, 1.3, 0.1]
        y = exponential(x, *true_params) + np.random.normal(0, 0.01, 2000)

        # Single fit (reference)
        popt_single, _ = curve_fit(exponential, x, y, p0=[2.0, 1.0, 0.1])

        # Chunked fit with better initial guess
        fitter = LargeDatasetFitter(config=self.config)
        result = fitter.fit(exponential, x, y, p0=[2.0, 1.0, 0.1])

        # For chunked fitting of exponential functions, we mainly care that it converges
        # to reasonable values, not necessarily identical to single fit
        self.assertTrue(result.success)
        # Check parameter a is within reasonable range
        self.assertGreater(result.x[0], 0.5)  # Should be positive
        self.assertLess(result.x[0], 5.0)  # Should not be too large
        # Check parameter b is within reasonable range
        self.assertGreater(result.x[1], 0.1)  # Should be positive for decay
        self.assertLess(result.x[1], 3.0)  # Should not be too large
        # Check offset c is small
        self.assertLess(abs(result.x[2]), 0.5)

    def test_polynomial_chunking_accuracy(self):
        """Test chunked fitting accuracy for polynomial function."""

        def poly3(x, a0, a1, a2, a3):
            return a0 + a1 * x + a2 * x**2 + a3 * x**3

        x = np.linspace(-2, 2, 1500)
        true_params = [1.0, -0.5, 0.3, -0.1]
        y = poly3(x, *true_params) + np.random.normal(0, 0.01, 1500)

        # Single fit
        popt_single, _ = curve_fit(poly3, x, y, p0=[1, 1, 1, 1])

        # Chunked fit
        fitter = LargeDatasetFitter(config=self.config)
        result = fitter.fit(poly3, x, y, p0=[1, 1, 1, 1])

        # Accuracy check - allow more tolerance for chunked polynomial fitting
        np.testing.assert_allclose(result.x, popt_single, rtol=0.05)
        np.testing.assert_allclose(result.x, true_params, rtol=0.05)

    def test_convergence_monitoring(self):
        """Test that parameter convergence is properly monitored."""

        def gaussian(x, amp, mu, sigma):
            return amp * jnp.exp(-((x - mu) ** 2) / (2 * sigma**2))

        x = np.linspace(-5, 5, 1000)
        true_params = [1.5, 0.5, 1.2]
        y = gaussian(x, *true_params) + np.random.normal(0, 0.01, 1000)

        # Use chunked fitting with convergence monitoring
        fitter = LargeDatasetFitter(config=self.config)
        result = fitter.fit(gaussian, x, y, p0=[1, 0, 1])

        # Check result validity
        self.assertTrue(result.success)
        # Chunked fitting may have slightly different convergence
        np.testing.assert_allclose(result.x, true_params, rtol=0.1)

        # Check that chunking info is included
        if hasattr(result, "n_chunks"):
            self.assertGreater(result["n_chunks"], 1)

    def test_failed_chunk_recovery(self):
        """Test recovery from failed chunks."""

        def problematic_func(x, a, b):
            # Function that can fail for certain parameter values
            with warnings.catch_warnings():
                warnings.simplefilter("ignore")
                return a * jnp.exp(-b * x)  # Can overflow for negative b

        x = np.linspace(0, 10, 1000)
        y = 2.0 * np.exp(-0.5 * x) + np.random.normal(0, 0.01, 1000)

        # This should handle failures gracefully
        fitter = LargeDatasetFitter(config=self.config)
        result = fitter.fit(problematic_func, x, y, p0=[1, 1])

        # Should still get reasonable results
        if result.success:
            # Allow more tolerance for potentially problematic function
            np.testing.assert_allclose(result.x, [2.0, 0.5], rtol=0.25)


class TestEndToEndValidation(unittest.TestCase):
    """End-to-end validation of the complete pipeline."""

    def test_curve_fit_large_small_dataset(self):
        """Test that curve_fit_large handles small datasets correctly."""

        def linear(x, a, b):
            return a * x + b

        x = np.linspace(0, 10, 100)
        y = 2.5 * x + 1.0 + np.random.normal(0, 0.1, 100)

        # Should automatically use regular curve_fit for small data
        popt, pcov = curve_fit_large(linear, x, y, p0=[1, 0])

        np.testing.assert_allclose(popt, [2.5, 1.0], rtol=0.1)
        self.assertIsNotNone(pcov)
        self.assertEqual(pcov.shape, (2, 2))

    def test_curve_fit_large_big_dataset(self):
        """Test curve_fit_large with dataset that triggers chunking."""

        def exponential(x, a, b, c):
            return a * jnp.exp(-b * x) + c

        # Create large dataset
        x = np.linspace(0, 5, 2_000_000)  # 2M points
        true_params = [2.5, 1.3, 0.1]
        y = exponential(x, *true_params) + np.random.normal(0, 0.01, 2_000_000)

        # Force chunking with small memory limit
        popt, pcov = curve_fit_large(
            exponential,
            x,
            y,
            p0=[2.0, 1.0, 0.1],
            memory_limit_gb=0.1,
            show_progress=False,
        )

        # For heavily chunked fitting (10 chunks), just verify reasonable results
        # Exponential fitting with many chunks is challenging
        self.assertIsNotNone(popt)
        self.assertEqual(pcov.shape, (3, 3))
        # Check that parameters are in reasonable ranges
        self.assertGreater(popt[0], 0.5)  # a should be positive
        self.assertLess(popt[0], 10.0)  # a shouldn't be too large
        self.assertGreater(popt[1], 0.1)  # b should be positive for decay
        self.assertLess(popt[1], 3.0)  # b shouldn't be too large
        self.assertLess(abs(popt[2]), 1.0)  # offset should be small

    def test_memory_estimation_accuracy(self):
        """Test that memory estimation is accurate."""
        # Test various dataset sizes
        test_cases = [
            (100_000, 3, 1.0),  # 100k points, 3 params, < 1GB
            (1_000_000, 5, 10.0),  # 1M points, 5 params, < 10GB
            (10_000_000, 4, 50.0),  # 10M points, 4 params, < 50GB
        ]

        for n_points, n_params, max_gb in test_cases:
            stats = estimate_memory_requirements(n_points, n_params)

            self.assertEqual(stats.n_points, n_points)
            self.assertEqual(stats.n_params, n_params)
            self.assertLess(stats.total_memory_estimate_gb, max_gb)
            self.assertGreater(stats.total_memory_estimate_gb, 0)

    def test_return_type_consistency(self):
        """Test that return types are consistent across different code paths."""

        def quadratic(x, a, b, c):
            return a * x**2 + b * x + c

        x_small = np.linspace(-2, 2, 100)
        y_small = quadratic(x_small, 1, -2, 1) + np.random.normal(0, 0.01, 100)

        # Test different code paths
        # 1. Regular curve_fit
        result1 = curve_fit(quadratic, x_small, y_small, p0=[1, 1, 1])
        self.assertIsInstance(result1, tuple)
        self.assertEqual(len(result1), 2)

        # 2. curve_fit_large with small data (uses regular curve_fit internally)
        result2 = curve_fit_large(quadratic, x_small, y_small, p0=[1, 1, 1])
        self.assertIsInstance(result2, tuple)
        self.assertEqual(len(result2), 2)

        # 3. LargeDatasetFitter (returns OptimizeResult)
        fitter = LargeDatasetFitter()
        result3 = fitter.fit(quadratic, x_small, y_small, p0=[1, 1, 1])
        from nlsq._optimize import OptimizeResult

        self.assertIsInstance(result3, OptimizeResult)
        self.assertTrue(hasattr(result3, "x"))
        self.assertTrue(hasattr(result3, "success"))

    def test_edge_cases(self):
        """Test edge cases in the fitting pipeline."""
        # Test with single data point (should fail gracefully)
        with self.assertRaises(ValueError):
            curve_fit_large(lambda x, a: a * x, [1.0], [2.0], p0=[1.0])

        # Test with empty data
        with self.assertRaises(ValueError):
            curve_fit_large(lambda x, a: a * x, [], [], p0=[1.0])

        # Test with mismatched data sizes
        with self.assertRaises(ValueError):
            curve_fit_large(lambda x, a: a * x, [1, 2, 3], [1, 2], p0=[1.0])


class TestPerformanceBenchmarks(unittest.TestCase):
    """Performance benchmarks for chunked vs non-chunked fitting."""

    @unittest.skip("Performance test - run manually")
    def test_performance_comparison(self):
        """Compare performance of chunked vs non-chunked fitting."""
        import time

        def exponential(x, a, b, c):
            return a * jnp.exp(-b * x) + c

        # Generate test data
        sizes = [10_000, 100_000, 500_000]
        results = {}

        for size in sizes:
            x = np.linspace(0, 5, size)
            y = 2.5 * np.exp(-1.3 * x) + 0.1 + np.random.normal(0, 0.01, size)

            # Time regular fit
            start = time.time()
            popt1, _ = curve_fit(exponential, x, y, p0=[1, 1, 0])
            time_regular = time.time() - start

            # Time chunked fit
            config = LDMemoryConfig(memory_limit_gb=0.01)
            fitter = LargeDatasetFitter(config=config)
            start = time.time()
            result = fitter.fit(exponential, x, y, p0=[1, 1, 0])
            time_chunked = time.time() - start

            results[size] = {
                "regular": time_regular,
                "chunked": time_chunked,
                "ratio": time_chunked / time_regular,
            }

            # Chunked should be within 2x of regular performance
            self.assertLess(results[size]["ratio"], 2.0)

        print(f"Performance results: {results}")


if __name__ == "__main__":
    unittest.main()
