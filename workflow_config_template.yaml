# ==============================================================================
# NLSQ Workflow Configuration Template
# ==============================================================================
# Version: 3.0 (Updated for NLSQ v0.4.0+ with CLI support)
#
# This template provides comprehensive configuration options for NLSQ curve
# fitting workflows. Copy this file to your project and customize as needed.
#
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ CLI USAGE                                                                   │
# ├─────────────────────────────────────────────────────────────────────────────┤
# │ Single fit:    nlsq fit workflow.yaml                                       │
# │ Batch fits:    nlsq batch w1.yaml w2.yaml w3.yaml                           │
# │                nlsq batch configs/*.yaml    # Shell glob expansion          │
# │ System info:   nlsq info                                                    │
# └─────────────────────────────────────────────────────────────────────────────┘
#
# Quick Start:
#   1. Copy this file: cp workflow_config_template.yaml my_workflow.yaml
#   2. Configure: data.input_file, data.columns, model.type, model.name
#   3. Run: nlsq fit my_workflow.yaml
#
# Minimal Configuration Example:
#   data:
#     input_file: "data/experiment.csv"
#     columns: { x: 0, y: 1 }
#   model:
#     type: "builtin"
#     name: "exponential_decay"
#     auto_p0: true
#   export:
#     results_file: "results.json"
#
# Python API (alternative to CLI):
#   from nlsq.workflow import load_yaml_config
#   config = load_yaml_config("my_workflow.yaml")
#
# Documentation: https://nlsq.readthedocs.io/en/latest/guides/workflow_options.html
# ==============================================================================

# ==============================================================================
# TUTORIAL: Understanding NLSQ Workflow System
# ==============================================================================
#
# NLSQ provides a tiered workflow system that automatically selects optimal
# fitting strategies based on your dataset size, available memory, and goals.
#
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ WORKFLOW TIERS (Processing Strategies)                                      │
# ├─────────────────────────────────────────────────────────────────────────────┤
# │ STANDARD              │ Direct curve_fit() for datasets < 10K points       │
# │ CHUNKED               │ Memory-managed chunking for 10K-10M points          │
# │ STREAMING             │ Mini-batch gradient descent for 10M-100M points     │
# │ STREAMING_CHECKPOINT  │ Streaming with fault tolerance for 100M+ points     │
# └─────────────────────────────────────────────────────────────────────────────┘
#
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ OPTIMIZATION GOALS                                                          │
# ├─────────────────────────────────────────────────────────────────────────────┤
# │ FAST              │ Speed priority, looser tolerances, no multi-start       │
# │ ROBUST            │ Balanced precision/speed, multi-start enabled           │
# │ GLOBAL            │ Synonym for ROBUST, emphasizes global optimum search    │
# │ QUALITY           │ Highest precision, tighter tolerances, validation       │
# │ MEMORY_EFFICIENT  │ Minimize memory with streaming/small chunks             │
# └─────────────────────────────────────────────────────────────────────────────┘
#
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ DATASET SIZE TIERS & RECOMMENDED TOLERANCES                                 │
# ├─────────────────────────────────────────────────────────────────────────────┤
# │ Size Tier    │ Data Points   │ Recommended tol │ Notes                      │
# │──────────────│───────────────│─────────────────│────────────────────────────│
# │ TINY         │ < 1,000       │ 1e-12           │ Maximum precision          │
# │ SMALL        │ 1K - 10K      │ 1e-10           │ High precision             │
# │ MEDIUM       │ 10K - 100K    │ 1e-9            │ Balanced                   │
# │ LARGE        │ 100K - 1M     │ 1e-8            │ Standard (NLSQ default)    │
# │ VERY_LARGE   │ 1M - 10M      │ 1e-7            │ Chunked processing         │
# │ HUGE         │ 10M - 100M    │ 1e-6            │ Streaming mode             │
# │ MASSIVE      │ > 100M        │ 1e-5            │ Streaming + checkpoints    │
# └─────────────────────────────────────────────────────────────────────────────┘
#
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ WORKFLOW SELECTION MATRIX                                                   │
# ├─────────────────────────────────────────────────────────────────────────────┤
# │ Dataset Size │ Low (<16GB) │ Medium (16-64GB) │ High (64-128GB) │ >128GB    │
# │──────────────│─────────────│──────────────────│─────────────────│───────────│
# │ Small <10K   │ standard    │ standard         │ standard        │ +quality  │
# │ Medium 10K-1M│ chunked     │ standard         │ standard+ms     │ +ms       │
# │ Large 1M-10M │ streaming   │ chunked          │ chunked+ms      │ chunked+ms│
# │ Huge 10M-100M│ stream+ckpt │ streaming        │ chunked         │ chunked+ms│
# │ Massive >100M│ stream+ckpt │ streaming+ckpt   │ streaming       │ stream+ms │
# └─────────────────────────────────────────────────────────────────────────────┘
# (ms = multi-start, ckpt = checkpointing)
#
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ FOUR-PHASE HYBRID STREAMING OPTIMIZER                                       │
# ├─────────────────────────────────────────────────────────────────────────────┤
# │ The streaming optimizer uses a 4-phase approach for optimal convergence:    │
# │                                                                             │
# │ Phase 0: Parameter Normalization                                            │
# │   - Scales parameters to similar magnitudes                                 │
# │   - Strategies: 'auto', 'bounds', 'p0', 'none'                              │
# │   - Improves gradient signal quality                                        │
# │                                                                             │
# │ Phase 1: Adam Warmup (Mini-batch gradient descent)                          │
# │   - Fast initial convergence from any starting point                        │
# │   - Adaptive switching based on plateau/gradient criteria                   │
# │   - Default: 200-500 iterations                                             │
# │   - Protected by 4-Layer Defense Strategy                        │
# │                                                                             │
# │ Phase 2: Streaming Gauss-Newton                                             │
# │   - Exact J^T J accumulation across data chunks                             │
# │   - Quadratic convergence near optimum                                      │
# │   - Trust region control for stability                                      │
# │                                                                             │
# │ Phase 3: Covariance & Denormalization                                       │
# │   - Compute exact covariance matrix                                         │
# │   - Transform back to original parameter space                              │
# └─────────────────────────────────────────────────────────────────────────────┘
#
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ 4-LAYER DEFENSE STRATEGY                                          │
# ├─────────────────────────────────────────────────────────────────────────────┤
# │ Prevents Adam warmup divergence when initial parameters are near optimal:   │
# │                                                                             │
# │ Layer 1: Warm Start Detection                                               │
# │   - Skips warmup if initial loss < warm_start_threshold (default: 1%)       │
# │   - Prevents pushing away from good initial guesses                         │
# │   - Use Case: Refinement workflows, iterative fitting                       │
# │                                                                             │
# │ Layer 2: Adaptive Learning Rate                                             │
# │   - Refinement mode (< 10% variance): LR = 1e-6 (ultra-conservative)        │
# │   - Careful mode (10-100% variance): LR = 1e-5 (conservative)               │
# │   - Exploration mode (≥ 100% variance): LR = 0.001 (standard)               │
# │   - Use Case: Multi-scale parameters, varying initial guess quality         │
# │                                                                             │
# │ Layer 3: Cost-Increase Guard                                                │
# │   - Aborts warmup if loss increases > cost_increase_tolerance (default: 5%) │
# │   - Returns best parameters found during warmup                             │
# │   - Use Case: Detecting divergence early, protecting expensive iterations   │
# │                                                                             │
# │ Layer 4: Step Clipping                                                      │
# │   - Limits parameter update magnitude to max_warmup_step_size (default: 0.1)│
# │   - Prevents large jumps that could overshoot optimum                       │
# │   - Use Case: Ill-conditioned problems, multi-scale parameters              │
# │                                                                             │
# │ Defense Presets:                                                            │
# │   - default:           All layers ON (recommended)                          │
# │   - defense_strict:    Lower thresholds for warm-start refinement           │
# │   - defense_relaxed:   Higher thresholds for exploration                    │
# │   - defense_disabled:  Pre-0.3.6 behavior (no protection)                   │
# │   - scientific_default: Tuned for physics/scientific computing              │
# └─────────────────────────────────────────────────────────────────────────────┘
#
# ==============================================================================
# TUTORIAL: Common Configuration Patterns
# ==============================================================================
#
# Pattern 1: Small Dataset, Maximum Precision (< 10K points)
# ──────────────────────────────────────────────────────────
#   default_workflow: "quality"
#   # or custom:
#   workflows:
#     my_precision:
#       tier: "STANDARD"
#       goal: "QUALITY"
#       gtol: 1.0e-12
#       ftol: 1.0e-12
#       xtol: 1.0e-12
#       enable_multistart: true
#       n_starts: 25
#
# Pattern 2: Large Dataset, Balanced Performance (100K - 10M points)
# ──────────────────────────────────────────────────────────────────
#   default_workflow: "large_robust"
#   # or custom:
#   workflows:
#     my_large:
#       tier: "CHUNKED"
#       goal: "ROBUST"
#       gtol: 1.0e-8
#       ftol: 1.0e-8
#       xtol: 1.0e-8
#       enable_multistart: true
#       n_starts: 10
#       chunk_size: 100000
#
# Pattern 3: Huge Dataset, Memory Constrained (10M+ points, <16GB RAM)
# ────────────────────────────────────────────────────────────────────
#   default_workflow: "streaming"
#   memory_limit_gb: 8.0
#   # or custom:
#   workflows:
#     my_streaming:
#       tier: "STREAMING"
#       goal: "MEMORY_EFFICIENT"
#       gtol: 1.0e-6
#       enable_checkpoints: true
#       checkpoint_dir: "checkpoints"
#
# Pattern 4: HPC Multi-GPU Cluster (PBS Pro)
# ──────────────────────────────────────────
#   default_workflow: "hpc_distributed"
#   # Auto-detects cluster via PBS_NODEFILE
#
# Pattern 5: Quick Development/Testing
# ────────────────────────────────────
#   default_workflow: "fast"
#   runtime:
#     precision: "float32"
#   fitting:
#     termination:
#       max_iterations: 50
#
# Pattern 6: Warm Start Refinement
# ──────────────────────────────────────────
# Use when refining parameters from a previous fit.
#   default_workflow: "streaming"
#   hybrid_streaming:
#     defense_layers:
#       preset: "strict"  # Or configure layers individually:
#       layer1_warm_start:
#         enabled: true
#         threshold: 0.005  # 0.5% - stricter than default 1%
#       layer2_adaptive_lr:
#         enabled: true
#         lr_refinement: 1.0e-7  # More conservative
#       layer3_cost_guard:
#         enabled: true
#         tolerance: 0.02  # 2% - tighter than default 5%
#
# Pattern 7: Production Monitoring with Telemetry
# ─────────────────────────────────────────────────────────
# For batch processing with defense layer monitoring.
#   default_workflow: "streaming"
#   hybrid_streaming:
#     defense_layers:
#       preset: null  # Use defaults
#     telemetry:
#       enabled: true
#       export_format: "prometheus"
#   # In code:
#   #   from nlsq import get_defense_telemetry, reset_defense_telemetry
#   #   reset_defense_telemetry()
#   #   for dataset in datasets:
#   #       curve_fit(model, x, y, method="hybrid_streaming")
#   #   telemetry = get_defense_telemetry()
#   #   push_to_prometheus(telemetry.export_metrics())
#
# ==============================================================================

metadata:
  # Human-friendly name for logs/reports. Keep short, unique, and stable.
  workflow_name: "example_workflow"
  # Brief description of workflow purpose.
  description: "NLSQ workflow configuration template with optimized defaults."
  # Configuration version for tracking changes.
  version: "2.1"
  # Optional ownership information.
  author: ""
  contact: ""

# ==============================================================================
# WORKFLOW SYSTEM SETTINGS
# ==============================================================================
# These settings control the NLSQ workflow tier and optimization behavior.
# The default_workflow can be a preset name or a custom workflow defined below.
#
# Built-in presets:
#   - standard:         Standard curve_fit() with default tolerances
#   - quality:          Highest precision with multi-start and tight tolerances
#   - fast:             Speed-optimized with looser tolerances
#   - large_robust:     Chunked processing with multi-start for large datasets
#   - streaming:        AdaptiveHybridStreamingOptimizer for huge datasets
#   - hpc_distributed:  Multi-GPU/node configuration for HPC clusters
#   - memory_efficient: Minimize memory with streaming/small chunks

default_workflow: "standard"

# Memory limit for workflow auto-selection. Set to null to auto-detect.
# Performance: Lower limits force streaming/chunked tiers.
# Precision: Higher limits allow more data in memory for better accuracy.
memory_limit_gb: null  # Auto-detect (recommended)

# Custom workflow definitions. Each entry maps to WorkflowConfig.from_dict().
# Override any preset or create entirely new configurations.
workflows:
  # Example: Precision-focused workflow for publication-quality results
  precision_first:
    tier: "STANDARD"
    goal: "QUALITY"
    gtol: 1.0e-10
    ftol: 1.0e-10
    xtol: 1.0e-10
    enable_multistart: true
    n_starts: 20
    sampler: "lhs"           # "lhs", "sobol", or "halton"
    center_on_p0: true       # Center samples around initial guess
    scale_factor: 1.0        # Exploration region scale
    enable_checkpoints: false

  # Example: Large dataset with multi-start
  large_multistart:
    tier: "CHUNKED"
    goal: "ROBUST"
    gtol: 1.0e-8
    ftol: 1.0e-8
    xtol: 1.0e-8
    enable_multistart: true
    n_starts: 10
    sampler: "lhs"
    chunk_size: 100000       # Points per chunk (CHUNKED tier only)
    enable_checkpoints: false

  # Example: Memory-constrained streaming
  low_memory_streaming:
    tier: "STREAMING"
    goal: "MEMORY_EFFICIENT"
    gtol: 1.0e-7
    ftol: 1.0e-7
    xtol: 1.0e-7
    enable_multistart: false
    chunk_size: 5000         # Smaller chunks for lower memory
    enable_checkpoints: true
    checkpoint_dir: "checkpoints"

# ==============================================================================
# PATHS
# ==============================================================================
# File system locations for workflow artifacts.
# Use absolute paths for production; relative paths resolve from project_root.

paths:
  # Base directory for relative paths. Set explicitly for portability.
  project_root: "/path/to/project"
  # Input data location.
  input_dir: "data/input"
  # Primary outputs (results, reports).
  output_dir: "data/output"
  # Log files; keep separate for cleanup.
  logs_dir: "logs"
  # JIT compilation cache; safe to clear.
  cache_dir: "cache"
  # Temporary scratch space; use fast local storage.
  temp_dir: "tmp"

# ==============================================================================
# RUNTIME
# ==============================================================================
# Execution environment settings affecting performance and reproducibility.

runtime:
  # Device selection: "cpu" or "gpu".
  # CPU is default for reproducibility; GPU provides 150-270x speedup.
  # Note: GPU requires Linux + NVIDIA + CUDA 12.1-12.9
  device: "cpu"

  # Thread/worker counts for parallel sections.
  # Performance: Set <= physical cores to avoid oversubscription.
  threads: 4
  workers: 2

  # Numerical precision.
  # "float64" (default): Maximum accuracy, recommended for scientific work.
  # "float32": Faster, less memory, but reduced precision.
  # Precision priority per user requirements - always use float64.
  precision: "float64"
  dtype: "float64"

  # Random seed for reproducibility.
  # Determinism: Same seed produces identical results on same hardware.
  random_seed: 42

  # Verbosity and logging.
  # 0: Silent, 1: Progress, 2: Detailed, 3: Debug
  verbosity: 1
  log_level: "INFO"

# ==============================================================================
# LOGGING
# ==============================================================================
# Logging configuration for debugging and audit trails.

logging:
  # Log file path (relative to logs_dir or absolute).
  log_file: "workflow.log"
  # Console logging toggle.
  console: true
  # Structured logging for external tool ingestion.
  structured:
    enabled: false
    format: "json"  # "json" for tooling compatibility
  # Log rotation to prevent unbounded growth.
  rotation:
    enabled: true
    max_bytes: 10485760  # 10 MB
    backup_count: 5
  # Timestamps.
  timestamps:
    enabled: true
    timezone: "UTC"

# ==============================================================================
# DATA
# ==============================================================================
# Dataset configuration for CLI data loading.
# Supports: ASCII text (.txt, .dat, .asc), CSV (.csv), NPZ (.npz), HDF5 (.h5, .hdf5)
#
# Usage with CLI:
#   nlsq fit workflow.yaml           # Single fit
#   nlsq batch w1.yaml w2.yaml ...   # Parallel batch fitting

data:
  # Dataset identifier for reports/exports.
  dataset_id: "dataset_001"

  # ==========================================================================
  # INPUT FILE (Required for CLI)
  # ==========================================================================
  # Path to the data file (relative to project_root or absolute).
  input_file: "data/experiment_001.csv"

  # File format: "auto" (detect from extension) or explicit.
  # Supported: "ascii", "csv", "npz", "hdf5"
  format: "auto"

  # ==========================================================================
  # COLUMN SELECTION
  # ==========================================================================
  # Specify which columns contain x, y, and optional sigma (uncertainty) data.
  # Use integer index (0-based) for ASCII/CSV, or key/path names for NPZ/HDF5.
  columns:
    x: 0              # Column index (int) or column name (str for CSV with header)
    y: 1              # Column index (int) or column name (str for CSV with header)
    sigma: null       # Optional: column for y uncertainties (null = no weights)

  # ==========================================================================
  # ASCII TEXT FORMAT OPTIONS (.txt, .dat, .asc)
  # ==========================================================================
  # For whitespace or tab-delimited text files.
  ascii:
    delimiter: null           # null = any whitespace, or explicit: "\t", " ", ","
    comment_char: "#"         # Lines starting with this are skipped
    skip_header: 0            # Number of header lines to skip
    skip_footer: 0            # Number of footer lines to skip
    usecols: null             # Specific columns to read, e.g., [0, 1] (null = all)
    dtype: "float64"          # Data type: "float64", "float32"

  # ==========================================================================
  # CSV FORMAT OPTIONS (.csv)
  # ==========================================================================
  csv:
    delimiter: ","            # Field delimiter
    header: true              # First row contains column names
    skip_header: 0            # Additional header lines to skip (after column names)
    encoding: "utf-8"         # File encoding
    missing_values:           # Values treated as NaN
      - ""
      - "NA"
      - "null"
      - "NaN"

  # ==========================================================================
  # NPZ FORMAT OPTIONS (.npz)
  # ==========================================================================
  # NumPy compressed archive format.
  npz:
    x_key: "x"                # Array key for x data
    y_key: "y"                # Array key for y data
    sigma_key: null           # Optional: array key for sigma (null = no weights)

  # ==========================================================================
  # HDF5 FORMAT OPTIONS (.h5, .hdf5)
  # ==========================================================================
  # Hierarchical Data Format 5 for large datasets.
  hdf5:
    x_path: "/data/x"         # Dataset path for x data
    y_path: "/data/y"         # Dataset path for y data
    sigma_path: null          # Optional: dataset path for sigma (null = no weights)

  # ==========================================================================
  # LEGACY: File discovery patterns (for programmatic use)
  # ==========================================================================
  file_patterns:
    - "*.csv"
    - "*.txt"
    - "*.dat"
    - "*.npz"
    - "*.h5"
    - "*.hdf5"

  # ==========================================================================
  # VALIDATION RULES
  # ==========================================================================
  validation:
    require_finite: true      # Reject NaN/Inf values
    min_points: 2             # Minimum data points required
    units:
      x: "unitless"
      y: "unitless"
    ranges:
      x:
        min: null
        max: null
      y:
        min: null
        max: null

# ==============================================================================
# PREPROCESSING
# ==============================================================================
# Data preprocessing before fitting.
# Note: Subsampling was removed in v0.2.0. NLSQ uses streaming for 100% data.

preprocessing:
  # Normalization/scaling before fitting.
  # Warning: Changing preprocessing changes parameter interpretation.
  normalization:
    enabled: false
    method: "standard"  # "standard", "minmax", "robust", "none"
  # Outlier filtering.
  filtering:
    enabled: false
    method: "none"      # "none", "iqr", "zscore", "custom"
    threshold: 3.0

# ==============================================================================
# MODEL
# ==============================================================================
# Model function configuration for CLI fitting.
#
# Three model types are supported:
#   1. "builtin"    - Use nlsq.functions library (recommended)
#   2. "custom"     - Load from external Python file
#   3. "polynomial" - Generate polynomial of specified degree
#
# ┌─────────────────────────────────────────────────────────────────────────────┐
# │ BUILTIN MODELS (nlsq.functions)                                             │
# ├─────────────────────────────────────────────────────────────────────────────┤
# │ Name               │ Function                    │ Parameters               │
# │────────────────────│─────────────────────────────│──────────────────────────│
# │ linear             │ a*x + b                     │ a, b                     │
# │ exponential_decay  │ a*exp(-b*x) + c             │ a, b, c                  │
# │ exponential_growth │ a*exp(b*x) + c              │ a, b, c                  │
# │ gaussian           │ amp*exp(-(x-mu)²/(2σ²))     │ amp, mu, sigma           │
# │ sigmoid            │ L/(1+exp(-k*(x-x0))) + b    │ L, x0, k, b              │
# │ power_law          │ a*x^b                       │ a, b                     │
# │ polynomial         │ Σ(aᵢ*x^i)                   │ a0, a1, ..., an          │
# └─────────────────────────────────────────────────────────────────────────────┘

model:
  # ==========================================================================
  # MODEL SOURCE TYPE (Required for CLI)
  # ==========================================================================
  # Options: "builtin", "custom", "polynomial"
  type: "builtin"

  # ==========================================================================
  # BUILTIN MODEL CONFIGURATION
  # ==========================================================================
  # When type: "builtin", specify the model name from nlsq.functions.
  name: "exponential_decay"   # See table above for available models

  # Automatic initial parameter estimation (uses data characteristics).
  # Each builtin model has a corresponding estimate_p0_<name>() function.
  auto_p0: true

  # Automatic bounds from builtin model defaults.
  # Each builtin model has a corresponding bounds_<name>() function.
  auto_bounds: false

  # ==========================================================================
  # CUSTOM MODEL CONFIGURATION
  # ==========================================================================
  # When type: "custom", load model function from external Python file.
  # The file must define a function: def model_name(x, *params) -> array
  #
  # Example custom model file (models.py):
  #   import jax.numpy as jnp
  #   def my_model(x, a, b, c):
  #       return a * jnp.exp(-b * x) + c
  #
  custom:
    file: "models.py"         # Python file path (relative to project_root)
    function: "my_model"      # Function name to import

  # ==========================================================================
  # POLYNOMIAL MODEL CONFIGURATION
  # ==========================================================================
  # When type: "polynomial", generate polynomial of specified degree.
  # f(x) = a0 + a1*x + a2*x² + ... + an*x^n
  polynomial:
    degree: 3                 # Polynomial degree (n)

  # ==========================================================================
  # MODEL IDENTIFIER
  # ==========================================================================
  model_id: "model_v1"        # Unique identifier for reports

  # ==========================================================================
  # PARAMETER DEFINITIONS
  # ==========================================================================
  # Manual parameter configuration. Used when:
  #   - auto_p0: false (provides initial values)
  #   - auto_bounds: false (provides bounds)
  #   - type: "custom" (required for custom models)
  #
  # Bounds should be informed by physical constraints; overly tight bounds bias.
  parameters:
    - name: "amplitude"
      initial: 1.0
      bounds: [0.0, 10.0]
      fixed: false
      transform: "none"       # "none", "log", "logit", "exp"
    - name: "decay_rate"
      initial: 0.1
      bounds: [0.001, 1.0]
      fixed: false
      transform: "none"
    - name: "offset"
      initial: 0.0
      bounds: [-1.0, 1.0]
      fixed: false
      transform: "none"

  # ==========================================================================
  # PARAMETER CONSTRAINTS (Advanced)
  # ==========================================================================
  constraints:
    enabled: false
    expressions:
      - "amplitude > offset"

# ==============================================================================
# FITTING
# ==============================================================================
# Core fitting algorithm configuration.

fitting:
  # Fitting method.
  method: "nlsq"  # "nlsq", "hybrid_streaming"
  # Objective function options.
  objective:
    weights: null        # null or array/column reference
    robust_loss: "none"  # "none", "huber", "soft_l1", "cauchy"
  # Convergence tolerances.
  # These should match dataset size tier recommendations (see tutorial above).
  termination:
    ftol: 1.0e-8   # Function tolerance
    xtol: 1.0e-8   # Parameter tolerance
    gtol: 1.0e-8   # Gradient tolerance
    max_iterations: 200
    max_function_evals: 2000
  # Jacobian computation.
  jacobian:
    mode: "auto"   # "auto" uses JAX autodiff (recommended)
    finite_diff_step: 1.0e-8
  # Multi-start optimization for global optimum search.
  multistart:
    enabled: false
    num_starts: 10
    sampler: "lhs"       # "lhs", "sobol", "halton"
    center_on_p0: true   # Center around initial guess
    scale_factor: 1.0    # Exploration region scale
    seed: 42

# ==============================================================================
# HYBRID STREAMING OPTIMIZER
# ==============================================================================
# Configuration for the four-phase AdaptiveHybridStreamingOptimizer.
# Used when tier is STREAMING or STREAMING_CHECKPOINT.

hybrid_streaming:
  # Phase 0: Parameter Normalization
  normalize: true
  normalization_strategy: "auto"  # "auto", "bounds", "p0", "none"

  # Phase 1: Adam Warmup
  warmup_iterations: 200        # Initial iterations before switch check
  max_warmup_iterations: 500    # Force switch to Phase 2
  warmup_learning_rate: 0.001   # Adam learning rate (overridden by adaptive LR)
  loss_plateau_threshold: 1.0e-4   # Plateau detection threshold
  gradient_norm_threshold: 1.0e-3  # Early switch if gradient small

  # Optax learning rate schedule (optional)
  use_learning_rate_schedule: false
  lr_schedule_warmup_steps: 50
  lr_schedule_decay_steps: 450
  lr_schedule_end_value: 0.0001
  gradient_clip_value: null  # Set to 1.0 for gradient clipping

  # ==========================================================================
  # 4-LAYER DEFENSE STRATEGY
  # ==========================================================================
  # Prevents Adam warmup divergence when initial parameters are near optimal.
  # All layers are enabled by default for improved stability.
  #
  # Presets available in code:
  #   - HybridStreamingConfig()                  # Default (all ON)
  #   - HybridStreamingConfig.defense_strict()  # Lower thresholds
  #   - HybridStreamingConfig.defense_relaxed() # Higher thresholds
  #   - HybridStreamingConfig.defense_disabled() # Pre-0.3.6 behavior
  #   - HybridStreamingConfig.scientific_default() # Physics/scientific
  #
  # Telemetry API for monitoring:
  #   from nlsq import get_defense_telemetry, reset_defense_telemetry
  #   telemetry = get_defense_telemetry()
  #   print(telemetry.get_summary())
  #   print(telemetry.get_trigger_rates())
  #   print(telemetry.export_metrics())  # Prometheus-compatible
  # ==========================================================================

  defense_layers:
    # Defense preset: "default", "strict", "relaxed", "disabled", "scientific"
    # Set to null to use individual layer settings below.
    preset: null

    # Layer 1: Warm Start Detection
    # Skips warmup if initial loss < threshold * data_variance
    # Use Case: Refinement workflows, warm starts from previous fits
    layer1_warm_start:
      enabled: true
      threshold: 0.01  # 1% of data variance (strict: 0.005, relaxed: 0.02)

    # Layer 2: Adaptive Learning Rate
    # Automatically selects LR based on initial fit quality
    # Use Case: Multi-scale parameters, varying initial guess quality
    layer2_adaptive_lr:
      enabled: true
      lr_refinement: 1.0e-6    # < 10% variance (ultra-conservative)
      lr_careful: 1.0e-5       # 10-100% variance (conservative)
      lr_exploration: 0.001    # >= 100% variance (standard Adam LR)

    # Layer 3: Cost-Increase Guard
    # Aborts warmup if loss increases beyond tolerance from initial
    # Use Case: Early divergence detection, protecting expensive iterations
    layer3_cost_guard:
      enabled: true
      tolerance: 0.05  # 5% increase allowed (strict: 0.02, relaxed: 0.10)

    # Layer 4: Step Clipping
    # Limits maximum parameter update magnitude per iteration
    # Use Case: Ill-conditioned problems, preventing overshooting
    layer4_step_clipping:
      enabled: true
      max_step_size: 0.1  # Max L2 norm (strict: 0.05, relaxed: 0.2)

  # Phase 2: Gauss-Newton
  gauss_newton_max_iterations: 100
  gauss_newton_tol: 1.0e-8
  trust_region_initial: 1.0
  regularization_factor: 1.0e-10

  # Streaming chunk configuration
  chunk_size: 10000  # Points per chunk

  # Fault tolerance
  enable_checkpoints: true
  checkpoint_frequency: 100
  checkpoint_dir: null  # Auto-generate timestamped directory
  validate_numerics: true
  enable_fault_tolerance: true
  max_retries_per_batch: 2
  min_success_rate: 0.5

  # Precision control
  # "auto": float32 for Phase 1, float64 for Phase 2+ (recommended)
  precision: "auto"

  # Multi-device (multi-GPU) support
  enable_multi_device: false

  # Progress monitoring
  callback_frequency: 10

  # Telemetry for production monitoring
  # Tracks defense layer activation rates across fits
  telemetry:
    enabled: true
    export_format: "prometheus"  # "prometheus", "json", "none"

  # Tournament selection for multi-start streaming
  enable_multistart: false
  n_starts: 10
  multistart_sampler: "lhs"
  elimination_rounds: 3
  elimination_fraction: 0.5
  batches_per_round: 50

# ==============================================================================
# GLOBAL OPTIMIZATION
# ==============================================================================
# Multi-start global optimization with Latin Hypercube Sampling.

global_optimization:
  # Presets: "fast", "robust", "global", "thorough", "streaming"
  preset: null  # Use preset or define custom below

  # Custom configuration (overrides preset if set)
  n_starts: 10               # Number of starting points (0 = disabled)
  sampler: "lhs"             # "lhs", "sobol", "halton"
  center_on_p0: true         # Center samples around initial guess
  scale_factor: 1.0          # Exploration region scale factor

  # Tournament selection for large datasets
  elimination_rounds: 3      # Rounds of elimination
  elimination_fraction: 0.5  # Fraction eliminated per round
  batches_per_round: 50      # Data batches for evaluation

# ==============================================================================
# TRUST REGION / STEP SIZE
# ==============================================================================
# Advanced optimization tuning. Change only for troubleshooting convergence.

optimization:
  step_size:
    initial: 1.0
    min: 1.0e-10
    max: 10.0
  trust_region:
    enabled: true
    initial_radius: 1.0
    max_radius: 100.0
    min_radius: 1.0e-10
  damping:
    lambda_init: 1.0
    lambda_min: 1.0e-10
    lambda_max: 1.0e10

# ==============================================================================
# WORKFLOW STEPS
# ==============================================================================
# Ordered pipeline steps. Disable unneeded steps for performance.

workflow_steps:
  - name: "load_data"
    enabled: true
    inputs: []
    outputs: ["raw_dataset"]
  - name: "validate_data"
    enabled: true
    inputs: ["raw_dataset"]
    outputs: ["validated_dataset"]
  - name: "preprocess"
    enabled: true
    inputs: ["validated_dataset"]
    outputs: ["processed_dataset"]
  - name: "fit_model"
    enabled: true
    inputs: ["processed_dataset"]
    outputs: ["fit_result"]
  - name: "postprocess"
    enabled: true
    inputs: ["fit_result"]
    outputs: ["final_result"]

# ==============================================================================
# VALIDATION AND QUALITY CONTROL
# ==============================================================================

validation_and_qc:
  # Sanity checks for outputs.
  sanity_checks:
    enabled: true
    checks:
      - "finite_parameters"
      - "nonnegative_variances"
      - "positive_definite_covariance"
  # Numerical parity checks (useful for CPU/GPU comparison).
  numerical_parity:
    enabled: false
    tolerance: 1.0e-8
  # Acceptance thresholds for fit quality.
  acceptance:
    max_rmse: null
    max_mae: null
    min_r2: null

# ==============================================================================
# REPORTING
# ==============================================================================

reporting:
  summary:
    enabled: true
    include_tables: true
    include_plots: false  # Disable for batch runs
  formats:
    - "json"
    - "csv"

# ==============================================================================
# VISUALIZATION
# ==============================================================================
# Publication-quality figure generation for fit results.
# Generates: main fit plot, residuals plot, and optional histogram.
#
# Output formats: PDF (vector), SVG (vector), PNG (raster), EPS (LaTeX)
# Style presets: "publication", "presentation", "nature", "science", "minimal"

visualization:
  # ==========================================================================
  # MASTER SWITCH
  # ==========================================================================
  enabled: true

  # ==========================================================================
  # OUTPUT SETTINGS
  # ==========================================================================
  # Directory for saving figures (relative to project_root).
  output_dir: "figures"

  # Filename prefix. Generates: {prefix}_combined.pdf, {prefix}_main.pdf, etc.
  filename_prefix: "fit"

  # Output formats to generate (all will be saved).
  formats:
    - "pdf"                       # Vector format, best for publications
    - "png"                       # Raster format for web/slides
    # - "svg"                     # Vector format for web
    # - "eps"                     # Vector format for LaTeX

  # ==========================================================================
  # PUBLICATION QUALITY SETTINGS
  # ==========================================================================
  # Resolution in dots per inch.
  # 300 DPI: Standard for most journals
  # 600 DPI: High-resolution print
  dpi: 300

  # Figure size in inches [width, height].
  # Single column: [3.5, 2.625] (Nature, Science)
  # Double column: [7.0, 5.25]
  # Full page: [7.0, 9.0]
  figure_size: [6.0, 4.5]

  # ==========================================================================
  # STYLE PRESETS
  # ==========================================================================
  # Preset styles optimized for different contexts:
  #   "publication"  - Clean serif fonts, 300 DPI, suitable for most journals
  #   "presentation" - Larger sans-serif fonts, lower DPI for slides
  #   "nature"       - Nature journal specifications (3.5" width, Arial font)
  #   "science"      - Science journal specifications
  #   "minimal"      - No top/right spines, no grid, clean look
  style: "publication"

  # ==========================================================================
  # FONT SETTINGS
  # ==========================================================================
  font:
    # Font family: "serif" (Times-like), "sans-serif" (Arial-like), "monospace"
    family: "serif"

    # Base font size in points (axis labels, legends scale from this)
    size: 10

    # Math font: "cm" (Computer Modern), "stix", "dejavusans", "dejavuserif"
    math_fontset: "cm"

  # ==========================================================================
  # LAYOUT OPTIONS
  # ==========================================================================
  # "combined": Main plot + residuals in subplots (single figure)
  # "separate": Individual files for each plot type
  layout: "combined"

  # ==========================================================================
  # MAIN PLOT (Data + Fitted Curve)
  # ==========================================================================
  main_plot:
    # Title (null = no title, or custom string)
    title: null

    # Axis labels (use LaTeX: "$x$ (units)" for math)
    x_label: "x"
    y_label: "y"

    # Grid lines
    show_grid: true
    grid_alpha: 0.3

    # --------------------------------------------------------------------------
    # Raw Data Appearance
    # --------------------------------------------------------------------------
    data:
      marker: "o"                 # Marker style: "o", "s", "^", "D", "x", "+"
      color: "#1f77b4"            # Color (hex, name, or RGB tuple)
      size: 20                    # Marker size in points²
      alpha: 0.7                  # Transparency (0-1)
      label: "Data"               # Legend label

      # Error bars (if sigma provided in data)
      show_errorbars: true
      errorbar_color: null        # null = same as marker color
      capsize: 2                  # Error bar cap width in points

    # --------------------------------------------------------------------------
    # Fitted Curve Appearance
    # --------------------------------------------------------------------------
    fit:
      color: "#d62728"            # Line color
      linewidth: 1.5              # Line width in points
      linestyle: "-"              # Style: "-", "--", "-.", ":"
      label: "Fit"                # Legend label
      n_points: 500               # Number of points for smooth curve

    # --------------------------------------------------------------------------
    # Confidence Band (requires covariance matrix)
    # --------------------------------------------------------------------------
    confidence_band:
      enabled: false
      level: 0.95                 # Confidence level (0.95 = 95% CI)
      color: null                 # null = use fit line color
      alpha: 0.2                  # Band transparency

    # --------------------------------------------------------------------------
    # Legend
    # --------------------------------------------------------------------------
    legend:
      enabled: true
      location: "best"            # "best", "upper right", "upper left", etc.
      frameon: true               # Show legend frame
      fontsize: null              # null = use base font size

    # --------------------------------------------------------------------------
    # Fit Statistics Annotation (optional)
    # --------------------------------------------------------------------------
    annotation:
      enabled: true
      show_r_squared: true
      show_rmse: false
      show_chi_squared: false
      location: "upper right"     # Text box location
      fontsize: 9

  # ==========================================================================
  # RESIDUALS PLOT
  # ==========================================================================
  residuals_plot:
    enabled: true

    # Plot type: "scatter", "stem", "line"
    type: "scatter"

    # Labels
    title: null                   # null = no title
    x_label: "x"
    y_label: "Residual"

    # Zero reference line
    show_zero_line: true
    zero_line_style: "--"
    zero_line_color: "gray"
    zero_line_width: 1.0

    # Marker appearance (for scatter/stem)
    marker: "o"
    color: "#2ca02c"              # Green
    size: 15
    alpha: 0.7

    # --------------------------------------------------------------------------
    # Standard Deviation Bands
    # --------------------------------------------------------------------------
    # Horizontal bands showing ±1σ, ±2σ regions
    std_bands:
      enabled: true
      levels: [1, 2]              # Show ±1σ and ±2σ bands
      colors:
        - "#fff3cd"               # Light yellow for ±1σ
        - "#ffe69c"               # Darker yellow for ±2σ
      alpha: 0.4

    # --------------------------------------------------------------------------
    # Normality Reference (optional)
    # --------------------------------------------------------------------------
    # Show expected distribution for normally distributed residuals
    normality_check:
      enabled: false
      show_expected_bounds: true  # Show ±2σ where 95% should fall

  # ==========================================================================
  # HISTOGRAM OF RESIDUALS (Optional)
  # ==========================================================================
  histogram:
    enabled: false

    # Number of bins: "auto", "sqrt", "sturges", or integer
    bins: "auto"

    # Overlay normal distribution fit
    show_normal_fit: true
    normal_color: "#d62728"

    # Bar appearance
    color: "#9467bd"              # Purple
    alpha: 0.7
    edgecolor: "white"

    # Labels
    title: "Residual Distribution"
    x_label: "Residual"
    y_label: "Frequency"

  # ==========================================================================
  # COLOR SCHEMES (Predefined palettes)
  # ==========================================================================
  # Colorblind-friendly palettes for accessibility
  color_schemes:
    # Default scheme
    default:
      data: "#1f77b4"             # Blue
      fit: "#d62728"              # Red
      residuals: "#2ca02c"        # Green
      confidence: "#ff7f0e"       # Orange

    # Colorblind-safe (Okabe-Ito palette)
    colorblind:
      data: "#0072B2"             # Blue
      fit: "#D55E00"              # Vermilion
      residuals: "#009E73"        # Bluish green
      confidence: "#F0E442"       # Yellow

    # Grayscale for B&W printing
    grayscale:
      data: "#404040"
      fit: "#000000"
      residuals: "#808080"
      confidence: "#C0C0C0"

  # Active color scheme (reference above or use "default")
  active_scheme: "default"

# ==============================================================================
# EXPORT
# ==============================================================================
# Output file configuration for CLI fitting results.

export:
  # ==========================================================================
  # RESULTS FILE (Required for CLI)
  # ==========================================================================
  # Path for fit results (relative to project_root or absolute).
  # File extension determines format: .json, .csv, .npz
  results_file: "output/fit_results.json"

  # ==========================================================================
  # EXPORT CONTENTS
  # ==========================================================================
  include:
    parameters: true          # popt (fitted parameters)
    covariance: true          # pcov (covariance matrix)
    uncertainties: true       # Parameter uncertainties (sqrt of diag(pcov))
    statistics: true          # r_squared, rmse, chi_squared, etc.
    residuals: false          # y - y_fit (can be large for big datasets)
    fitted_values: false      # y_fit values (can be large)
    convergence_info: true    # Iterations, function evaluations, status

  # ==========================================================================
  # OUTPUT FORMATS
  # ==========================================================================
  # Generate multiple output formats.
  formats:
    - "json"                  # Human-readable, includes all metadata
    - "csv"                   # Tabular parameters and statistics

  # ==========================================================================
  # ARTIFACT BUNDLE
  # ==========================================================================
  artifact_bundle:
    enabled: true
    include_data_snapshot: false
    include_logs: true
  config_snapshot:
    enabled: true
    filename: "config_snapshot.yaml"

# ==============================================================================
# BATCH PROCESSING
# ==============================================================================
# Configuration for parallel batch fitting via `nlsq batch` command.
#
# Usage:
#   nlsq batch workflow1.yaml workflow2.yaml workflow3.yaml
#   nlsq batch configs/*.yaml

batch:
  # ==========================================================================
  # PARALLELIZATION
  # ==========================================================================
  # Maximum parallel workers. null = auto (min of CPU count and file count).
  max_workers: null

  # Continue processing remaining files if one fails.
  continue_on_error: true

  # ==========================================================================
  # BATCH OUTPUT
  # ==========================================================================
  # Aggregate summary file for all batch results.
  summary_file: "output/batch_summary.json"

  # Summary format: "json", "csv", or "both"
  summary_format: "json"

  # Include individual result paths in summary.
  include_result_paths: true

# ==============================================================================
# ADVANCED
# ==============================================================================

advanced:
  # Memory management.
  memory:
    adaptive_fraction: 0.5   # Fraction of available memory to use
    max_gb: null             # Hard limit (null = no limit)
  # Timeouts (seconds).
  timeouts:
    per_step: 300            # 5 minutes per step
    total: 7200              # 2 hours total
  # Checkpointing for long runs.
  checkpointing:
    enabled: false
    interval_steps: 50
    directory: "checkpoints"
  # Debug flags.
  debug:
    enabled: false
    trace: false
    save_intermediate_arrays: false
    profile_jit: false       # Profile JAX JIT compilation

# ==============================================================================
# EXAMPLE PROFILES
# ==============================================================================
# Copy and uncomment these profiles, then set default_workflow to the name.

# precision_publication:
#   # Maximum precision for publication-quality results
#   runtime:
#     device: "cpu"
#     precision: "float64"
#     dtype: "float64"
#     random_seed: 42
#   fitting:
#     termination:
#       ftol: 1.0e-12
#       xtol: 1.0e-12
#       gtol: 1.0e-12
#       max_iterations: 500
#       max_function_evals: 10000
#     multistart:
#       enabled: true
#       num_starts: 25
#       sampler: "lhs"
#   validation_and_qc:
#     sanity_checks:
#       enabled: true
#     numerical_parity:
#       enabled: true
#       tolerance: 1.0e-10

# fast_development:
#   # Quick iteration during development
#   runtime:
#     device: "cpu"
#     precision: "float32"
#     verbosity: 2
#   fitting:
#     termination:
#       ftol: 1.0e-6
#       max_iterations: 50
#       max_function_evals: 200
#     multistart:
#       enabled: false
#   reporting:
#     summary:
#       include_plots: false

# large_dataset_gpu:
#   # GPU-accelerated large dataset processing
#   runtime:
#     device: "gpu"
#     precision: "float64"
#     threads: 8
#   hybrid_streaming:
#     warmup_iterations: 300
#     chunk_size: 50000
#     precision: "auto"
#     enable_checkpoints: true
#   advanced:
#     memory:
#       adaptive_fraction: 0.7

# hpc_cluster:
#   # HPC multi-node configuration (PBS Pro)
#   runtime:
#     device: "gpu"
#     precision: "float64"
#   hybrid_streaming:
#     enable_multi_device: true
#     chunk_size: 100000
#     enable_checkpoints: true
#     checkpoint_frequency: 50
#     enable_multistart: true
#     n_starts: 20
#   advanced:
#     checkpointing:
#       enabled: true
#       interval_steps: 25

# memory_constrained:
#   # Minimal memory footprint
#   runtime:
#     device: "cpu"
#     precision: "float32"
#     threads: 2
#     workers: 1
#   hybrid_streaming:
#     chunk_size: 2000
#     precision: "float32"
#     enable_checkpoints: true
#     checkpoint_frequency: 25
#   advanced:
#     memory:
#       adaptive_fraction: 0.3
#       max_gb: 4.0

# warm_start_refinement:
#   # Optimized for refining parameters from previous fits
#   # Uses strict defense layers to prevent divergence
#   runtime:
#     device: "cpu"
#     precision: "float64"
#   hybrid_streaming:
#     warmup_iterations: 100  # Shorter warmup (may be skipped anyway)
#     defense_layers:
#       preset: "strict"
#       # Strict preset uses:
#       # - warm_start_threshold: 0.005 (0.5%)
#       # - cost_increase_tolerance: 0.02 (2%)
#       # - max_step_size: 0.05
#     telemetry:
#       enabled: true
#       export_format: "json"

# exploration_from_poor_guess:
#   # When starting from potentially poor initial parameters
#   # Uses relaxed defense to allow more exploration
#   runtime:
#     device: "gpu"
#     precision: "float64"
#   hybrid_streaming:
#     warmup_iterations: 500  # Longer warmup for exploration
#     defense_layers:
#       preset: "relaxed"
#       # Relaxed preset uses:
#       # - warm_start_threshold: 0.02 (2%)
#       # - cost_increase_tolerance: 0.10 (10%)
#       # - max_step_size: 0.2
#     telemetry:
#       enabled: true
#       export_format: "prometheus"

# regression_testing:
#   # For comparing against pre-0.3.6 results
#   # Disables all defense layers for exact behavior match
#   runtime:
#     device: "cpu"
#     precision: "float64"
#     random_seed: 42
#   hybrid_streaming:
#     defense_layers:
#       preset: "disabled"
#       # All layers OFF - pre-0.3.6 behavior
#     telemetry:
#       enabled: false

# scientific_computing:
#   # Tuned for physics/scientific models
#   # Uses scientific_default preset with production monitoring
#   runtime:
#     device: "cpu"
#     precision: "float64"
#   hybrid_streaming:
#     normalization_strategy: "bounds"  # Better for multi-scale physics
#     defense_layers:
#       preset: "scientific"
#       # Scientific preset: balanced for physics models
#     telemetry:
#       enabled: true
#       export_format: "prometheus"
#   validation_and_qc:
#     sanity_checks:
#       enabled: true
#       checks:
#         - "finite_parameters"
#         - "nonnegative_variances"
#         - "positive_definite_covariance"
